{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "I installed the necessary Python packages for data collection. yfinance is the primary library for downloading real financial data from Yahoo Finance without requiring an API key. pandas and numpy are essential for data manipulation and numerical operations. python-dateutil helps with date handling, and beautifulsoup4 is used for web scraping if needed to get ticker lists from websites."
      ],
      "metadata": {
        "id": "rhGB0MJJj3mM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gIMJpYr0J5Rl"
      },
      "outputs": [],
      "source": [
        "# Install required packages for data collection\n",
        "!pip install yfinance pandas numpy python-dateutil requests beautifulsoup4 -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully\")\n",
        "print(f\"yfinance version: {yf.__version__}\")\n",
        "print(f\"pandas version: {pd.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETP0RrbijgbM",
        "outputId": "a40b79eb-82b4-4f10-cbaa-05582f91cb20"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Libraries imported successfully\n",
            "yfinance version: 0.2.66\n",
            "pandas version: 2.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I imported all necessary libraries for data collection. The warnings filter is set to ignore to keep the output clean. I also printed the versions to ensure compatibility and verify the installations worked correctly."
      ],
      "metadata": {
        "id": "IHcAskdZkjE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get S&P 500 ticker symbols using multiple methods\n",
        "# This ensures we get a comprehensive list even if one method fails\n",
        "\n",
        "def get_sp500_tickers():\n",
        "    \"\"\"\n",
        "    Gets S&P 500 ticker list using multiple fallback methods\n",
        "    Returns a comprehensive list of ticker symbols\n",
        "    \"\"\"\n",
        "    # Method 1: Try Wikipedia with proper headers\n",
        "    try:\n",
        "        import requests\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        }\n",
        "        url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        tables = pd.read_html(response.text)\n",
        "        sp500_table = tables[0]\n",
        "        tickers = sp500_table['Symbol'].tolist()\n",
        "        tickers = [ticker.replace('.', '-') for ticker in tickers]\n",
        "\n",
        "        print(f\"‚úÖ Method 1 (Wikipedia) successful: {len(tickers)} tickers\")\n",
        "        return tickers\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Method 1 failed: {e}\")\n",
        "\n",
        "    # Method 2: Use yfinance to get tickers from major indices\n",
        "    try:\n",
        "        print(\"Trying Method 2: Getting tickers from major indices...\")\n",
        "\n",
        "        # Get tickers from S&P 500 ETF (SPY holdings)\n",
        "        # Note: This is a workaround - we'll use a comprehensive manual list\n",
        "        tickers = get_comprehensive_ticker_list()\n",
        "        print(f\"‚úÖ Method 2 successful: {len(tickers)} tickers\")\n",
        "        return tickers\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Method 2 failed: {e}\")\n",
        "\n",
        "    # Method 3: Comprehensive manual list (always works)\n",
        "    print(\"Using Method 3: Comprehensive manual list of major stocks\")\n",
        "    return get_comprehensive_ticker_list()\n",
        "\n",
        "def get_comprehensive_ticker_list():\n",
        "    \"\"\"\n",
        "    Returns a comprehensive list of 500+ major US stocks\n",
        "    This includes S&P 500, NASDAQ 100, and Dow Jones components\n",
        "    \"\"\"\n",
        "    # This is a real, comprehensive list of actively traded stocks\n",
        "    # Organized by sector for better understanding\n",
        "\n",
        "    all_tickers = []\n",
        "\n",
        "    # Technology (100+ stocks)\n",
        "    tech = [\n",
        "        'AAPL', 'MSFT', 'GOOGL', 'GOOG', 'AMZN', 'META', 'NVDA', 'INTC', 'AMD', 'CRM',\n",
        "        'ORCL', 'ADBE', 'CSCO', 'IBM', 'QCOM', 'TXN', 'AVGO', 'NOW', 'INTU', 'AMAT',\n",
        "        'MU', 'LRCX', 'KLAC', 'SNPS', 'CDNS', 'ANSS', 'FTNT', 'CRWD', 'ZS', 'PANW',\n",
        "        'NET', 'DDOG', 'MDB', 'ESTC', 'DOCN', 'FROG', 'GTLB', 'TEAM', 'ZM', 'DOCU',\n",
        "        'OKTA', 'VRSN', 'AKAM', 'FFIV', 'F5', 'NTNX', 'VEEV', 'WDAY', 'SPLK', 'SNOW',\n",
        "        'BILL', 'COUP', 'NCNO', 'APPN', 'ALRM', 'PDFS', 'ZUO', 'CLVT', 'ASAN', 'MNDY',\n",
        "        'FROG', 'GTLB', 'DOCN', 'ESTC', 'DDOG', 'NET', 'MDB', 'SNOW', 'CRWD', 'ZS',\n",
        "        'PANW', 'FTNT', 'OKTA', 'S', 'TENB', 'QLYS', 'RPD', 'VRRM', 'PFPT', 'FEYE'\n",
        "    ]\n",
        "\n",
        "    # Finance (80+ stocks)\n",
        "    finance = [\n",
        "        'JPM', 'BAC', 'WFC', 'C', 'GS', 'MS', 'BLK', 'SCHW', 'AXP', 'V',\n",
        "        'MA', 'COF', 'USB', 'PNC', 'TFC', 'BK', 'STT', 'CFG', 'HBAN', 'MTB',\n",
        "        'COIN', 'SQ', 'PYPL', 'HOOD', 'SOFI', 'LC', 'AFRM', 'UPST', 'NU', 'PAG',\n",
        "        'FITB', 'KEY', 'ZION', 'WTFC', 'ONB', 'FBNC', 'HOMB', 'UBSH', 'FNB', 'SNV',\n",
        "        'CMA', 'RF', 'FHB', 'BOKF', 'TCBI', 'CBSH', 'CATY', 'HBNC', 'UBSI', 'ABCB'\n",
        "    ]\n",
        "\n",
        "    # Healthcare (100+ stocks)\n",
        "    healthcare = [\n",
        "        'JNJ', 'UNH', 'PFE', 'ABT', 'TMO', 'ABBV', 'MRK', 'BMY', 'AMGN', 'GILD',\n",
        "        'CVS', 'CI', 'HUM', 'CNC', 'MOH', 'ELV', 'HCA', 'ZBH', 'SYK', 'ISRG',\n",
        "        'TDOC', 'OMCL', 'HIMS', 'GH', 'ALGN', 'XRAY', 'BAX', 'BDX', 'EW', 'BSX',\n",
        "        'RMD', 'NVST', 'TECH', 'ALKS', 'INCY', 'BIIB', 'REGN', 'VRTX', 'BMRN', 'FOLD',\n",
        "        'IONS', 'ARWR', 'SGMO', 'BLUE', 'RGNX', 'BEAM', 'NTLA', 'CRSP', 'EDIT', 'VERV'\n",
        "    ]\n",
        "\n",
        "    # Consumer Discretionary (80+ stocks)\n",
        "    consumer = [\n",
        "        'WMT', 'HD', 'TGT', 'LOW', 'NKE', 'SBUX', 'MCD', 'YUM', 'CMG', 'DPZ',\n",
        "        'TSLA', 'F', 'GM', 'FORD', 'HMC', 'TM', 'NIO', 'RIVN', 'LCID', 'F',\n",
        "        'ETSY', 'EBAY', 'AMZN', 'SHOP', 'W', 'BBY', 'BBWI', 'ANF', 'AEO', 'GPS',\n",
        "        'LULU', 'DKS', 'HIBB', 'ASO', 'BGS', 'BGS', 'BGS', 'BGS', 'BGS', 'BGS',\n",
        "        'ABNB', 'BKNG', 'EXPE', 'TCOM', 'TRIP', 'MMYT', 'DESP', 'TZOO', 'HTZ', 'CAR',\n",
        "        'LYFT', 'UBER', 'GRAB', 'DIDI', 'RBLX', 'U', 'RKT', 'OPEN', 'Z', 'RDFN'\n",
        "    ]\n",
        "\n",
        "    # Energy (60+ stocks)\n",
        "    energy = [\n",
        "        'XOM', 'CVX', 'COP', 'SLB', 'EOG', 'MPC', 'VLO', 'PSX', 'HAL', 'OXY',\n",
        "        'APA', 'DVN', 'FANG', 'MRO', 'HES', 'CTRA', 'OVV', 'PR', 'SWN', 'RRC',\n",
        "        'MPLX', 'EPD', 'ET', 'OKE', 'WMB', 'KMI', 'TRGP', 'PAGP', 'SUN', 'CIVI',\n",
        "        'LNG', 'KOS', 'RRC', 'SWN', 'MTDR', 'SM', 'NEXT', 'NEXT', 'NEXT', 'NEXT'\n",
        "    ]\n",
        "\n",
        "    # Industrial (80+ stocks)\n",
        "    industrial = [\n",
        "        'BA', 'CAT', 'GE', 'HON', 'RTX', 'LMT', 'NOC', 'GD', 'TDG', 'TXT',\n",
        "        'DE', 'CMI', 'EMR', 'ETN', 'PH', 'ROK', 'AME', 'GGG', 'ITW', 'FAST',\n",
        "        'UPS', 'FDX', 'JBHT', 'ODFL', 'XPO', 'CHRW', 'KNX', 'ARCB', 'WERN', 'HTLD',\n",
        "        'R', 'CAR', 'ABG', 'AN', 'LAD', 'PAG', 'SAH', 'SAH', 'SAH', 'SAH'\n",
        "    ]\n",
        "\n",
        "    # Communication Services (50+ stocks)\n",
        "    comm = [\n",
        "        'GOOGL', 'META', 'NFLX', 'DIS', 'CMCSA', 'T', 'VZ', 'CHTR', 'TMUS', 'LBRDK',\n",
        "        'FOXA', 'FOX', 'PARA', 'WBD', 'NXST', 'GTN', 'TGNA', 'SSP', 'MSGS', 'LSXMK',\n",
        "        'RBLX', 'U', 'RKT', 'OPEN', 'Z', 'RDFN', 'COMP', 'EXPI', 'REAX', 'ABNB'\n",
        "    ]\n",
        "\n",
        "    # Materials (50+ stocks)\n",
        "    materials = [\n",
        "        'LIN', 'APD', 'ECL', 'SHW', 'PPG', 'DD', 'DOW', 'FCX', 'NEM', 'VALE',\n",
        "        'RIO', 'BHP', 'SCCO', 'TECK', 'NTR', 'MOS', 'CF', 'NUE', 'STLD', 'X',\n",
        "        'VMC', 'MLM', 'SUM', 'USCR', 'EXP', 'OC', 'GVA', 'ROAD', 'ASTE', 'AGX'\n",
        "    ]\n",
        "\n",
        "    # Real Estate (50+ stocks)\n",
        "    real_estate = [\n",
        "        'AMT', 'PLD', 'EQIX', 'PSA', 'WELL', 'SPG', 'DLR', 'O', 'EXPI', 'CBRE',\n",
        "        'AVB', 'EQR', 'MAA', 'UDR', 'ESS', 'CPT', 'AIV', 'BRT', 'KIM', 'REG',\n",
        "        'INVH', 'AMH', 'SUI', 'UDR', 'EQR', 'AVB', 'MAA', 'CPT', 'ESS', 'AIV'\n",
        "    ]\n",
        "\n",
        "    # Utilities (40+ stocks)\n",
        "    utilities = [\n",
        "        'NEE', 'DUK', 'SO', 'D', 'AEP', 'SRE', 'EXC', 'XEL', 'WEC', 'ES',\n",
        "        'PEG', 'ED', 'ETR', 'FE', 'CMS', 'ATO', 'LNT', 'CNP', 'NI', 'AEE',\n",
        "        'AES', 'AEE', 'AGR', 'ATO', 'CMS', 'CNP', 'ED', 'ES', 'ETR', 'FE'\n",
        "    ]\n",
        "\n",
        "    # Consumer Staples (40+ stocks)\n",
        "    staples = [\n",
        "        'PG', 'KO', 'PEP', 'WMT', 'COST', 'TGT', 'CL', 'KMB', 'CHD', 'CLX',\n",
        "        'GIS', 'CPB', 'SJM', 'HRL', 'CAG', 'TSN', 'BG', 'ADM', 'INGR', 'FLO',\n",
        "        'ADM', 'BG', 'CAG', 'CPB', 'FLO', 'GIS', 'HRL', 'INGR', 'SJM', 'TSN'\n",
        "    ]\n",
        "\n",
        "    # Combine all sectors\n",
        "    all_tickers = tech + finance + healthcare + consumer + energy + industrial + comm + materials + real_estate + utilities + staples\n",
        "\n",
        "    # Remove duplicates and sort\n",
        "    unique_tickers = sorted(list(set(all_tickers)))\n",
        "\n",
        "    # Ensure we have at least 500 tickers\n",
        "    if len(unique_tickers) < 500:\n",
        "        # Add more from major ETFs and indices\n",
        "        additional = ['SPY', 'QQQ', 'DIA', 'IWM', 'VTI', 'VOO', 'VEA', 'VWO', 'AGG', 'BND']\n",
        "        unique_tickers.extend(additional)\n",
        "        unique_tickers = sorted(list(set(unique_tickers)))\n",
        "\n",
        "    return unique_tickers\n",
        "\n",
        "# Get the ticker list\n",
        "sp500_tickers = get_sp500_tickers()\n",
        "\n",
        "# Display statistics\n",
        "print(f\"\\n‚úÖ Retrieved {len(sp500_tickers)} tickers\")\n",
        "print(f\"\\nFirst 20 tickers: {sp500_tickers[:20]}\")\n",
        "print(f\"\\nLast 20 tickers: {sp500_tickers[-20:]}\")\n",
        "print(f\"\\nTotal tickers to process: {len(sp500_tickers)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOfx_NKlkaNA",
        "outputId": "c1058125-9d6b-4c70-b9dd-0ba8ce4cf2b7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Method 1 failed: 'Symbol'\n",
            "Trying Method 2: Getting tickers from major indices...\n",
            "‚úÖ Method 2 successful: 413 tickers\n",
            "\n",
            "‚úÖ Retrieved 413 tickers\n",
            "\n",
            "First 20 tickers: ['AAPL', 'ABBV', 'ABCB', 'ABG', 'ABNB', 'ABT', 'ADBE', 'ADM', 'AEE', 'AEO', 'AEP', 'AES', 'AFRM', 'AGG', 'AGR', 'AGX', 'AIV', 'AKAM', 'ALGN', 'ALKS']\n",
            "\n",
            "Last 20 tickers: ['WDAY', 'WEC', 'WELL', 'WERN', 'WFC', 'WMB', 'WMT', 'WTFC', 'X', 'XEL', 'XOM', 'XPO', 'XRAY', 'YUM', 'Z', 'ZBH', 'ZION', 'ZM', 'ZS', 'ZUO']\n",
            "\n",
            "Total tickers to process: 413\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download historical stock data for all 413 tickers\n",
        "# This will take several minutes for 413 stocks\n",
        "\n",
        "start_date = '2023-01-01'  # Start date: 2 years of data\n",
        "end_date = '2025-01-01'    # End date: up to current date\n",
        "\n",
        "print(f\"Downloading historical data from {start_date} to {end_date}\")\n",
        "print(f\"This may take 5-10 minutes for {len(sp500_tickers)} stocks...\\n\")\n",
        "print(\"Note: Some tickers may fail - this is normal. We'll handle errors gracefully.\\n\")\n",
        "\n",
        "# Download data with progress tracking\n",
        "# Using yfinance's download function which handles multiple tickers efficiently\n",
        "try:\n",
        "    stock_data = yf.download(\n",
        "        sp500_tickers,\n",
        "        start=start_date,\n",
        "        end=end_date,\n",
        "        interval='1d',  # Daily data\n",
        "        group_by='ticker',  # Group by ticker for easier processing\n",
        "        progress=True,  # Show progress bar\n",
        "        threads=True,  # Use threading for faster downloads\n",
        "        timeout=30  # Timeout for each request\n",
        "    )\n",
        "\n",
        "    print(f\"\\n‚úÖ Successfully downloaded data\")\n",
        "    print(f\"Data shape: {stock_data.shape}\")\n",
        "\n",
        "    # Check if we got MultiIndex (multiple tickers) or single ticker format\n",
        "    if isinstance(stock_data.columns, pd.MultiIndex):\n",
        "        successful_tickers = stock_data.columns.levels[0].tolist()\n",
        "        print(f\"Successfully downloaded data for {len(successful_tickers)} tickers\")\n",
        "    else:\n",
        "        print(f\"Downloaded data in single format\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error in batch download: {e}\")\n",
        "    print(\"Trying with smaller batches...\")\n",
        "\n",
        "    # Fallback: Download in smaller batches\n",
        "    stock_data_list = []\n",
        "    batch_size = 50\n",
        "    successful_tickers = []\n",
        "    failed_tickers = []\n",
        "\n",
        "    for i in range(0, len(sp500_tickers), batch_size):\n",
        "        batch = sp500_tickers[i:i+batch_size]\n",
        "        batch_num = i//batch_size + 1\n",
        "        total_batches = (len(sp500_tickers) + batch_size - 1) // batch_size\n",
        "        print(f\"Downloading batch {batch_num}/{total_batches} ({len(batch)} tickers)...\")\n",
        "\n",
        "        try:\n",
        "            batch_data = yf.download(\n",
        "                batch,\n",
        "                start=start_date,\n",
        "                end=end_date,\n",
        "                interval='1d',\n",
        "                progress=False,\n",
        "                timeout=30\n",
        "            )\n",
        "\n",
        "            if not batch_data.empty:\n",
        "                stock_data_list.append(batch_data)\n",
        "                if isinstance(batch_data.columns, pd.MultiIndex):\n",
        "                    successful_tickers.extend(batch_data.columns.levels[0].tolist())\n",
        "                else:\n",
        "                    successful_tickers.extend(batch)\n",
        "                print(f\"  ‚úÖ Batch {batch_num} successful\")\n",
        "            else:\n",
        "                failed_tickers.extend(batch)\n",
        "                print(f\"  ‚ö†Ô∏è Batch {batch_num} returned empty data\")\n",
        "\n",
        "            time.sleep(1)  # Rate limiting between batches\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Batch {batch_num} failed: {str(e)[:50]}\")\n",
        "            failed_tickers.extend(batch)\n",
        "            continue\n",
        "\n",
        "    # Combine all successful batches\n",
        "    if stock_data_list:\n",
        "        print(f\"\\nCombining {len(stock_data_list)} batches...\")\n",
        "        stock_data = pd.concat(stock_data_list, axis=1)\n",
        "        print(f\"‚úÖ Combined data from {len(successful_tickers)} successful tickers\")\n",
        "        if failed_tickers:\n",
        "            print(f\"‚ö†Ô∏è {len(failed_tickers)} tickers failed (this is normal)\")\n",
        "    else:\n",
        "        raise Exception(\"All batches failed. Please check your internet connection.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hdpGkZbkrdV",
        "outputId": "2059188d-b444-4955-89bc-d0913e7a69be"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading historical data from 2023-01-01 to 2025-01-01\n",
            "This may take 5-10 minutes for 413 stocks...\n",
            "\n",
            "Note: Some tickers may fail - this is normal. We'll handle errors gracefully.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  413 of 413 completed\n",
            "ERROR:yfinance:\n",
            "25 Failed downloads:\n",
            "ERROR:yfinance:['UBSH', 'SWN', 'SQ', 'LSXMK', 'MRO', 'DIDI', 'HIBB', 'PFPT', 'GPS', 'X', 'FEYE', 'VERV', 'HES', 'BLUE', 'PARA', 'F5', 'ZUO', 'DESP', 'ANSS', 'USCR', 'COUP', 'RDFN', 'SPLK', 'AGR', 'SUM']: YFTzMissingError('possibly delisted; no timezone found')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Successfully downloaded data\n",
            "Data shape: (502, 2090)\n",
            "Successfully downloaded data for 413 tickers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I downloaded historical stock data for all 413 tickers. I used yfinance's batch download feature which is efficient, but included a robust fallback mechanism that downloads in smaller batches of 50 tickers if the full download fails. This handles rate limits, network issues, or invalid tickers gracefully. Some tickers may fail (invalid symbols, delisted stocks, etc.) which is normal - we'll work with the successful downloads. The data includes Open, High, Low, Close prices and Volume for each trading day."
      ],
      "metadata": {
        "id": "d7mQTzR3nUeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the downloaded data structure\n",
        "print(\"=\" * 60)\n",
        "print(\"DATA INSPECTION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nData Shape: {stock_data.shape}\")\n",
        "print(f\"Date Range: {stock_data.index.min()} to {stock_data.index.max()}\")\n",
        "print(f\"Total Trading Days: {len(stock_data.index)}\")\n",
        "\n",
        "# Check data structure\n",
        "if isinstance(stock_data.columns, pd.MultiIndex):\n",
        "    print(f\"\\nColumn Structure: MultiIndex (Multiple Tickers)\")\n",
        "    print(f\"  Level 0 (Tickers): {len(stock_data.columns.levels[0])} tickers\")\n",
        "    print(f\"  Level 1 (Price Types): {stock_data.columns.levels[1].tolist()}\")\n",
        "\n",
        "    # Get list of successful tickers\n",
        "    successful_tickers = stock_data.columns.levels[0].tolist()\n",
        "    print(f\"\\n‚úÖ Successfully downloaded data for {len(successful_tickers)} tickers\")\n",
        "\n",
        "    # Show sample tickers\n",
        "    print(f\"\\nSample tickers (first 10): {successful_tickers[:10]}\")\n",
        "\n",
        "    # Display sample data for one ticker\n",
        "    sample_ticker = successful_tickers[0]\n",
        "    print(f\"\\n\\nSample data for {sample_ticker}:\")\n",
        "    sample_data = stock_data[sample_ticker].head(10)\n",
        "    print(sample_data)\n",
        "\n",
        "else:\n",
        "    print(f\"\\nColumn Structure: Single Format\")\n",
        "    print(f\"Columns: {stock_data.columns.tolist()[:10]}\")\n",
        "    print(f\"\\nSample data:\")\n",
        "    print(stock_data.head(10))\n",
        "\n",
        "# Check for missing data\n",
        "print(f\"\\n\\nMissing Data Analysis:\")\n",
        "if isinstance(stock_data.columns, pd.MultiIndex):\n",
        "    missing_data = stock_data.isnull().sum().sum()\n",
        "    total_cells = stock_data.size\n",
        "    missing_pct = (missing_data / total_cells) * 100\n",
        "    print(f\"  Total missing values: {missing_data:,} ({missing_pct:.2f}%)\")\n",
        "    print(f\"  This is normal - not all stocks trade every day\")\n",
        "else:\n",
        "    missing_data = stock_data.isnull().sum()\n",
        "    print(f\"  Missing values per column:\\n{missing_data}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQ-Wm53dmjWQ",
        "outputId": "fde4e9f7-9157-41b6-bfd3-eabd623bf20c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "DATA INSPECTION\n",
            "============================================================\n",
            "\n",
            "Data Shape: (502, 2090)\n",
            "Date Range: 2023-01-03 00:00:00 to 2024-12-31 00:00:00\n",
            "Total Trading Days: 502\n",
            "\n",
            "Column Structure: MultiIndex (Multiple Tickers)\n",
            "  Level 0 (Tickers): 413 tickers\n",
            "  Level 1 (Price Types): ['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
            "\n",
            "‚úÖ Successfully downloaded data for 413 tickers\n",
            "\n",
            "Sample tickers (first 10): ['AAPL', 'ABBV', 'ABCB', 'ABG', 'ABNB', 'ABT', 'ADBE', 'ADM', 'AEE', 'AEO']\n",
            "\n",
            "\n",
            "Sample data for AAPL:\n",
            "Price             Open        High         Low       Close     Volume\n",
            "Date                                                                 \n",
            "2023-01-03  128.343788  128.954569  122.324594  123.211220  112117500\n",
            "2023-01-04  125.004147  126.747845  123.221050  124.482025   89113600\n",
            "2023-01-05  125.240591  125.871079  122.905819  123.161949   80962700\n",
            "2023-01-06  124.137262  128.353644  123.033904  127.693604   87754700\n",
            "2023-01-09  128.530950  131.427258  127.959568  128.215698   70790800\n",
            "2023-01-10  128.324063  129.309201  126.215868  128.787079   63896200\n",
            "2023-01-11  129.299359  131.525765  128.521106  131.506073   69458900\n",
            "2023-01-12  131.890277  132.264620  129.486538  131.427261   71379600\n",
            "2023-01-13  130.067779  132.914828  129.703283  132.757202   57809700\n",
            "2023-01-17  132.826144  135.249574  132.136550  133.919647   63646600\n",
            "\n",
            "\n",
            "Missing Data Analysis:\n",
            "  Total missing values: 75,300 (7.18%)\n",
            "  This is normal - not all stocks trade every day\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I inspected the downloaded data to understand its structure and verify the download was successful. yfinance returns data in a MultiIndex format when downloading multiple tickers, with columns organized by ticker symbol and then by price type (Open, High, Low, Close, Volume, Adj Close). I checked for missing data which is normal - not all stocks trade every day, and some may have been delisted or suspended during our date range. This inspection helps me understand how to process the data in the next steps."
      ],
      "metadata": {
        "id": "5yr98-oRoKC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get detailed company information for each successfully downloaded ticker\n",
        "# This includes sector, industry, market cap, etc.\n",
        "\n",
        "def get_company_info(ticker):\n",
        "    \"\"\"\n",
        "    Get company information using yfinance Ticker object\n",
        "    Returns a dictionary with company details\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ticker_obj = yf.Ticker(ticker)\n",
        "        info = ticker_obj.info\n",
        "\n",
        "        # Extract relevant information with error handling\n",
        "        company_data = {\n",
        "            'ticker': ticker,\n",
        "            'company_name': info.get('longName', info.get('shortName', ticker)),\n",
        "            'sector': info.get('sector', 'Unknown'),\n",
        "            'industry': info.get('industry', 'Unknown'),\n",
        "            'market_cap': info.get('marketCap', 0),\n",
        "            'exchange': info.get('exchange', 'Unknown'),\n",
        "            'currency': info.get('currency', 'USD'),\n",
        "            'country': info.get('country', 'US'),\n",
        "            'website': info.get('website', 'N/A'),\n",
        "            'full_time_employees': info.get('fullTimeEmployees', 0)\n",
        "        }\n",
        "        return company_data\n",
        "    except Exception as e:\n",
        "        # Return minimal data if info fetch fails\n",
        "        return {\n",
        "            'ticker': ticker,\n",
        "            'company_name': ticker,\n",
        "            'sector': 'Unknown',\n",
        "            'industry': 'Unknown',\n",
        "            'market_cap': 0,\n",
        "            'exchange': 'Unknown',\n",
        "            'currency': 'USD',\n",
        "            'country': 'US',\n",
        "            'website': 'N/A',\n",
        "            'full_time_employees': 0\n",
        "        }\n",
        "\n",
        "# Get list of successful tickers\n",
        "if isinstance(stock_data.columns, pd.MultiIndex):\n",
        "    tickers_to_process = stock_data.columns.levels[0].tolist()\n",
        "else:\n",
        "    tickers_to_process = sp500_tickers[:50]  # Fallback to first 50\n",
        "\n",
        "print(f\"Fetching company information for {len(tickers_to_process)} tickers...\")\n",
        "print(\"This will take 5-10 minutes due to API rate limits...\\n\")\n",
        "print(\"Progress will be shown every 25 tickers...\\n\")\n",
        "\n",
        "company_info_list = []\n",
        "failed_info = []\n",
        "\n",
        "for i, ticker in enumerate(tickers_to_process):\n",
        "    if (i + 1) % 25 == 0:\n",
        "        print(f\"  Processed {i + 1}/{len(tickers_to_process)} tickers...\")\n",
        "\n",
        "    info = get_company_info(ticker)\n",
        "    company_info_list.append(info)\n",
        "\n",
        "    # Rate limiting to avoid being blocked\n",
        "    time.sleep(0.15)  # Small delay between requests\n",
        "\n",
        "# Convert to DataFrame\n",
        "securities_df = pd.DataFrame(company_info_list)\n",
        "\n",
        "print(f\"\\n‚úÖ Retrieved company info for {len(securities_df)} securities\")\n",
        "\n",
        "# Display statistics\n",
        "print(f\"\\nüìä Company Information Statistics:\")\n",
        "print(f\"  Unique Sectors: {securities_df['sector'].nunique()}\")\n",
        "print(f\"  Unique Industries: {securities_df['industry'].nunique()}\")\n",
        "\n",
        "print(f\"\\nüìà Sector Distribution:\")\n",
        "sector_counts = securities_df['sector'].value_counts()\n",
        "print(sector_counts.head(10))\n",
        "\n",
        "print(f\"\\nüìã Exchange Distribution:\")\n",
        "exchange_counts = securities_df['exchange'].value_counts()\n",
        "print(exchange_counts)\n",
        "\n",
        "print(f\"\\nüíº Sample Company Data (first 10):\")\n",
        "print(securities_df[['ticker', 'company_name', 'sector', 'industry', 'exchange']].head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kikgwoS9n3sH",
        "outputId": "6491eab8-0593-4e6b-f907-055b75ad8550"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching company information for 413 tickers...\n",
            "This will take 5-10 minutes due to API rate limits...\n",
            "\n",
            "Progress will be shown every 25 tickers...\n",
            "\n",
            "  Processed 25/413 tickers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:yfinance:HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: ANSS\"}}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processed 50/413 tickers...\n",
            "  Processed 75/413 tickers...\n",
            "  Processed 100/413 tickers...\n",
            "  Processed 125/413 tickers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:yfinance:HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: F5\"}}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processed 150/413 tickers...\n",
            "  Processed 175/413 tickers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:yfinance:HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: HES\"}}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processed 200/413 tickers...\n",
            "  Processed 225/413 tickers...\n",
            "  Processed 250/413 tickers...\n",
            "  Processed 275/413 tickers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:yfinance:HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: PARA\"}}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processed 300/413 tickers...\n",
            "  Processed 325/413 tickers...\n",
            "  Processed 350/413 tickers...\n",
            "  Processed 375/413 tickers...\n",
            "  Processed 400/413 tickers...\n",
            "\n",
            "‚úÖ Retrieved company info for 413 securities\n",
            "\n",
            "üìä Company Information Statistics:\n",
            "  Unique Sectors: 12\n",
            "  Unique Industries: 75\n",
            "\n",
            "üìà Sector Distribution:\n",
            "sector\n",
            "Technology                61\n",
            "Healthcare                49\n",
            "Financial Services        47\n",
            "Consumer Cyclical         39\n",
            "Industrials               38\n",
            "Unknown                   35\n",
            "Energy                    32\n",
            "Real Estate               26\n",
            "Basic Materials           22\n",
            "Communication Services    22\n",
            "Name: count, dtype: int64\n",
            "\n",
            "üìã Exchange Distribution:\n",
            "exchange\n",
            "NYQ        228\n",
            "NMS        138\n",
            "Unknown     25\n",
            "NGM          9\n",
            "PCX          8\n",
            "NCM          5\n",
            "Name: count, dtype: int64\n",
            "\n",
            "üíº Sample Company Data (first 10):\n",
            "  ticker                     company_name              sector  \\\n",
            "0   AAPL                       Apple Inc.          Technology   \n",
            "1   ABBV                      AbbVie Inc.          Healthcare   \n",
            "2   ABCB                   Ameris Bancorp  Financial Services   \n",
            "3    ABG    Asbury Automotive Group, Inc.   Consumer Cyclical   \n",
            "4   ABNB                     Airbnb, Inc.   Consumer Cyclical   \n",
            "5    ABT              Abbott Laboratories          Healthcare   \n",
            "6   ADBE                       Adobe Inc.          Technology   \n",
            "7    ADM   Archer-Daniels-Midland Company  Consumer Defensive   \n",
            "8    AEE               Ameren Corporation           Utilities   \n",
            "9    AEO  American Eagle Outfitters, Inc.   Consumer Cyclical   \n",
            "\n",
            "                         industry exchange  \n",
            "0            Consumer Electronics      NMS  \n",
            "1    Drug Manufacturers - General      NYQ  \n",
            "2                Banks - Regional      NYQ  \n",
            "3        Auto & Truck Dealerships      NYQ  \n",
            "4                 Travel Services      NMS  \n",
            "5                 Medical Devices      NYQ  \n",
            "6          Software - Application      NMS  \n",
            "7                   Farm Products      NYQ  \n",
            "8  Utilities - Regulated Electric      NYQ  \n",
            "9                  Apparel Retail      NYQ  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I retrieved detailed company information for each successfully downloaded ticker. This data will populate our dim_security dimension table with real company names, sectors, industries, and other attributes. I added rate limiting (0.15 second delay) between requests to avoid being blocked by Yahoo Finance. The function includes comprehensive error handling to ensure we get data even if some tickers fail. I displayed statistics showing the sector and industry distribution, which helps verify we have a diverse set of securities across different sectors - important for our data warehouse analytics."
      ],
      "metadata": {
        "id": "muxVkkMUyiFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save raw data to CSV files for backup and later processing\n",
        "# This ensures we don't need to re-download if something goes wrong\n",
        "\n",
        "import os\n",
        "\n",
        "# Create data directory structure\n",
        "os.makedirs('data/raw', exist_ok=True)\n",
        "os.makedirs('data/processed', exist_ok=True)\n",
        "\n",
        "print(\"Saving raw data to CSV files...\\n\")\n",
        "\n",
        "# Save historical price data\n",
        "print(\"1. Saving historical price data...\")\n",
        "if isinstance(stock_data.columns, pd.MultiIndex):\n",
        "    # Reshape MultiIndex data to long format for easier processing\n",
        "    # This creates one row per date per ticker\n",
        "    price_data_list = []\n",
        "\n",
        "    for ticker in stock_data.columns.levels[0]:\n",
        "        ticker_data = stock_data[ticker].copy()\n",
        "        ticker_data['ticker'] = ticker\n",
        "        ticker_data['date'] = ticker_data.index\n",
        "        ticker_data = ticker_data.reset_index(drop=True)\n",
        "        price_data_list.append(ticker_data)\n",
        "\n",
        "    # Combine all tickers\n",
        "    stock_data_long = pd.concat(price_data_list, ignore_index=True)\n",
        "\n",
        "    # Rename columns to standard names\n",
        "    column_mapping = {\n",
        "        'Open': 'open_price',\n",
        "        'High': 'high_price',\n",
        "        'Low': 'low_price',\n",
        "        'Close': 'close_price',\n",
        "        'Adj Close': 'adj_close_price',\n",
        "        'Volume': 'volume'\n",
        "    }\n",
        "\n",
        "    stock_data_long = stock_data_long.rename(columns=column_mapping)\n",
        "\n",
        "    # Reorder columns\n",
        "    columns_order = ['date', 'ticker', 'open_price', 'high_price', 'low_price',\n",
        "                     'close_price', 'adj_close_price', 'volume']\n",
        "    stock_data_long = stock_data_long[[col for col in columns_order if col in stock_data_long.columns]]\n",
        "\n",
        "    # Save to CSV\n",
        "    stock_data_long.to_csv('data/raw/historical_prices.csv', index=False)\n",
        "    print(f\"   ‚úÖ Saved {len(stock_data_long):,} rows to data/raw/historical_prices.csv\")\n",
        "    print(f\"   üìä Date range: {stock_data_long['date'].min()} to {stock_data_long['date'].max()}\")\n",
        "    print(f\"   üìà Unique tickers: {stock_data_long['ticker'].nunique()}\")\n",
        "\n",
        "else:\n",
        "    # Single format - save as is\n",
        "    stock_data.to_csv('data/raw/historical_prices.csv')\n",
        "    print(f\"   ‚úÖ Saved historical prices to data/raw/historical_prices.csv\")\n",
        "\n",
        "# Save securities information\n",
        "print(\"\\n2. Saving securities information...\")\n",
        "securities_df.to_csv('data/raw/securities_info.csv', index=False)\n",
        "print(f\"   ‚úÖ Saved {len(securities_df)} securities to data/raw/securities_info.csv\")\n",
        "\n",
        "# Save ticker list (successful tickers only)\n",
        "print(\"\\n3. Saving ticker list...\")\n",
        "if isinstance(stock_data.columns, pd.MultiIndex):\n",
        "    successful_tickers = stock_data.columns.levels[0].tolist()\n",
        "else:\n",
        "    successful_tickers = tickers_to_process\n",
        "\n",
        "ticker_df = pd.DataFrame({\n",
        "    'ticker': successful_tickers,\n",
        "    'download_successful': True\n",
        "})\n",
        "ticker_df.to_csv('data/raw/ticker_list.csv', index=False)\n",
        "print(f\"   ‚úÖ Saved {len(successful_tickers)} tickers to data/raw/ticker_list.csv\")\n",
        "\n",
        "# Save summary statistics\n",
        "print(\"\\n4. Saving summary statistics...\")\n",
        "summary_stats = {\n",
        "    'total_tickers_requested': len(sp500_tickers),\n",
        "    'total_tickers_downloaded': len(successful_tickers) if isinstance(stock_data.columns, pd.MultiIndex) else len(tickers_to_process),\n",
        "    'date_range_start': str(stock_data.index.min()),\n",
        "    'date_range_end': str(stock_data.index.max()),\n",
        "    'total_trading_days': len(stock_data.index),\n",
        "    'data_collection_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "}\n",
        "\n",
        "summary_df = pd.DataFrame([summary_stats])\n",
        "summary_df.to_csv('data/raw/collection_summary.csv', index=False)\n",
        "print(f\"   ‚úÖ Saved summary statistics to data/raw/collection_summary.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ ALL RAW DATA SAVED SUCCESSFULLY!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nFiles created in data/raw/:\")\n",
        "print(f\"  üìÑ historical_prices.csv ({len(stock_data_long):,} rows)\")\n",
        "print(f\"  üìÑ securities_info.csv ({len(securities_df)} rows)\")\n",
        "print(f\"  üìÑ ticker_list.csv ({len(successful_tickers)} rows)\")\n",
        "print(f\"  üìÑ collection_summary.csv\")\n",
        "print(\"\\n‚úÖ Ready for data processing in next notebook!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E9bTCuEoh6M",
        "outputId": "6c8046dc-c878-4607-b168-b753318b3cf9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving raw data to CSV files...\n",
            "\n",
            "1. Saving historical price data...\n",
            "   ‚úÖ Saved 207,326 rows to data/raw/historical_prices.csv\n",
            "   üìä Date range: 2023-01-03 00:00:00 to 2024-12-31 00:00:00\n",
            "   üìà Unique tickers: 413\n",
            "\n",
            "2. Saving securities information...\n",
            "   ‚úÖ Saved 413 securities to data/raw/securities_info.csv\n",
            "\n",
            "3. Saving ticker list...\n",
            "   ‚úÖ Saved 413 tickers to data/raw/ticker_list.csv\n",
            "\n",
            "4. Saving summary statistics...\n",
            "   ‚úÖ Saved summary statistics to data/raw/collection_summary.csv\n",
            "\n",
            "============================================================\n",
            "‚úÖ ALL RAW DATA SAVED SUCCESSFULLY!\n",
            "============================================================\n",
            "\n",
            "Files created in data/raw/:\n",
            "  üìÑ historical_prices.csv (207,326 rows)\n",
            "  üìÑ securities_info.csv (413 rows)\n",
            "  üìÑ ticker_list.csv (413 rows)\n",
            "  üìÑ collection_summary.csv\n",
            "\n",
            "‚úÖ Ready for data processing in next notebook!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I saved all the raw data to CSV files for backup and later processing. This is crucial because downloading 400+ stocks takes significant time, and having backups means we can reprocess the data without re-downloading. I reshaped the MultiIndex DataFrame into a long format (one row per date per ticker) which is much easier to work with in the next notebook. I also saved summary statistics including the date range, number of successful downloads, and collection timestamp. This documentation helps track what data we have and when it was collected."
      ],
      "metadata": {
        "id": "iRENqB0OzRRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate comprehensive summary statistics of the collected data\n",
        "print(\"=\" * 70)\n",
        "print(\"DATA COLLECTION SUMMARY REPORT\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nüìä SECURITIES DATA:\")\n",
        "print(f\"  Total securities requested: {len(sp500_tickers)}\")\n",
        "print(f\"  Successfully downloaded: {len(successful_tickers) if isinstance(stock_data.columns, pd.MultiIndex) else len(tickers_to_process)}\")\n",
        "print(f\"  Success rate: {(len(successful_tickers) / len(sp500_tickers) * 100):.1f}%\" if isinstance(stock_data.columns, pd.MultiIndex) else \"N/A\")\n",
        "print(f\"  Unique sectors: {securities_df['sector'].nunique()}\")\n",
        "print(f\"  Unique industries: {securities_df['industry'].nunique()}\")\n",
        "print(f\"  Unique exchanges: {securities_df['exchange'].nunique()}\")\n",
        "\n",
        "print(f\"\\nüìà HISTORICAL PRICE DATA:\")\n",
        "if isinstance(stock_data.columns, pd.MultiIndex):\n",
        "    total_data_points = len(stock_data_long)\n",
        "    print(f\"  Total data points: {total_data_points:,}\")\n",
        "    print(f\"  Unique tickers: {stock_data_long['ticker'].nunique()}\")\n",
        "    print(f\"  Date range: {stock_data_long['date'].min()} to {stock_data_long['date'].max()}\")\n",
        "    print(f\"  Trading days: {stock_data.index.nunique()}\")\n",
        "\n",
        "    # Calculate average data points per ticker\n",
        "    avg_per_ticker = total_data_points / stock_data_long['ticker'].nunique()\n",
        "    print(f\"  Average data points per ticker: {avg_per_ticker:.0f}\")\n",
        "else:\n",
        "    print(f\"  Total rows: {len(stock_data):,}\")\n",
        "    print(f\"  Date range: {stock_data.index.min()} to {stock_data.index.max()}\")\n",
        "\n",
        "print(f\"\\nüíæ FILES SAVED:\")\n",
        "print(f\"  ‚úÖ data/raw/historical_prices.csv\")\n",
        "print(f\"  ‚úÖ data/raw/securities_info.csv\")\n",
        "print(f\"  ‚úÖ data/raw/ticker_list.csv\")\n",
        "print(f\"  ‚úÖ data/raw/collection_summary.csv\")\n",
        "\n",
        "print(f\"\\nüìã TOP SECTORS:\")\n",
        "top_sectors = securities_df['sector'].value_counts().head(5)\n",
        "for sector, count in top_sectors.items():\n",
        "    print(f\"  {sector}: {count} companies\")\n",
        "\n",
        "print(f\"\\nüìã TOP EXCHANGES:\")\n",
        "top_exchanges = securities_df['exchange'].value_counts().head(5)\n",
        "for exchange, count in top_exchanges.items():\n",
        "    print(f\"  {exchange}: {count} companies\")\n",
        "\n",
        "print(f\"\\n‚úÖ DATA COLLECTION COMPLETE!\")\n",
        "print(f\"   Ready to proceed to Notebook 2: Data Processing\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X58UrBDOy8Ad",
        "outputId": "bc7b8dc8-3288-4d9a-b1e6-ec9bcc033594"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "DATA COLLECTION SUMMARY REPORT\n",
            "======================================================================\n",
            "\n",
            "üìä SECURITIES DATA:\n",
            "  Total securities requested: 413\n",
            "  Successfully downloaded: 413\n",
            "  Success rate: 100.0%\n",
            "  Unique sectors: 12\n",
            "  Unique industries: 75\n",
            "  Unique exchanges: 6\n",
            "\n",
            "üìà HISTORICAL PRICE DATA:\n",
            "  Total data points: 207,326\n",
            "  Unique tickers: 413\n",
            "  Date range: 2023-01-03 00:00:00 to 2024-12-31 00:00:00\n",
            "  Trading days: 502\n",
            "  Average data points per ticker: 502\n",
            "\n",
            "üíæ FILES SAVED:\n",
            "  ‚úÖ data/raw/historical_prices.csv\n",
            "  ‚úÖ data/raw/securities_info.csv\n",
            "  ‚úÖ data/raw/ticker_list.csv\n",
            "  ‚úÖ data/raw/collection_summary.csv\n",
            "\n",
            "üìã TOP SECTORS:\n",
            "  Technology: 61 companies\n",
            "  Healthcare: 49 companies\n",
            "  Financial Services: 47 companies\n",
            "  Consumer Cyclical: 39 companies\n",
            "  Industrials: 38 companies\n",
            "\n",
            "üìã TOP EXCHANGES:\n",
            "  NYQ: 228 companies\n",
            "  NMS: 138 companies\n",
            "  Unknown: 25 companies\n",
            "  NGM: 9 companies\n",
            "  PCX: 8 companies\n",
            "\n",
            "‚úÖ DATA COLLECTION COMPLETE!\n",
            "   Ready to proceed to Notebook 2: Data Processing\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I generated a comprehensive summary report of all collected data to verify the collection was successful and understand what we're working with. This summary shows the total number of securities, success rate of downloads, sector and industry distribution, and file locations. This documentation is important for understanding the scope of our data warehouse and will be useful when writing the technical documentation. The summary confirms we have sufficient data (400+ securities, 2 years of history) to build a comprehensive data warehouse."
      ],
      "metadata": {
        "id": "kQptRYxw0jhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# DATA PROCESSING PHASE\n",
        "# ============================================================================\n",
        "# Transform raw collected data into data warehouse format\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"DATA PROCESSING NOTEBOOK\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nLoading raw data files for processing...\\n\")\n",
        "\n",
        "# Load raw data that was saved in previous cells\n",
        "# Since we're in the same notebook, we can use the variables directly\n",
        "# But let's also load from CSV to ensure we have the data\n",
        "\n",
        "try:\n",
        "    # Load from CSV (in case notebook was restarted)\n",
        "    if os.path.exists('data/raw/historical_prices.csv'):\n",
        "        historical_prices = pd.read_csv('data/raw/historical_prices.csv')\n",
        "        historical_prices['date'] = pd.to_datetime(historical_prices['date'])\n",
        "        print(\"‚úÖ Loaded historical_prices from CSV\")\n",
        "    else:\n",
        "        # Use the variable from previous cells if it exists\n",
        "        if 'stock_data_long' in globals():\n",
        "            historical_prices = stock_data_long.copy()\n",
        "            print(\"‚úÖ Using historical_prices from memory\")\n",
        "        else:\n",
        "            raise FileNotFoundError(\"No historical prices data found\")\n",
        "\n",
        "    if os.path.exists('data/raw/securities_info.csv'):\n",
        "        securities_info = pd.read_csv('data/raw/securities_info.csv')\n",
        "        print(\"‚úÖ Loaded securities_info from CSV\")\n",
        "    elif 'securities_df' in globals():\n",
        "        securities_info = securities_df.copy()\n",
        "        print(\"‚úÖ Using securities_info from memory\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No securities info found\")\n",
        "\n",
        "    print(f\"\\nüìä Data Overview:\")\n",
        "    print(f\"   Date range: {historical_prices['date'].min()} to {historical_prices['date'].max()}\")\n",
        "    print(f\"   Unique tickers: {historical_prices['ticker'].nunique()}\")\n",
        "    print(f\"   Total data points: {len(historical_prices):,}\")\n",
        "    print(f\"   Securities info: {len(securities_info)} rows\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading data: {e}\")\n",
        "    print(\"Please ensure data collection cells have been run successfully.\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvLw8DTo97tq",
        "outputId": "05221462-5bab-4b4d-84d9-9ab12aa39b19"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "DATA PROCESSING NOTEBOOK\n",
            "======================================================================\n",
            "\n",
            "Loading raw data files for processing...\n",
            "\n",
            "‚úÖ Loaded historical_prices from CSV\n",
            "‚úÖ Loaded securities_info from CSV\n",
            "\n",
            "üìä Data Overview:\n",
            "   Date range: 2023-01-03 00:00:00 to 2024-12-31 00:00:00\n",
            "   Unique tickers: 413\n",
            "   Total data points: 207,326\n",
            "   Securities info: 413 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm starting the data processing phase in the same notebook. I load the raw data either from CSV files (if the notebook was restarted) or use the variables from memory (if running continuously). This flexibility ensures the notebook works whether run all at once or in separate sessions. I verify the data is loaded correctly before proceeding with transformations."
      ],
      "metadata": {
        "id": "PM1urvtK_U8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dim_date dimension table\n",
        "# This is a standard date dimension with all necessary date attributes\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CREATING DIMENSION TABLES\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n1. Creating dim_date dimension table...\\n\")\n",
        "\n",
        "# Get date range from historical data\n",
        "start_date = historical_prices['date'].min().date()\n",
        "end_date = historical_prices['date'].max().date()\n",
        "\n",
        "# Generate all dates in range (including weekends for completeness)\n",
        "all_dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "\n",
        "# Create date dimension\n",
        "dim_date_list = []\n",
        "\n",
        "for date in all_dates:\n",
        "    date_key = int(date.strftime('%Y%m%d'))  # YYYYMMDD format as integer\n",
        "\n",
        "    date_row = {\n",
        "        'date_key': date_key,\n",
        "        'date': date.date(),\n",
        "        'year': date.year,\n",
        "        'quarter': date.quarter,\n",
        "        'month': date.month,\n",
        "        'day': date.day,\n",
        "        'day_of_week': date.dayofweek,  # 0=Monday, 6=Sunday\n",
        "        'day_name': date.strftime('%A'),\n",
        "        'month_name': date.strftime('%B'),\n",
        "        'quarter_name': f\"Q{date.quarter} {date.year}\",\n",
        "        'is_weekend': date.weekday() >= 5,  # Saturday=5, Sunday=6\n",
        "        'is_trading_day': date.weekday() < 5,  # Monday-Friday\n",
        "        'is_month_end': date.day == (date + pd.offsets.MonthEnd(0)).day,\n",
        "        'is_quarter_end': date in pd.date_range(start=date, end=date, freq='Q'),\n",
        "        'is_year_end': date.month == 12 and date.day == 31,\n",
        "        'week_of_year': date.isocalendar()[1],\n",
        "        'day_of_year': date.timetuple().tm_yday\n",
        "    }\n",
        "    dim_date_list.append(date_row)\n",
        "\n",
        "dim_date = pd.DataFrame(dim_date_list)\n",
        "\n",
        "print(f\"‚úÖ Created dim_date with {len(dim_date)} rows\")\n",
        "print(f\"   Date range: {dim_date['date'].min()} to {dim_date['date'].max()}\")\n",
        "print(f\"   Trading days: {dim_date['is_trading_day'].sum()}\")\n",
        "print(f\"   Weekends: {dim_date['is_weekend'].sum()}\")\n",
        "\n",
        "# Display sample\n",
        "print(f\"\\nüìã Sample dim_date data:\")\n",
        "print(dim_date.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAiG-9cJ_NrY",
        "outputId": "ccd665ca-8632-4ca5-dcc5-1d2036c4eecb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "CREATING DIMENSION TABLES\n",
            "======================================================================\n",
            "\n",
            "1. Creating dim_date dimension table...\n",
            "\n",
            "‚úÖ Created dim_date with 729 rows\n",
            "   Date range: 2023-01-03 to 2024-12-31\n",
            "   Trading days: 521\n",
            "   Weekends: 208\n",
            "\n",
            "üìã Sample dim_date data:\n",
            "   date_key        date  year  quarter  month  day  day_of_week   day_name  \\\n",
            "0  20230103  2023-01-03  2023        1      1    3            1    Tuesday   \n",
            "1  20230104  2023-01-04  2023        1      1    4            2  Wednesday   \n",
            "2  20230105  2023-01-05  2023        1      1    5            3   Thursday   \n",
            "3  20230106  2023-01-06  2023        1      1    6            4     Friday   \n",
            "4  20230107  2023-01-07  2023        1      1    7            5   Saturday   \n",
            "5  20230108  2023-01-08  2023        1      1    8            6     Sunday   \n",
            "6  20230109  2023-01-09  2023        1      1    9            0     Monday   \n",
            "7  20230110  2023-01-10  2023        1      1   10            1    Tuesday   \n",
            "8  20230111  2023-01-11  2023        1      1   11            2  Wednesday   \n",
            "9  20230112  2023-01-12  2023        1      1   12            3   Thursday   \n",
            "\n",
            "  month_name quarter_name  is_weekend  is_trading_day  is_month_end  \\\n",
            "0    January      Q1 2023       False            True         False   \n",
            "1    January      Q1 2023       False            True         False   \n",
            "2    January      Q1 2023       False            True         False   \n",
            "3    January      Q1 2023       False            True         False   \n",
            "4    January      Q1 2023        True           False         False   \n",
            "5    January      Q1 2023        True           False         False   \n",
            "6    January      Q1 2023       False            True         False   \n",
            "7    January      Q1 2023       False            True         False   \n",
            "8    January      Q1 2023       False            True         False   \n",
            "9    January      Q1 2023       False            True         False   \n",
            "\n",
            "   is_quarter_end  is_year_end  week_of_year  day_of_year  \n",
            "0           False        False             1            3  \n",
            "1           False        False             1            4  \n",
            "2           False        False             1            5  \n",
            "3           False        False             1            6  \n",
            "4           False        False             1            7  \n",
            "5           False        False             1            8  \n",
            "6           False        False             2            9  \n",
            "7           False        False             2           10  \n",
            "8           False        False             2           11  \n",
            "9           False        False             2           12  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I created the dim_date dimension table which is a standard date dimension in data warehousing. This table contains one row for every day in our date range, including weekends. Each row has a date_key (YYYYMMDD integer format for efficient joins), plus various date attributes like year, quarter, month, day of week, and flags for trading days, weekends, month ends, etc. This dimension enables time-based analytics and filtering in our queries."
      ],
      "metadata": {
        "id": "GmM59TX5_gaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dim_time dimension table\n",
        "# This represents time of day for intraday analysis\n",
        "\n",
        "print(\"\\n2. Creating dim_time dimension table...\\n\")\n",
        "\n",
        "# Trading hours: 9:30 AM to 4:00 PM (930 to 1600 in 24-hour format)\n",
        "# We'll create entries for every minute in trading hours\n",
        "\n",
        "trading_start = 930  # 9:30 AM\n",
        "trading_end = 1600    # 4:00 PM\n",
        "\n",
        "time_list = []\n",
        "\n",
        "# Generate time entries for every minute in trading hours\n",
        "for hour in range(9, 16):  # 9 AM to 3 PM\n",
        "    for minute in range(60):\n",
        "        if hour == 9 and minute < 30:  # Skip before 9:30 AM\n",
        "            continue\n",
        "        if hour == 16:  # Stop at 4:00 PM\n",
        "            break\n",
        "\n",
        "        # Create time key (HHMM format as integer)\n",
        "        time_key = hour * 100 + minute\n",
        "\n",
        "        # Create time object\n",
        "        time_obj = pd.Timestamp(f\"2024-01-01 {hour:02d}:{minute:02d}:00\").time()\n",
        "\n",
        "        time_row = {\n",
        "            'time_key': time_key,\n",
        "            'time': str(time_obj),\n",
        "            'hour': hour,\n",
        "            'minute': minute,\n",
        "            'second': 0,\n",
        "            'hour_24': hour,\n",
        "            'hour_12': hour if hour <= 12 else hour - 12,\n",
        "            'am_pm': 'AM' if hour < 12 else 'PM',\n",
        "            'trading_session': 'Pre-Market' if hour < 9 or (hour == 9 and minute < 30)\n",
        "                              else 'Regular' if hour < 16\n",
        "                              else 'After-Hours',\n",
        "            'minute_of_day': hour * 60 + minute,\n",
        "            'is_trading_hours': hour >= 9 and (hour < 16 or (hour == 9 and minute >= 30))\n",
        "        }\n",
        "        time_list.append(time_row)\n",
        "\n",
        "dim_time = pd.DataFrame(time_list)\n",
        "\n",
        "print(f\"‚úÖ Created dim_time with {len(dim_time)} rows\")\n",
        "print(f\"   Time range: {dim_time['time'].min()} to {dim_time['time'].max()}\")\n",
        "print(f\"   Trading hours entries: {dim_time['is_trading_hours'].sum()}\")\n",
        "\n",
        "# Display sample\n",
        "print(f\"\\nüìã Sample dim_time data:\")\n",
        "print(dim_time.head(10))\n",
        "print(f\"\\n...\")\n",
        "print(dim_time.tail(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwwH2RJU_Y2r",
        "outputId": "1aa018d9-e15d-4fc6-aeeb-76054c6c2c8b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2. Creating dim_time dimension table...\n",
            "\n",
            "‚úÖ Created dim_time with 390 rows\n",
            "   Time range: 09:30:00 to 15:59:00\n",
            "   Trading hours entries: 390\n",
            "\n",
            "üìã Sample dim_time data:\n",
            "   time_key      time  hour  minute  second  hour_24  hour_12 am_pm  \\\n",
            "0       930  09:30:00     9      30       0        9        9    AM   \n",
            "1       931  09:31:00     9      31       0        9        9    AM   \n",
            "2       932  09:32:00     9      32       0        9        9    AM   \n",
            "3       933  09:33:00     9      33       0        9        9    AM   \n",
            "4       934  09:34:00     9      34       0        9        9    AM   \n",
            "5       935  09:35:00     9      35       0        9        9    AM   \n",
            "6       936  09:36:00     9      36       0        9        9    AM   \n",
            "7       937  09:37:00     9      37       0        9        9    AM   \n",
            "8       938  09:38:00     9      38       0        9        9    AM   \n",
            "9       939  09:39:00     9      39       0        9        9    AM   \n",
            "\n",
            "  trading_session  minute_of_day  is_trading_hours  \n",
            "0         Regular            570              True  \n",
            "1         Regular            571              True  \n",
            "2         Regular            572              True  \n",
            "3         Regular            573              True  \n",
            "4         Regular            574              True  \n",
            "5         Regular            575              True  \n",
            "6         Regular            576              True  \n",
            "7         Regular            577              True  \n",
            "8         Regular            578              True  \n",
            "9         Regular            579              True  \n",
            "\n",
            "...\n",
            "     time_key      time  hour  minute  second  hour_24  hour_12 am_pm  \\\n",
            "380      1550  15:50:00    15      50       0       15        3    PM   \n",
            "381      1551  15:51:00    15      51       0       15        3    PM   \n",
            "382      1552  15:52:00    15      52       0       15        3    PM   \n",
            "383      1553  15:53:00    15      53       0       15        3    PM   \n",
            "384      1554  15:54:00    15      54       0       15        3    PM   \n",
            "385      1555  15:55:00    15      55       0       15        3    PM   \n",
            "386      1556  15:56:00    15      56       0       15        3    PM   \n",
            "387      1557  15:57:00    15      57       0       15        3    PM   \n",
            "388      1558  15:58:00    15      58       0       15        3    PM   \n",
            "389      1559  15:59:00    15      59       0       15        3    PM   \n",
            "\n",
            "    trading_session  minute_of_day  is_trading_hours  \n",
            "380         Regular            950              True  \n",
            "381         Regular            951              True  \n",
            "382         Regular            952              True  \n",
            "383         Regular            953              True  \n",
            "384         Regular            954              True  \n",
            "385         Regular            955              True  \n",
            "386         Regular            956              True  \n",
            "387         Regular            957              True  \n",
            "388         Regular            958              True  \n",
            "389         Regular            959              True  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I created the dim_time dimension table for intraday time analysis. This table contains time entries for trading hours (9:30 AM to 4:00 PM) at minute-level granularity. Each row has a time_key (HHMM integer format), time attributes (hour, minute, AM/PM), and trading session classification. While we're using daily data primarily, this dimension supports future intraday analysis and is part of our complete dimensional model."
      ],
      "metadata": {
        "id": "ogqGwsoG_sQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dim_security dimension table\n",
        "# This includes SCD Type 2 support for tracking changes over time\n",
        "\n",
        "print(\"\\n3. Creating dim_security dimension table...\\n\")\n",
        "\n",
        "# Start with securities info from data collection\n",
        "dim_security_list = []\n",
        "\n",
        "# Get unique list of tickers that have data\n",
        "tickers_with_data = historical_prices['ticker'].unique()\n",
        "\n",
        "for idx, row in securities_info.iterrows():\n",
        "    ticker = row['ticker']\n",
        "\n",
        "    # Only include tickers that have price data\n",
        "    if ticker not in tickers_with_data:\n",
        "        continue\n",
        "\n",
        "    # Create security key (surrogate key)\n",
        "    security_key = idx + 1\n",
        "\n",
        "    # For SCD Type 2, we'll set effective dates\n",
        "    # In a real scenario, we'd track changes, but for initial load:\n",
        "    effective_date = historical_prices[historical_prices['ticker'] == ticker]['date'].min().date()\n",
        "\n",
        "    # Use a date within pandas' range (max is 2262-04-11, but 2099-12-31 is safer and standard)\n",
        "    expiry_date = datetime(2099, 12, 31).date()  # Far future date for current records (within pandas range)\n",
        "    is_current = True\n",
        "    version = 1\n",
        "\n",
        "    security_row = {\n",
        "        'security_key': security_key,\n",
        "        'security_id': ticker,  # Natural key (ticker symbol)\n",
        "        'ticker_symbol': ticker,\n",
        "        'security_name': row['company_name'] if pd.notna(row['company_name']) else ticker,\n",
        "        'asset_class': 'EQUITY',  # All our data is equities\n",
        "        'sector': row['sector'] if pd.notna(row['sector']) else 'Unknown',\n",
        "        'industry': row['industry'] if pd.notna(row['industry']) else 'Unknown',\n",
        "        'currency_code': row['currency'] if pd.notna(row['currency']) else 'USD',\n",
        "        'exchange_listed': row['exchange'] if pd.notna(row['exchange']) else 'Unknown',\n",
        "        'market_cap': int(row['market_cap']) if pd.notna(row['market_cap']) else 0,\n",
        "        'lot_size': 1,  # Standard lot size\n",
        "        'tick_size': 0.01,  # Standard tick size for most stocks\n",
        "        'multiplier': 1.0,  # For equities\n",
        "        'expiry_date': None,  # Only for derivatives\n",
        "        'strike_price': None,  # Only for options\n",
        "        'option_type': None,  # Only for options\n",
        "        'underlying_security_key': None,  # Only for derivatives\n",
        "        'effective_date': effective_date,  # SCD Type 2\n",
        "        'expiry_date_scd': expiry_date,  # SCD Type 2\n",
        "        'is_current': is_current,  # SCD Type 2\n",
        "        'version': version,  # SCD Type 2\n",
        "        'is_active': True,\n",
        "        'last_price': 0,  # Initialize to 0, will be updated\n",
        "        'created_at': datetime.now()\n",
        "    }\n",
        "    dim_security_list.append(security_row)\n",
        "\n",
        "dim_security = pd.DataFrame(dim_security_list)\n",
        "\n",
        "# Update last_price from most recent price data\n",
        "print(\"   Updating last prices from historical data...\")\n",
        "latest_prices = historical_prices.groupby('ticker')['close_price'].last().reset_index()\n",
        "latest_prices.columns = ['ticker_symbol', 'last_price_new']  # Use different column name\n",
        "\n",
        "# Merge and update\n",
        "dim_security = dim_security.merge(latest_prices, on='ticker_symbol', how='left')\n",
        "dim_security['last_price'] = dim_security['last_price_new'].fillna(dim_security['last_price'])\n",
        "dim_security = dim_security.drop('last_price_new', axis=1)  # Drop the temporary column\n",
        "\n",
        "print(f\"‚úÖ Created dim_security with {len(dim_security)} rows\")\n",
        "print(f\"   Unique sectors: {dim_security['sector'].nunique()}\")\n",
        "print(f\"   Unique industries: {dim_security['industry'].nunique()}\")\n",
        "print(f\"   Unique exchanges: {dim_security['exchange_listed'].nunique()}\")\n",
        "\n",
        "# Display sample\n",
        "print(f\"\\nüìã Sample dim_security data:\")\n",
        "print(dim_security[['security_key', 'ticker_symbol', 'security_name', 'sector', 'industry', 'exchange_listed', 'last_price']].head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSYTmvM3_mZN",
        "outputId": "8e686b33-83e6-4a44-bfe8-808fbd5eeaae"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "3. Creating dim_security dimension table...\n",
            "\n",
            "   Updating last prices from historical data...\n",
            "‚úÖ Created dim_security with 413 rows\n",
            "   Unique sectors: 12\n",
            "   Unique industries: 75\n",
            "   Unique exchanges: 6\n",
            "\n",
            "üìã Sample dim_security data:\n",
            "   security_key ticker_symbol                    security_name  \\\n",
            "0             1          AAPL                       Apple Inc.   \n",
            "1             2          ABBV                      AbbVie Inc.   \n",
            "2             3          ABCB                   Ameris Bancorp   \n",
            "3             4           ABG    Asbury Automotive Group, Inc.   \n",
            "4             5          ABNB                     Airbnb, Inc.   \n",
            "5             6           ABT              Abbott Laboratories   \n",
            "6             7          ADBE                       Adobe Inc.   \n",
            "7             8           ADM   Archer-Daniels-Midland Company   \n",
            "8             9           AEE               Ameren Corporation   \n",
            "9            10           AEO  American Eagle Outfitters, Inc.   \n",
            "\n",
            "               sector                        industry exchange_listed  \\\n",
            "0          Technology            Consumer Electronics             NMS   \n",
            "1          Healthcare    Drug Manufacturers - General             NYQ   \n",
            "2  Financial Services                Banks - Regional             NYQ   \n",
            "3   Consumer Cyclical        Auto & Truck Dealerships             NYQ   \n",
            "4   Consumer Cyclical                 Travel Services             NMS   \n",
            "5          Healthcare                 Medical Devices             NYQ   \n",
            "6          Technology          Software - Application             NMS   \n",
            "7  Consumer Defensive                   Farm Products             NYQ   \n",
            "8           Utilities  Utilities - Regulated Electric             NYQ   \n",
            "9   Consumer Cyclical                  Apparel Retail             NYQ   \n",
            "\n",
            "   last_price  \n",
            "0  249.292496  \n",
            "1  171.696503  \n",
            "2   61.991486  \n",
            "3  243.029999  \n",
            "4  131.410004  \n",
            "5  111.006607  \n",
            "6  444.679993  \n",
            "7   49.032490  \n",
            "8   87.223518  \n",
            "9   16.015043  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I fixed the merge issue by initializing last_price to 0 in the initial row creation, then using a temporary column name (last_price_new) during the merge to avoid column name conflicts. After merging, I update the last_price column with the new values and drop the temporary column. This ensures we have the most recent closing price for each security while handling cases where a ticker might not have price data."
      ],
      "metadata": {
        "id": "iM2M5GUyA-rB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create remaining dimension tables\n",
        "# These are simpler dimensions that don't require complex processing\n",
        "\n",
        "print(\"\\n4. Creating additional dimension tables...\\n\")\n",
        "\n",
        "# 1. dim_asset_class\n",
        "print(\"   a. Creating dim_asset_class...\")\n",
        "dim_asset_class = pd.DataFrame([\n",
        "    {'asset_class_key': 1, 'asset_class_code': 'EQUITY', 'asset_class_name': 'Equity', 'description': 'Common Stock'},\n",
        "    {'asset_class_key': 2, 'asset_class_code': 'OPTION', 'asset_class_name': 'Option', 'description': 'Stock Option'},\n",
        "    {'asset_class_key': 3, 'asset_class_code': 'FUTURE', 'asset_class_name': 'Future', 'description': 'Futures Contract'},\n",
        "    {'asset_class_key': 4, 'asset_class_code': 'BOND', 'asset_class_name': 'Bond', 'description': 'Corporate Bond'},\n",
        "])\n",
        "print(f\"      ‚úÖ Created dim_asset_class with {len(dim_asset_class)} rows\")\n",
        "\n",
        "# 2. dim_sector (extracted from securities)\n",
        "print(\"   b. Creating dim_sector...\")\n",
        "unique_sectors = dim_security['sector'].unique()\n",
        "dim_sector_list = []\n",
        "for idx, sector in enumerate(unique_sectors, 1):\n",
        "    dim_sector_list.append({\n",
        "        'sector_key': idx,\n",
        "        'sector_code': sector.upper().replace(' ', '_')[:20],  # Clean code\n",
        "        'sector_name': sector,\n",
        "        'description': f'{sector} sector'\n",
        "    })\n",
        "dim_sector = pd.DataFrame(dim_sector_list)\n",
        "print(f\"      ‚úÖ Created dim_sector with {len(dim_sector)} rows\")\n",
        "\n",
        "# 3. dim_trader (simulated traders)\n",
        "print(\"   c. Creating dim_trader (simulated)...\")\n",
        "# Create 50 simulated traders across 5 desks\n",
        "desks = ['Equities', 'Derivatives', 'Fixed Income', 'Commodities', 'FX']\n",
        "trader_names = [\n",
        "    'John Smith', 'Sarah Johnson', 'Michael Brown', 'Emily Davis', 'David Wilson',\n",
        "    'Jessica Martinez', 'Christopher Anderson', 'Amanda Taylor', 'Matthew Thomas', 'Ashley Jackson',\n",
        "    'Daniel White', 'Stephanie Harris', 'Andrew Martin', 'Nicole Thompson', 'Joshua Garcia',\n",
        "    'Michelle Martinez', 'Ryan Robinson', 'Lauren Clark', 'Kevin Rodriguez', 'Samantha Lewis',\n",
        "    'Brandon Lee', 'Rachel Walker', 'Tyler Hall', 'Megan Allen', 'Justin Young',\n",
        "    'Brittany King', 'Nathan Wright', 'Kayla Lopez', 'Jordan Hill', 'Taylor Scott',\n",
        "    'Austin Green', 'Morgan Adams', 'Cameron Baker', 'Jordan Gonzalez', 'Alex Nelson',\n",
        "    'Casey Carter', 'Jamie Mitchell', 'Riley Perez', 'Avery Roberts', 'Quinn Turner',\n",
        "    'Dakota Phillips', 'Skylar Campbell', 'River Parker', 'Phoenix Evans', 'Sage Edwards',\n",
        "    'Rowan Collins', 'Quinn Stewart', 'Sage Sanchez', 'River Morris', 'Phoenix Rogers'\n",
        "]\n",
        "\n",
        "dim_trader_list = []\n",
        "for idx, name in enumerate(trader_names[:50], 1):\n",
        "    desk = desks[idx % len(desks)]\n",
        "    trader_row = {\n",
        "        'trader_key': idx,\n",
        "        'trader_id': f'TR{idx:03d}',\n",
        "        'full_name': name,\n",
        "        'desk_name': desk,\n",
        "        'authorization_level': np.random.randint(1, 6),  # 1-5 risk levels\n",
        "        'trader_type': np.random.choice(['Proprietary', 'Agency'], p=[0.7, 0.3]),\n",
        "        'certifications': 'Series 7, Series 63',  # Simplified as string\n",
        "        'compliance_status': 'Active',\n",
        "        'effective_date': datetime(2023, 1, 1).date(),  # SCD Type 2\n",
        "        'expiry_date': datetime(2099, 12, 31).date(),  # SCD Type 2\n",
        "        'is_current': True,  # SCD Type 2\n",
        "        'version': 1  # SCD Type 2\n",
        "    }\n",
        "    dim_trader_list.append(trader_row)\n",
        "\n",
        "dim_trader = pd.DataFrame(dim_trader_list)\n",
        "print(f\"      ‚úÖ Created dim_trader with {len(dim_trader)} rows\")\n",
        "\n",
        "# 4. dim_account (simulated accounts)\n",
        "print(\"   d. Creating dim_account (simulated)...\")\n",
        "account_types = ['Individual', 'Institutional', 'Proprietary']\n",
        "risk_profiles = ['Conservative', 'Moderate', 'Aggressive']\n",
        "\n",
        "dim_account_list = []\n",
        "for idx in range(1, 201):  # 200 accounts\n",
        "    account_row = {\n",
        "        'account_key': idx,\n",
        "        'account_number': f'ACC{idx:06d}',\n",
        "        'account_name': f'Account {idx}',\n",
        "        'account_type': np.random.choice(account_types),\n",
        "        'parent_account_key': None if idx <= 10 else np.random.randint(1, 11),  # Hierarchy\n",
        "        'account_level': 1 if idx <= 10 else 2,\n",
        "        'risk_profile': np.random.choice(risk_profiles),\n",
        "        'margin_limit': np.random.uniform(100000, 10000000),\n",
        "        'cash_balance': np.random.uniform(50000, 5000000),\n",
        "        'total_equity': 0,  # Will be calculated\n",
        "        'kyc_status': 'Verified',\n",
        "        'kyc_expiry_date': datetime(2025, 12, 31).date(),\n",
        "        'opening_date': datetime(2022, 1, 1).date() + timedelta(days=np.random.randint(0, 365)),\n",
        "        'is_active': True\n",
        "    }\n",
        "    dim_account_list.append(account_row)\n",
        "\n",
        "dim_account = pd.DataFrame(dim_account_list)\n",
        "print(f\"      ‚úÖ Created dim_account with {len(dim_account)} rows\")\n",
        "\n",
        "# 5. dim_exchange\n",
        "print(\"   e. Creating dim_exchange...\")\n",
        "exchanges = securities_info['exchange'].value_counts().head(15).index.tolist()\n",
        "dim_exchange_list = []\n",
        "for idx, exchange in enumerate(exchanges, 1):\n",
        "    exchange_row = {\n",
        "        'exchange_key': idx,\n",
        "        'exchange_code': exchange,\n",
        "        'exchange_name': exchange,\n",
        "        'country': 'US',\n",
        "        'trading_hours': '09:30-16:00 ET',\n",
        "        'settlement_cycle': 'T+2'\n",
        "    }\n",
        "    dim_exchange_list.append(exchange_row)\n",
        "\n",
        "dim_exchange = pd.DataFrame(dim_exchange_list)\n",
        "print(f\"      ‚úÖ Created dim_exchange with {len(dim_exchange)} rows\")\n",
        "\n",
        "# 6. dim_counterparty (simulated)\n",
        "print(\"   f. Creating dim_counterparty (simulated)...\")\n",
        "counterparties = ['Goldman Sachs', 'Morgan Stanley', 'JPMorgan', 'Citigroup', 'Bank of America',\n",
        "                  'Credit Suisse', 'Deutsche Bank', 'UBS', 'Barclays', 'Wells Fargo',\n",
        "                  'Interactive Brokers', 'Charles Schwab', 'TD Ameritrade', 'E*TRADE', 'Fidelity',\n",
        "                  'Vanguard', 'BlackRock', 'State Street', 'BNY Mellon', 'Northern Trust',\n",
        "                  'Raymond James', 'Edward Jones', 'LPL Financial', 'Ameriprise', 'Stifel',\n",
        "                  'RBC Capital', 'BMO Capital', 'CIBC', 'Scotiabank', 'TD Securities']\n",
        "\n",
        "dim_counterparty_list = []\n",
        "for idx, name in enumerate(counterparties, 1):\n",
        "    counterparty_row = {\n",
        "        'counterparty_key': idx,\n",
        "        'counterparty_id': f'CP{idx:03d}',\n",
        "        'counterparty_name': name,\n",
        "        'credit_rating': np.random.choice(['AAA', 'AA', 'A', 'BBB', 'BB'], p=[0.1, 0.2, 0.3, 0.3, 0.1]),\n",
        "        'exposure_limit': np.random.uniform(1000000, 100000000)\n",
        "    }\n",
        "    dim_counterparty_list.append(counterparty_row)\n",
        "\n",
        "dim_counterparty = pd.DataFrame(dim_counterparty_list)\n",
        "print(f\"      ‚úÖ Created dim_counterparty with {len(dim_counterparty)} rows\")\n",
        "\n",
        "# 7. dim_strategy (simulated)\n",
        "print(\"   g. Creating dim_strategy (simulated)...\")\n",
        "strategies = ['Momentum', 'Mean Reversion', 'Pairs Trading', 'Statistical Arbitrage',\n",
        "              'Market Making', 'High Frequency', 'Swing Trading', 'Day Trading',\n",
        "              'Value Investing', 'Growth Investing', 'Index Arbitrage', 'Volatility Trading']\n",
        "\n",
        "dim_strategy_list = []\n",
        "for idx, strategy in enumerate(strategies, 1):\n",
        "    strategy_row = {\n",
        "        'strategy_key': idx,\n",
        "        'strategy_id': f'STR{idx:02d}',\n",
        "        'strategy_name': strategy,\n",
        "        'strategy_type': 'Algorithmic' if idx <= 6 else 'Discretionary',\n",
        "        'risk_parameters': f'Max drawdown: {np.random.randint(5, 20)}%'\n",
        "    }\n",
        "    dim_strategy_list.append(strategy_row)\n",
        "\n",
        "dim_strategy = pd.DataFrame(dim_strategy_list)\n",
        "print(f\"      ‚úÖ Created dim_strategy with {len(dim_strategy)} rows\")\n",
        "\n",
        "# 8. dim_trade_attributes (junk dimension)\n",
        "print(\"   h. Creating dim_trade_attributes (junk dimension)...\")\n",
        "# Create all combinations of trade flags\n",
        "attributes_list = []\n",
        "attr_id = 1\n",
        "for is_alg in [True, False]:\n",
        "    for is_day in [True, False]:\n",
        "        for requires_review in [True, False]:\n",
        "            for settlement_status in ['Pending', 'Settled', 'Failed']:\n",
        "                attr_row = {\n",
        "                    'attributes_key': attr_id,\n",
        "                    'is_algorithmic': is_alg,\n",
        "                    'is_day_trade': is_day,\n",
        "                    'requires_review': requires_review,\n",
        "                    'settlement_status': settlement_status\n",
        "                }\n",
        "                attributes_list.append(attr_row)\n",
        "                attr_id += 1\n",
        "\n",
        "dim_trade_attributes = pd.DataFrame(attributes_list)\n",
        "print(f\"      ‚úÖ Created dim_trade_attributes with {len(dim_trade_attributes)} rows\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ ALL DIMENSION TABLES CREATED SUCCESSFULLY!\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"   dim_date: {len(dim_date)} rows\")\n",
        "print(f\"   dim_time: {len(dim_time)} rows\")\n",
        "print(f\"   dim_security: {len(dim_security)} rows\")\n",
        "print(f\"   dim_asset_class: {len(dim_asset_class)} rows\")\n",
        "print(f\"   dim_sector: {len(dim_sector)} rows\")\n",
        "print(f\"   dim_trader: {len(dim_trader)} rows\")\n",
        "print(f\"   dim_account: {len(dim_account)} rows\")\n",
        "print(f\"   dim_exchange: {len(dim_exchange)} rows\")\n",
        "print(f\"   dim_counterparty: {len(dim_counterparty)} rows\")\n",
        "print(f\"   dim_strategy: {len(dim_strategy)} rows\")\n",
        "print(f\"   dim_trade_attributes: {len(dim_trade_attributes)} rows\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iof6Xggy_xJI",
        "outputId": "9afe6e57-7f06-490f-e224-a4d1a44d911b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "4. Creating additional dimension tables...\n",
            "\n",
            "   a. Creating dim_asset_class...\n",
            "      ‚úÖ Created dim_asset_class with 4 rows\n",
            "   b. Creating dim_sector...\n",
            "      ‚úÖ Created dim_sector with 12 rows\n",
            "   c. Creating dim_trader (simulated)...\n",
            "      ‚úÖ Created dim_trader with 50 rows\n",
            "   d. Creating dim_account (simulated)...\n",
            "      ‚úÖ Created dim_account with 200 rows\n",
            "   e. Creating dim_exchange...\n",
            "      ‚úÖ Created dim_exchange with 6 rows\n",
            "   f. Creating dim_counterparty (simulated)...\n",
            "      ‚úÖ Created dim_counterparty with 30 rows\n",
            "   g. Creating dim_strategy (simulated)...\n",
            "      ‚úÖ Created dim_strategy with 12 rows\n",
            "   h. Creating dim_trade_attributes (junk dimension)...\n",
            "      ‚úÖ Created dim_trade_attributes with 24 rows\n",
            "\n",
            "======================================================================\n",
            "‚úÖ ALL DIMENSION TABLES CREATED SUCCESSFULLY!\n",
            "======================================================================\n",
            "\n",
            "Summary:\n",
            "   dim_date: 729 rows\n",
            "   dim_time: 390 rows\n",
            "   dim_security: 413 rows\n",
            "   dim_asset_class: 4 rows\n",
            "   dim_sector: 12 rows\n",
            "   dim_trader: 50 rows\n",
            "   dim_account: 200 rows\n",
            "   dim_exchange: 6 rows\n",
            "   dim_counterparty: 30 rows\n",
            "   dim_strategy: 12 rows\n",
            "   dim_trade_attributes: 24 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I created all the remaining dimension tables. These include simple lookup tables (asset_class, sector, exchange) and simulated tables for traders, accounts, counterparties, and strategies. The dim_trader and dim_account tables include realistic hierarchies and attributes. The dim_trade_attributes is a 'junk dimension' that combines multiple boolean flags into a single dimension to reduce the width of our fact table. All these dimensions will be used to create foreign keys in our fact tables."
      ],
      "metadata": {
        "id": "W2GHXGegBO0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate realistic trade data from historical prices\n",
        "# This simulates trading activity based on actual price movements and volumes\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"GENERATING TRADE DATA\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nGenerating realistic trade data from historical prices...\")\n",
        "print(\"This simulates trading activity based on actual market data...\\n\")\n",
        "print(\"‚ö†Ô∏è This will take 5-10 minutes for 413 tickers...\\n\")\n",
        "\n",
        "def generate_trades_from_prices(price_data, securities_df, dim_trader, dim_account,\n",
        "                                dim_exchange, dim_counterparty, dim_strategy, dim_trade_attributes):\n",
        "    \"\"\"\n",
        "    Generate realistic trade transactions from historical price data\n",
        "    \"\"\"\n",
        "    trades_list = []\n",
        "    trade_id = 1\n",
        "\n",
        "    # Get mappings for foreign keys\n",
        "    security_map = dict(zip(securities_df['ticker_symbol'], securities_df['security_key']))\n",
        "    trader_keys = dim_trader['trader_key'].tolist()\n",
        "    account_keys = dim_account['account_key'].tolist()\n",
        "    exchange_keys = dim_exchange['exchange_key'].tolist()\n",
        "    counterparty_keys = dim_counterparty['counterparty_key'].tolist()\n",
        "    strategy_keys = dim_strategy['strategy_key'].tolist()\n",
        "    attributes_keys = dim_trade_attributes['attributes_key'].tolist()\n",
        "\n",
        "    # Process each ticker\n",
        "    tickers = price_data['ticker'].unique()\n",
        "    total_tickers = len(tickers)\n",
        "\n",
        "    for ticker_idx, ticker in enumerate(tickers):\n",
        "        if (ticker_idx + 1) % 50 == 0:\n",
        "            print(f\"  Processing ticker {ticker_idx + 1}/{total_tickers}...\")\n",
        "\n",
        "        ticker_data = price_data[price_data['ticker'] == ticker].sort_values('date')\n",
        "        security_key = security_map.get(ticker)\n",
        "\n",
        "        if security_key is None:\n",
        "            continue\n",
        "\n",
        "        # For each trading day, generate multiple trades\n",
        "        for _, day_data in ticker_data.iterrows():\n",
        "            date = day_data['date']\n",
        "            close_price = day_data['close_price']\n",
        "            volume = day_data['volume']\n",
        "\n",
        "            # Skip if no volume or price\n",
        "            if pd.isna(volume) or volume == 0 or pd.isna(close_price):\n",
        "                continue\n",
        "\n",
        "            # Generate number of trades based on volume\n",
        "            # Higher volume = more trades, but cap at reasonable number\n",
        "            volume_factor = min(volume / 1000000, 50)  # Scale based on volume\n",
        "            num_trades = max(1, int(np.random.poisson(volume_factor)))\n",
        "            num_trades = min(num_trades, 20)  # Cap at 20 trades per day per ticker\n",
        "\n",
        "            # Generate trades for this day\n",
        "            for trade_num in range(num_trades):\n",
        "                # Random time during trading hours (9:30 AM - 4:00 PM)\n",
        "                hour = np.random.randint(9, 16)\n",
        "                minute = np.random.randint(0, 60)\n",
        "                if hour == 9 and minute < 30:\n",
        "                    minute = 30\n",
        "                if hour == 16:\n",
        "                    hour = 15\n",
        "                    minute = 59\n",
        "\n",
        "                trade_timestamp = pd.Timestamp(date).replace(hour=hour, minute=minute, second=np.random.randint(0, 60))\n",
        "\n",
        "                # Trade type (BUY or SELL)\n",
        "                trade_type = np.random.choice(['BUY', 'SELL'], p=[0.5, 0.5])\n",
        "\n",
        "                # Quantity (realistic share amounts)\n",
        "                quantity = np.random.choice([100, 200, 500, 1000, 2000, 5000],\n",
        "                                          p=[0.3, 0.25, 0.2, 0.15, 0.07, 0.03])\n",
        "\n",
        "                # Price (slight variation from close price)\n",
        "                price_variation = np.random.uniform(-0.02, 0.02)  # ¬±2% variation\n",
        "                price = close_price * (1 + price_variation)\n",
        "                price = round(price, 2)\n",
        "\n",
        "                # Calculate trade value\n",
        "                trade_value = quantity * price\n",
        "\n",
        "                # Commission (typical broker fee)\n",
        "                commission = max(1.0, trade_value * 0.001)  # 0.1% or $1 minimum\n",
        "\n",
        "                # Net proceeds\n",
        "                net_proceeds = trade_value - commission if trade_type == 'SELL' else -(trade_value + commission)\n",
        "\n",
        "                # Date and time keys\n",
        "                date_key = int(date.strftime('%Y%m%d'))\n",
        "                time_key = hour * 100 + minute\n",
        "\n",
        "                # Random foreign keys\n",
        "                trader_key = np.random.choice(trader_keys)\n",
        "                account_key = np.random.choice(account_keys)\n",
        "                exchange_key = np.random.choice(exchange_keys)\n",
        "                counterparty_key = np.random.choice(counterparty_keys)\n",
        "                strategy_key = np.random.choice(strategy_keys)\n",
        "                attributes_key = np.random.choice(attributes_keys)\n",
        "\n",
        "                # Settlement date (T+2 for stocks)\n",
        "                settlement_date = date + pd.Timedelta(days=2)\n",
        "\n",
        "                trade_row = {\n",
        "                    'trade_timestamp': trade_timestamp,\n",
        "                    'date_key': date_key,\n",
        "                    'time_key': time_key,\n",
        "                    'security_key': security_key,\n",
        "                    'trader_key': trader_key,\n",
        "                    'account_key': account_key,\n",
        "                    'exchange_key': exchange_key,\n",
        "                    'counterparty_key': counterparty_key,\n",
        "                    'strategy_key': strategy_key,\n",
        "                    'attributes_key': attributes_key,\n",
        "                    'trade_type': trade_type,\n",
        "                    'quantity': quantity,\n",
        "                    'price': price,\n",
        "                    'trade_value': round(trade_value, 2),\n",
        "                    'commission': round(commission, 2),\n",
        "                    'net_proceeds': round(net_proceeds, 2),\n",
        "                    'realized_pnl': None,  # Will be calculated when position closes\n",
        "                    'portfolio_exposure': round(trade_value, 2),\n",
        "                    'margin_used': 0 if trade_type in ['BUY', 'SELL'] else round(trade_value * 0.5, 2),\n",
        "                    'order_id': f'ORD{trade_id:08d}',\n",
        "                    'execution_venue': np.random.choice(['NYSE', 'NASDAQ', 'Dark Pool']),\n",
        "                    'settlement_date': settlement_date.date(),\n",
        "                    'created_at': datetime.now()\n",
        "                }\n",
        "\n",
        "                trades_list.append(trade_row)\n",
        "                trade_id += 1\n",
        "\n",
        "    return pd.DataFrame(trades_list)\n",
        "\n",
        "# Generate trades (this will take several minutes)\n",
        "print(\"Generating trades... This may take 5-10 minutes...\\n\")\n",
        "fact_trades = generate_trades_from_prices(\n",
        "    historical_prices,\n",
        "    dim_security,\n",
        "    dim_trader,\n",
        "    dim_account,\n",
        "    dim_exchange,\n",
        "    dim_counterparty,\n",
        "    dim_strategy,\n",
        "    dim_trade_attributes\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Generated {len(fact_trades):,} trade records\")\n",
        "print(f\"   Date range: {fact_trades['trade_timestamp'].min()} to {fact_trades['trade_timestamp'].max()}\")\n",
        "print(f\"   Total trade value: ${fact_trades['trade_value'].sum():,.2f}\")\n",
        "print(f\"   Average trade size: ${fact_trades['trade_value'].mean():,.2f}\")\n",
        "\n",
        "# Display sample\n",
        "print(f\"\\nüìã Sample trade data:\")\n",
        "print(fact_trades[['trade_timestamp', 'security_key', 'trade_type', 'quantity', 'price', 'trade_value']].head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfvcmTArBJbw",
        "outputId": "c7126e94-5dd1-4709-e661-9ab6e5149014"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "GENERATING TRADE DATA\n",
            "======================================================================\n",
            "\n",
            "Generating realistic trade data from historical prices...\n",
            "This simulates trading activity based on actual market data...\n",
            "\n",
            "‚ö†Ô∏è This will take 5-10 minutes for 413 tickers...\n",
            "\n",
            "Generating trades... This may take 5-10 minutes...\n",
            "\n",
            "  Processing ticker 50/413...\n",
            "  Processing ticker 100/413...\n",
            "  Processing ticker 150/413...\n",
            "  Processing ticker 200/413...\n",
            "  Processing ticker 250/413...\n",
            "  Processing ticker 300/413...\n",
            "  Processing ticker 350/413...\n",
            "  Processing ticker 400/413...\n",
            "\n",
            "‚úÖ Generated 961,100 trade records\n",
            "   Date range: 2023-01-03 09:30:00 to 2024-12-31 15:59:52\n",
            "   Total trade value: $59,932,790,167.00\n",
            "   Average trade size: $62,358.54\n",
            "\n",
            "üìã Sample trade data:\n",
            "      trade_timestamp  security_key trade_type  quantity   price  trade_value\n",
            "0 2023-01-03 11:42:08             1        BUY       100  124.13      12413.0\n",
            "1 2023-01-03 13:29:35             1        BUY       500  121.52      60760.0\n",
            "2 2023-01-03 14:13:41             1       SELL       200  120.83      24166.0\n",
            "3 2023-01-03 12:14:05             1       SELL       100  124.31      12431.0\n",
            "4 2023-01-03 11:11:42             1       SELL       100  124.47      12447.0\n",
            "5 2023-01-03 14:32:51             1        BUY       200  123.55      24710.0\n",
            "6 2023-01-03 15:12:46             1       SELL       200  120.94      24188.0\n",
            "7 2023-01-03 09:38:03             1       SELL       100  123.94      12394.0\n",
            "8 2023-01-03 11:57:32             1       SELL      1000  124.37     124370.0\n",
            "9 2023-01-03 10:22:31             1        BUY       200  122.95      24590.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I generated realistic trade data from the historical price data. This function simulates trading activity by creating multiple trades per day based on actual trading volumes - higher volume days generate more trades. Each trade has realistic attributes including random timestamps during trading hours, quantities (100-5000 shares), prices with slight variation from closing prices, and proper foreign key relationships to all dimension tables. The trades are distributed across different traders, accounts, strategies, and counterparties to create a realistic trading scenario. This gives us the fact_trades data that will be loaded into our partitioned fact table in Supabase."
      ],
      "metadata": {
        "id": "ocFiZDghDZXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate realized PnL for closed positions\n",
        "# This simulates position closing and profit/loss calculation\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CALCULATING REALIZED PnL\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nCalculating realized PnL for closed positions...\")\n",
        "print(\"This simulates position closing based on trade pairs...\\n\")\n",
        "\n",
        "# Sort trades by account, security, and timestamp\n",
        "fact_trades_sorted = fact_trades.sort_values(['account_key', 'security_key', 'trade_timestamp']).copy()\n",
        "\n",
        "# Track positions per account per security\n",
        "positions = {}  # {(account_key, security_key): [list of trades]}\n",
        "\n",
        "realized_pnl_list = []\n",
        "\n",
        "for idx, trade in fact_trades_sorted.iterrows():\n",
        "    key = (trade['account_key'], trade['security_key'])\n",
        "\n",
        "    if key not in positions:\n",
        "        positions[key] = []\n",
        "\n",
        "    if trade['trade_type'] == 'BUY':\n",
        "        # Add to position\n",
        "        positions[key].append({\n",
        "            'trade_id': idx,\n",
        "            'quantity': trade['quantity'],\n",
        "            'price': trade['price'],\n",
        "            'timestamp': trade['trade_timestamp']\n",
        "        })\n",
        "    elif trade['trade_type'] == 'SELL':\n",
        "        # Try to match with existing positions (FIFO - First In First Out)\n",
        "        remaining_sell_qty = trade['quantity']\n",
        "        sell_price = trade['price']\n",
        "\n",
        "        while remaining_sell_qty > 0 and len(positions[key]) > 0:\n",
        "            # Get oldest position\n",
        "            oldest_position = positions[key][0]\n",
        "\n",
        "            if oldest_position['quantity'] <= remaining_sell_qty:\n",
        "                # Close entire position\n",
        "                closed_qty = oldest_position['quantity']\n",
        "                cost_basis = oldest_position['price']\n",
        "                realized_pnl = (sell_price - cost_basis) * closed_qty\n",
        "\n",
        "                realized_pnl_list.append({\n",
        "                    'trade_id': idx,  # The SELL trade\n",
        "                    'realized_pnl': realized_pnl,\n",
        "                    'closed_quantity': closed_qty\n",
        "                })\n",
        "\n",
        "                remaining_sell_qty -= closed_qty\n",
        "                positions[key].pop(0)  # Remove closed position\n",
        "            else:\n",
        "                # Partial close\n",
        "                closed_qty = remaining_sell_qty\n",
        "                cost_basis = oldest_position['price']\n",
        "                realized_pnl = (sell_price - cost_basis) * closed_qty\n",
        "\n",
        "                realized_pnl_list.append({\n",
        "                    'trade_id': idx,\n",
        "                    'realized_pnl': realized_pnl,\n",
        "                    'closed_quantity': closed_qty\n",
        "                })\n",
        "\n",
        "                # Update remaining position\n",
        "                positions[key][0]['quantity'] -= closed_qty\n",
        "                remaining_sell_qty = 0\n",
        "\n",
        "# Update fact_trades with realized PnL\n",
        "realized_pnl_df = pd.DataFrame(realized_pnl_list)\n",
        "if len(realized_pnl_df) > 0:\n",
        "    # Aggregate PnL per trade (in case multiple positions closed)\n",
        "    realized_pnl_agg = realized_pnl_df.groupby('trade_id')['realized_pnl'].sum().reset_index()\n",
        "    realized_pnl_agg.columns = ['trade_index', 'realized_pnl']\n",
        "\n",
        "    # Map back to fact_trades\n",
        "    fact_trades.loc[realized_pnl_agg['trade_index'], 'realized_pnl'] = realized_pnl_agg['realized_pnl'].values\n",
        "\n",
        "    print(f\"‚úÖ Calculated realized PnL for {len(realized_pnl_agg)} closed positions\")\n",
        "    print(f\"   Total realized PnL: ${realized_pnl_agg['realized_pnl'].sum():,.2f}\")\n",
        "    print(f\"   Average PnL per closed position: ${realized_pnl_agg['realized_pnl'].mean():,.2f}\")\n",
        "    print(f\"   Profitable trades: {(realized_pnl_agg['realized_pnl'] > 0).sum()}\")\n",
        "    print(f\"   Losing trades: {(realized_pnl_agg['realized_pnl'] < 0).sum()}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No closed positions found (this is normal if all trades are new)\")\n",
        "\n",
        "# Display statistics\n",
        "print(f\"\\nüìä Trade Statistics:\")\n",
        "print(f\"   Total trades: {len(fact_trades):,}\")\n",
        "print(f\"   Trades with realized PnL: {fact_trades['realized_pnl'].notna().sum():,}\")\n",
        "print(f\"   Open positions: {fact_trades['realized_pnl'].isna().sum():,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPklKyyMBS0C",
        "outputId": "4649d3e1-0b43-42d3-85c3-4d3610ab797a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "CALCULATING REALIZED PnL\n",
            "======================================================================\n",
            "\n",
            "Calculating realized PnL for closed positions...\n",
            "This simulates position closing based on trade pairs...\n",
            "\n",
            "‚úÖ Calculated realized PnL for 342926 closed positions\n",
            "   Total realized PnL: $1,424,622,862.00\n",
            "   Average PnL per closed position: $4,154.32\n",
            "   Profitable trades: 214183\n",
            "   Losing trades: 128448\n",
            "\n",
            "üìä Trade Statistics:\n",
            "   Total trades: 961,100\n",
            "   Trades with realized PnL: 342,926\n",
            "   Open positions: 618,174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I calculated realized profit and loss for closed positions using a FIFO (First In First Out) matching algorithm. When a SELL trade occurs, I match it with the oldest BUY trade for the same account and security, calculating the PnL as (sell_price - buy_price) √ó quantity. This simulates realistic position closing and gives us realized PnL data for our analytics. Some trades remain with NULL realized_pnl, representing open positions that haven't been closed yet."
      ],
      "metadata": {
        "id": "Ed_PzNAME5Vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create fact_portfolio_snapshots - OPTIMIZED VERSION\n",
        "# This uses a more efficient approach to generate snapshots\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CREATING PORTFOLIO SNAPSHOTS (OPTIMIZED)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nGenerating portfolio snapshots using optimized approach...\\n\")\n",
        "print(\"This will be much faster...\\n\")\n",
        "\n",
        "# Use a more efficient approach: Sample a subset of dates and accounts\n",
        "# For demonstration, we'll create snapshots for:\n",
        "# - First day of each month (12 dates)\n",
        "# - Top 50 accounts by trade volume\n",
        "# This gives us a manageable dataset while demonstrating the concept\n",
        "\n",
        "# Get first day of each month (faster than month-end)\n",
        "sample_dates = pd.date_range(\n",
        "    start=fact_trades['trade_timestamp'].min().date(),\n",
        "    end=fact_trades['trade_timestamp'].max().date(),\n",
        "    freq='MS'  # Month Start - faster to process\n",
        ")[:12]  # Limit to 12 months for speed\n",
        "\n",
        "print(f\"Creating snapshots for {len(sample_dates)} sample dates...\\n\")\n",
        "\n",
        "# Get top accounts by trade count (for faster processing)\n",
        "top_accounts = fact_trades['account_key'].value_counts().head(50).index.tolist()\n",
        "print(f\"Processing top {len(top_accounts)} accounts by trade volume...\\n\")\n",
        "\n",
        "snapshots_list = []\n",
        "snapshot_key = 1\n",
        "\n",
        "# Pre-calculate trade dates for faster filtering\n",
        "fact_trades['trade_date'] = fact_trades['trade_timestamp'].dt.date\n",
        "\n",
        "for snapshot_date in sample_dates:\n",
        "    snapshot_date_obj = snapshot_date.date()\n",
        "    date_key = int(snapshot_date_obj.strftime('%Y%m%d'))\n",
        "    snapshot_date_dt = pd.Timestamp(snapshot_date)\n",
        "\n",
        "    print(f\"  Processing {snapshot_date_obj}...\")\n",
        "\n",
        "    # Filter trades efficiently using pre-calculated date column\n",
        "    trades_up_to_date = fact_trades[\n",
        "        (fact_trades['trade_date'] <= snapshot_date_obj) &\n",
        "        (fact_trades['account_key'].isin(top_accounts))\n",
        "    ]\n",
        "\n",
        "    if len(trades_up_to_date) == 0:\n",
        "        continue\n",
        "\n",
        "    # Calculate positions using groupby (much faster than iterating)\n",
        "    buy_trades = trades_up_to_date[trades_up_to_date['trade_type'] == 'BUY'].groupby(\n",
        "        ['account_key', 'security_key']\n",
        "    ).agg({\n",
        "        'quantity': 'sum',\n",
        "        'trade_value': 'sum'\n",
        "    }).reset_index()\n",
        "    buy_trades.columns = ['account_key', 'security_key', 'buy_quantity', 'buy_value']\n",
        "\n",
        "    sell_trades = trades_up_to_date[trades_up_to_date['trade_type'] == 'SELL'].groupby(\n",
        "        ['account_key', 'security_key']\n",
        "    ).agg({\n",
        "        'quantity': 'sum',\n",
        "        'trade_value': 'sum'\n",
        "    }).reset_index()\n",
        "    sell_trades.columns = ['account_key', 'security_key', 'sell_quantity', 'sell_value']\n",
        "\n",
        "    # Merge buy and sell trades\n",
        "    positions = buy_trades.merge(\n",
        "        sell_trades,\n",
        "        on=['account_key', 'security_key'],\n",
        "        how='outer'\n",
        "    ).fillna(0)\n",
        "\n",
        "    # Calculate net positions\n",
        "    positions['position_quantity'] = positions['buy_quantity'] - positions['sell_quantity']\n",
        "    positions['total_cost'] = positions['buy_value'] - positions['sell_value']\n",
        "\n",
        "    # Filter out zero positions\n",
        "    positions = positions[positions['position_quantity'] != 0]\n",
        "\n",
        "    if len(positions) == 0:\n",
        "        continue\n",
        "\n",
        "    # Get prices for all securities at once (vectorized)\n",
        "    security_keys = positions['security_key'].unique()\n",
        "    security_tickers = dim_security[dim_security['security_key'].isin(security_keys)][\n",
        "        ['security_key', 'ticker_symbol']\n",
        "    ]\n",
        "\n",
        "    # Get latest prices for each security up to snapshot date\n",
        "    latest_prices = []\n",
        "    for _, sec_row in security_tickers.iterrows():\n",
        "        ticker = sec_row['ticker_symbol']\n",
        "        sec_key = sec_row['security_key']\n",
        "\n",
        "        day_prices = historical_prices[\n",
        "            (historical_prices['ticker'] == ticker) &\n",
        "            (historical_prices['date'].dt.date <= snapshot_date_obj)\n",
        "        ]\n",
        "\n",
        "        if len(day_prices) > 0:\n",
        "            latest_prices.append({\n",
        "                'security_key': sec_key,\n",
        "                'current_price': day_prices['close_price'].iloc[-1]\n",
        "            })\n",
        "\n",
        "    if len(latest_prices) == 0:\n",
        "        continue\n",
        "\n",
        "    price_df = pd.DataFrame(latest_prices)\n",
        "    positions = positions.merge(price_df, on='security_key', how='left')\n",
        "    positions = positions[positions['current_price'].notna()]\n",
        "\n",
        "    # Calculate all metrics at once (vectorized)\n",
        "    positions['average_cost'] = positions['total_cost'] / positions['position_quantity']\n",
        "    positions['market_value'] = positions['position_quantity'] * positions['current_price']\n",
        "    positions['unrealized_pnl'] = (positions['current_price'] - positions['average_cost']) * positions['position_quantity']\n",
        "\n",
        "    # Create snapshot rows\n",
        "    for _, pos in positions.iterrows():\n",
        "        snapshot_row = {\n",
        "            'snapshot_date_key': date_key,\n",
        "            'account_key': int(pos['account_key']),\n",
        "            'security_key': int(pos['security_key']),\n",
        "            'position_quantity': round(float(pos['position_quantity']), 8),\n",
        "            'average_cost': round(float(pos['average_cost']), 8),\n",
        "            'current_price': round(float(pos['current_price']), 8),\n",
        "            'market_value': round(float(pos['market_value']), 2),\n",
        "            'unrealized_pnl': round(float(pos['unrealized_pnl']), 2),\n",
        "            'realized_pnl_td': 0,\n",
        "            'exposure_percentage': 0.0,\n",
        "            'position_delta': 0,\n",
        "            'position_gamma': 0,\n",
        "            'var_contribution': 0,\n",
        "            'margin_requirement': 0,\n",
        "            'days_held': 1,\n",
        "            'snapshot_timestamp': snapshot_date_dt.replace(hour=16, minute=0)\n",
        "        }\n",
        "        snapshots_list.append(snapshot_row)\n",
        "        snapshot_key += 1\n",
        "\n",
        "fact_portfolio_snapshots = pd.DataFrame(snapshots_list)\n",
        "\n",
        "print(f\"\\n‚úÖ Created {len(fact_portfolio_snapshots):,} portfolio snapshot records\")\n",
        "print(f\"   Date range: {fact_portfolio_snapshots['snapshot_date_key'].min()} to {fact_portfolio_snapshots['snapshot_date_key'].max()}\")\n",
        "print(f\"   Unique accounts: {fact_portfolio_snapshots['account_key'].nunique()}\")\n",
        "print(f\"   Unique securities: {fact_portfolio_snapshots['security_key'].nunique()}\")\n",
        "if len(fact_portfolio_snapshots) > 0:\n",
        "    print(f\"   Total market value: ${fact_portfolio_snapshots['market_value'].sum():,.2f}\")\n",
        "\n",
        "# Display sample\n",
        "print(f\"\\nüìã Sample portfolio snapshot data:\")\n",
        "if len(fact_portfolio_snapshots) > 0:\n",
        "    print(fact_portfolio_snapshots.head(10))\n",
        "else:\n",
        "    print(\"No snapshots created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHTtL1QED_SD",
        "outputId": "d2173c6a-114d-4c26-e9f2-d79c173c704c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "CREATING PORTFOLIO SNAPSHOTS (OPTIMIZED)\n",
            "======================================================================\n",
            "\n",
            "Generating portfolio snapshots using optimized approach...\n",
            "\n",
            "This will be much faster...\n",
            "\n",
            "Creating snapshots for 12 sample dates...\n",
            "\n",
            "Processing top 50 accounts by trade volume...\n",
            "\n",
            "  Processing 2023-02-01...\n",
            "  Processing 2023-03-01...\n",
            "  Processing 2023-04-01...\n",
            "  Processing 2023-05-01...\n",
            "  Processing 2023-06-01...\n",
            "  Processing 2023-07-01...\n",
            "  Processing 2023-08-01...\n",
            "  Processing 2023-09-01...\n",
            "  Processing 2023-10-01...\n",
            "  Processing 2023-11-01...\n",
            "  Processing 2023-12-01...\n",
            "  Processing 2024-01-01...\n",
            "\n",
            "‚úÖ Created 166,869 portfolio snapshot records\n",
            "   Date range: 20230201 to 20240101\n",
            "   Unique accounts: 50\n",
            "   Unique securities: 388\n",
            "   Total market value: $-472,768,140.17\n",
            "\n",
            "üìã Sample portfolio snapshot data:\n",
            "   snapshot_date_key  account_key  security_key  position_quantity  \\\n",
            "0           20230201            2             2             -100.0   \n",
            "1           20230201            2             5              100.0   \n",
            "2           20230201            2             6              500.0   \n",
            "3           20230201            2            10              100.0   \n",
            "4           20230201            2            12              500.0   \n",
            "5           20230201            2            13             1000.0   \n",
            "6           20230201            2            23              100.0   \n",
            "7           20230201            2            25             -100.0   \n",
            "8           20230201            2            28              200.0   \n",
            "9           20230201            2            32             -200.0   \n",
            "\n",
            "   average_cost  current_price  market_value  unrealized_pnl  realized_pnl_td  \\\n",
            "0        140.13     132.594360     -13259.44          753.56                0   \n",
            "1        108.04     113.989998      11399.00          595.00                0   \n",
            "2        109.50     105.960861      52980.43        -1769.57                0   \n",
            "3         14.02      15.162044       1516.20          114.20                0   \n",
            "4         23.97      23.631624      11815.81         -169.19                0   \n",
            "5         15.98      17.719999      17720.00         1740.00                0   \n",
            "6         96.92      84.639999       8464.00        -1228.00                0   \n",
            "7        252.94     225.479950     -22547.99         2746.01                0   \n",
            "8         98.39     105.150002      21030.00         1352.00                0   \n",
            "9         40.04      39.107319      -7821.46          186.54                0   \n",
            "\n",
            "   exposure_percentage  position_delta  position_gamma  var_contribution  \\\n",
            "0                  0.0               0               0                 0   \n",
            "1                  0.0               0               0                 0   \n",
            "2                  0.0               0               0                 0   \n",
            "3                  0.0               0               0                 0   \n",
            "4                  0.0               0               0                 0   \n",
            "5                  0.0               0               0                 0   \n",
            "6                  0.0               0               0                 0   \n",
            "7                  0.0               0               0                 0   \n",
            "8                  0.0               0               0                 0   \n",
            "9                  0.0               0               0                 0   \n",
            "\n",
            "   margin_requirement  days_held  snapshot_timestamp  \n",
            "0                   0          1 2023-02-01 16:00:00  \n",
            "1                   0          1 2023-02-01 16:00:00  \n",
            "2                   0          1 2023-02-01 16:00:00  \n",
            "3                   0          1 2023-02-01 16:00:00  \n",
            "4                   0          1 2023-02-01 16:00:00  \n",
            "5                   0          1 2023-02-01 16:00:00  \n",
            "6                   0          1 2023-02-01 16:00:00  \n",
            "7                   0          1 2023-02-01 16:00:00  \n",
            "8                   0          1 2023-02-01 16:00:00  \n",
            "9                   0          1 2023-02-01 16:00:00  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I optimized the portfolio snapshot generation to use vectorized pandas operations instead of row-by-row iteration. This version processes only 12 sample dates and the top 50 accounts, which is sufficient to demonstrate the concept while being much faster. The key optimization is using groupby aggregations to calculate positions in bulk rather than iterating through each trade individually. This should complete in 1-2 minutes instead of 20+ minutes."
      ],
      "metadata": {
        "id": "YIymiPbONHux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save all processed dimension and fact tables\n",
        "# These will be loaded into Supabase in the next notebook\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"SAVING PROCESSED DATA\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create processed data directory\n",
        "os.makedirs('data/processed', exist_ok=True)\n",
        "\n",
        "print(\"\\nSaving dimension tables...\")\n",
        "\n",
        "# Save all dimension tables\n",
        "dim_date.to_csv('data/processed/dim_date.csv', index=False)\n",
        "print(f\"  ‚úÖ dim_date.csv ({len(dim_date):,} rows)\")\n",
        "\n",
        "dim_time.to_csv('data/processed/dim_time.csv', index=False)\n",
        "print(f\"  ‚úÖ dim_time.csv ({len(dim_time):,} rows)\")\n",
        "\n",
        "dim_security.to_csv('data/processed/dim_security.csv', index=False)\n",
        "print(f\"  ‚úÖ dim_security.csv ({len(dim_security):,} rows)\")\n",
        "\n",
        "dim_asset_class.to_csv('data/processed/dim_asset_class.csv', index=False)\n",
        "print(f\"  ‚úÖ dim_asset_class.csv ({len(dim_asset_class):,} rows)\")\n",
        "\n",
        "dim_sector.to_csv('data/processed/dim_sector.csv', index=False)\n",
        "print(f\"  ‚úÖ dim_sector.csv ({len(dim_sector):,} rows)\")\n",
        "\n",
        "dim_trader.to_csv('data/processed/dim_trader.csv', index=False)\n",
        "print(f\"  ‚úÖ dim_trader.csv ({len(dim_trader):,} rows)\")\n",
        "\n",
        "dim_account.to_csv('data/processed/dim_account.csv', index=False)\n",
        "print(f\"  ‚úÖ dim_account.csv ({len(dim_account):,} rows)\")\n",
        "\n",
        "dim_exchange.to_csv('data/processed/dim_exchange.csv', index=False)\n",
        "print(f\"  ‚úÖ dim_exchange.csv ({len(dim_exchange):,} rows)\")\n",
        "\n",
        "dim_counterparty.to_csv('data/processed/dim_counterparty.csv', index=False)\n",
        "print(f\"  ‚úÖ dim_counterparty.csv ({len(dim_counterparty):,} rows)\")\n",
        "\n",
        "dim_strategy.to_csv('data/processed/dim_strategy.csv', index=False)\n",
        "print(f\"  ‚úÖ dim_strategy.csv ({len(dim_strategy):,} rows)\")\n",
        "\n",
        "dim_trade_attributes.to_csv('data/processed/dim_trade_attributes.csv', index=False)\n",
        "print(f\"  ‚úÖ dim_trade_attributes.csv ({len(dim_trade_attributes):,} rows)\")\n",
        "\n",
        "print(\"\\nSaving fact tables...\")\n",
        "\n",
        "# Save fact tables\n",
        "fact_trades.to_csv('data/processed/fact_trades.csv', index=False)\n",
        "print(f\"  ‚úÖ fact_trades.csv ({len(fact_trades):,} rows)\")\n",
        "\n",
        "fact_portfolio_snapshots.to_csv('data/processed/fact_portfolio_snapshots.csv', index=False)\n",
        "print(f\"  ‚úÖ fact_portfolio_snapshots.csv ({len(fact_portfolio_snapshots):,} rows)\")\n",
        "\n",
        "# Save summary statistics\n",
        "summary_stats = {\n",
        "    'processing_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'total_dimensions': 11,\n",
        "    'total_fact_tables': 2,\n",
        "    'dim_date_rows': len(dim_date),\n",
        "    'dim_security_rows': len(dim_security),\n",
        "    'dim_trader_rows': len(dim_trader),\n",
        "    'dim_account_rows': len(dim_account),\n",
        "    'fact_trades_rows': len(fact_trades),\n",
        "    'fact_portfolio_snapshots_rows': len(fact_portfolio_snapshots),\n",
        "    'total_trade_value': fact_trades['trade_value'].sum(),\n",
        "    'date_range_start': str(fact_trades['trade_timestamp'].min()),\n",
        "    'date_range_end': str(fact_trades['trade_timestamp'].max())\n",
        "}\n",
        "\n",
        "summary_df = pd.DataFrame([summary_stats])\n",
        "summary_df.to_csv('data/processed/processing_summary.csv', index=False)\n",
        "print(f\"  ‚úÖ processing_summary.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ ALL PROCESSED DATA SAVED SUCCESSFULLY!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nüìä FINAL SUMMARY:\")\n",
        "print(f\"   Dimension Tables: 11 tables\")\n",
        "print(f\"   Fact Tables: 2 tables\")\n",
        "print(f\"   Total Trades: {len(fact_trades):,}\")\n",
        "print(f\"   Total Portfolio Snapshots: {len(fact_portfolio_snapshots):,}\")\n",
        "print(f\"   Total Trade Value: ${fact_trades['trade_value'].sum():,.2f}\")\n",
        "print(f\"\\n‚úÖ Ready for ETL to Supabase in next notebook!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1V6MEChE_3c",
        "outputId": "54a613ac-13be-4120-87d5-500986bf0405"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "SAVING PROCESSED DATA\n",
            "======================================================================\n",
            "\n",
            "Saving dimension tables...\n",
            "  ‚úÖ dim_date.csv (729 rows)\n",
            "  ‚úÖ dim_time.csv (390 rows)\n",
            "  ‚úÖ dim_security.csv (413 rows)\n",
            "  ‚úÖ dim_asset_class.csv (4 rows)\n",
            "  ‚úÖ dim_sector.csv (12 rows)\n",
            "  ‚úÖ dim_trader.csv (50 rows)\n",
            "  ‚úÖ dim_account.csv (200 rows)\n",
            "  ‚úÖ dim_exchange.csv (6 rows)\n",
            "  ‚úÖ dim_counterparty.csv (30 rows)\n",
            "  ‚úÖ dim_strategy.csv (12 rows)\n",
            "  ‚úÖ dim_trade_attributes.csv (24 rows)\n",
            "\n",
            "Saving fact tables...\n",
            "  ‚úÖ fact_trades.csv (961,100 rows)\n",
            "  ‚úÖ fact_portfolio_snapshots.csv (166,869 rows)\n",
            "  ‚úÖ processing_summary.csv\n",
            "\n",
            "======================================================================\n",
            "‚úÖ ALL PROCESSED DATA SAVED SUCCESSFULLY!\n",
            "======================================================================\n",
            "\n",
            "üìä FINAL SUMMARY:\n",
            "   Dimension Tables: 11 tables\n",
            "   Fact Tables: 2 tables\n",
            "   Total Trades: 961,100\n",
            "   Total Portfolio Snapshots: 166,869\n",
            "   Total Trade Value: $59,932,790,167.00\n",
            "\n",
            "‚úÖ Ready for ETL to Supabase in next notebook!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I set up the connection to Supabase. I'm using two methods: Colab secrets (recommended for security) or manual input. The connection string is obtained from Supabase Dashboard ‚Üí Settings ‚Üí Database. I test the connection first to ensure it works before proceeding with data loading. I also create a SQLAlchemy engine which allows us to use pandas' to_sql() method for efficient bulk data loading."
      ],
      "metadata": {
        "id": "o2naTawsT1Y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ETL PIPELINE TO SUPABASE\n",
        "# ============================================================================\n",
        "# Load processed data into Supabase PostgreSQL database\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"ETL PIPELINE TO SUPABASE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Install required packages\n",
        "!pip install psycopg2-binary sqlalchemy python-dotenv -q\n",
        "\n",
        "import psycopg2\n",
        "from sqlalchemy import create_engine\n",
        "import os\n",
        "\n",
        "print(\"\\n‚úÖ Packages installed and imported\")\n",
        "\n",
        "# Connection setup\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚ö†Ô∏è IMPORTANT: You MUST use Connection Pooler (not Direct connection)\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nüìã Steps to get the correct connection string:\")\n",
        "print(\"\\n1. Go to Supabase Dashboard ‚Üí Settings ‚Üí Database\")\n",
        "print(\"2. Find 'Connection string' section\")\n",
        "print(\"3. Click the 'Connection pooling' TAB (NOT 'Direct connection')\")\n",
        "print(\"4. Select 'Session mode'\")\n",
        "print(\"5. Copy the URI string\")\n",
        "print(\"\\n‚úÖ The pooler string should look like:\")\n",
        "print(\"   postgresql://postgres.kxjbelwsvuryfoolazml:YOUR_PASSWORD@\")\n",
        "print(\"   aws-0-us-east-1.pooler.supabase.com:6543/postgres\")\n",
        "print(\"\\n‚ùå NOT like this (direct connection - won't work):\")\n",
        "print(\"   postgresql://postgres:...@db.kxjbelwsvuryfoolazml.supabase.co:5432/postgres\")\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "# Get connection string\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    DATABASE_URL = userdata.get('SUPABASE_DATABASE_URL')\n",
        "    print(\"\\n‚úÖ Using connection string from Colab secrets\")\n",
        "except:\n",
        "    print(\"\\n‚ö†Ô∏è Please paste your Connection Pooler string below:\")\n",
        "    print(\"   (Make sure it's from the 'Connection pooling' tab, not 'Direct connection')\")\n",
        "    connection_input = input(\"\\nConnection Pooler string: \").strip()\n",
        "\n",
        "    if not connection_input:\n",
        "        raise ValueError(\"Database connection string required\")\n",
        "\n",
        "    DATABASE_URL = connection_input\n",
        "\n",
        "# Validate connection string format\n",
        "print(\"\\nüîç Validating connection string...\")\n",
        "\n",
        "if 'pooler.supabase.com' in DATABASE_URL:\n",
        "    print(\"   ‚úÖ Connection string uses pooler (correct!)\")\n",
        "elif 'db.kxjbelwsvuryfoolazml.supabase.co' in DATABASE_URL:\n",
        "    print(\"   ‚ùå ERROR: This is a Direct connection string (IPv6 only)\")\n",
        "    print(\"   ‚ö†Ô∏è You need the Connection Pooler string instead!\")\n",
        "    print(\"\\n   Please:\")\n",
        "    print(\"   1. Go back to Supabase Dashboard\")\n",
        "    print(\"   2. Click 'Connection pooling' TAB\")\n",
        "    print(\"   3. Select 'Session mode'\")\n",
        "    print(\"   4. Copy that URI string\")\n",
        "    raise ValueError(\"Please use Connection Pooler string, not Direct connection\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è Connection string format unclear - will try anyway\")\n",
        "\n",
        "# Check port\n",
        "if ':5432/' in DATABASE_URL:\n",
        "    print(\"   ‚ö†Ô∏è Port 5432 detected - should be 6543 for pooler\")\n",
        "    DATABASE_URL = DATABASE_URL.replace(':5432/', ':6543/')\n",
        "    print(\"   ‚úÖ Changed to port 6543\")\n",
        "elif ':6543/' in DATABASE_URL:\n",
        "    print(\"   ‚úÖ Port 6543 (pooler) - correct!\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è Port not clear in connection string\")\n",
        "\n",
        "# Test connection\n",
        "print(\"\\nüîå Testing connection to Supabase...\")\n",
        "\n",
        "try:\n",
        "    conn = psycopg2.connect(\n",
        "        DATABASE_URL,\n",
        "        connect_timeout=15\n",
        "    )\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(\"SELECT version();\")\n",
        "    version = cur.fetchone()\n",
        "    print(f\"‚úÖ Connected successfully!\")\n",
        "    print(f\"   PostgreSQL version: {version[0][:60]}...\")\n",
        "\n",
        "    # Test a simple query\n",
        "    cur.execute(\"SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public';\")\n",
        "    table_count = cur.fetchone()[0]\n",
        "    print(f\"   Public tables found: {table_count}\")\n",
        "\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "\n",
        "except Exception as e:\n",
        "    error_msg = str(e)\n",
        "    print(f\"‚ùå Connection failed: {error_msg[:150]}\")\n",
        "\n",
        "    if 'IPv6' in error_msg or 'Network is unreachable' in error_msg:\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"üîß SOLUTION: Use Connection Pooler\")\n",
        "        print(\"=\" * 70)\n",
        "        print(\"\\nYour connection string is using Direct connection (IPv6 only).\")\n",
        "        print(\"Google Colab needs IPv4, which requires Connection Pooler.\")\n",
        "        print(\"\\nüìã How to get Connection Pooler string:\")\n",
        "        print(\"\\n1. Go to: https://app.supabase.com/project/kxjbelwsvuryfoolazml/settings/database\")\n",
        "        print(\"2. Scroll to 'Connection string' section\")\n",
        "        print(\"3. Click 'Connection pooling' TAB (next to 'Direct connection')\")\n",
        "        print(\"4. Select 'Session mode'\")\n",
        "        print(\"5. Copy the URI string\")\n",
        "        print(\"\\n‚úÖ The pooler string will have:\")\n",
        "        print(\"   - Host: aws-0-[region].pooler.supabase.com\")\n",
        "        print(\"   - Port: 6543\")\n",
        "        print(\"   - Format: postgresql://postgres.[ref]:[PASSWORD]@[POOLER_HOST]:6543/postgres\")\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "    raise\n",
        "\n",
        "# Create SQLAlchemy engine\n",
        "engine = create_engine(\n",
        "    DATABASE_URL,\n",
        "    pool_pre_ping=True,\n",
        "    pool_recycle=300,\n",
        "    connect_args={\n",
        "        \"connect_timeout\": 15,\n",
        "        \"options\": \"-c statement_timeout=300000\"\n",
        "    }\n",
        ")\n",
        "print(\"‚úÖ SQLAlchemy engine created\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ CONNECTION ESTABLISHED!\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nReady to proceed with schema creation and data loading!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpleahR2NvC8",
        "outputId": "0a60eab5-bd13-4e28-b272-f2571116f5f5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ETL PIPELINE TO SUPABASE\n",
            "======================================================================\n",
            "\n",
            "‚úÖ Packages installed and imported\n",
            "\n",
            "======================================================================\n",
            "‚ö†Ô∏è IMPORTANT: You MUST use Connection Pooler (not Direct connection)\n",
            "======================================================================\n",
            "\n",
            "üìã Steps to get the correct connection string:\n",
            "\n",
            "1. Go to Supabase Dashboard ‚Üí Settings ‚Üí Database\n",
            "2. Find 'Connection string' section\n",
            "3. Click the 'Connection pooling' TAB (NOT 'Direct connection')\n",
            "4. Select 'Session mode'\n",
            "5. Copy the URI string\n",
            "\n",
            "‚úÖ The pooler string should look like:\n",
            "   postgresql://postgres.kxjbelwsvuryfoolazml:YOUR_PASSWORD@\n",
            "   aws-0-us-east-1.pooler.supabase.com:6543/postgres\n",
            "\n",
            "‚ùå NOT like this (direct connection - won't work):\n",
            "   postgresql://postgres:...@db.kxjbelwsvuryfoolazml.supabase.co:5432/postgres\n",
            "\n",
            "======================================================================\n",
            "\n",
            "‚ö†Ô∏è Please paste your Connection Pooler string below:\n",
            "   (Make sure it's from the 'Connection pooling' tab, not 'Direct connection')\n",
            "\n",
            "Connection Pooler string: postgresql://postgres.kxjbelwsvuryfoolazml:rYeQ3yJrJwxuQZsE@aws-1-eu-west-1.pooler.supabase.com:5432/postgres\n",
            "\n",
            "üîç Validating connection string...\n",
            "   ‚úÖ Connection string uses pooler (correct!)\n",
            "   ‚ö†Ô∏è Port 5432 detected - should be 6543 for pooler\n",
            "   ‚úÖ Changed to port 6543\n",
            "\n",
            "üîå Testing connection to Supabase...\n",
            "‚úÖ Connected successfully!\n",
            "   PostgreSQL version: PostgreSQL 17.6 on aarch64-unknown-linux-gnu, compiled by gc...\n",
            "   Public tables found: 0\n",
            "‚úÖ SQLAlchemy engine created\n",
            "\n",
            "======================================================================\n",
            "‚úÖ CONNECTION ESTABLISHED!\n",
            "======================================================================\n",
            "\n",
            "Ready to proceed with schema creation and data loading!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I created all dimension tables in Supabase with their proper data types, constraints, and foreign keys. I included SCD Type 2 fields (effective_date, expiry_date_scd, is_current, version) for dim_security and dim_trader to support temporal tracking. I also created indexes on frequently queried columns like ticker_symbol and is_current flags. The schema follows our data model specifications."
      ],
      "metadata": {
        "id": "ksy3cAvuU3m6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create database schema in Supabase\n",
        "# This creates all tables, partitions, and constraints\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CREATING DATABASE SCHEMA\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "conn = psycopg2.connect(DATABASE_URL)\n",
        "cur = conn.cursor()\n",
        "\n",
        "print(\"\\n1. Creating dimension tables...\\n\")\n",
        "\n",
        "# Create dim_date\n",
        "cur.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS dim_date (\n",
        "        date_key INTEGER PRIMARY KEY,\n",
        "        date DATE NOT NULL,\n",
        "        year INTEGER NOT NULL,\n",
        "        quarter INTEGER NOT NULL,\n",
        "        month INTEGER NOT NULL,\n",
        "        day INTEGER NOT NULL,\n",
        "        day_of_week INTEGER NOT NULL,\n",
        "        day_name VARCHAR(10) NOT NULL,\n",
        "        month_name VARCHAR(10) NOT NULL,\n",
        "        quarter_name VARCHAR(20) NOT NULL,\n",
        "        is_weekend BOOLEAN NOT NULL,\n",
        "        is_trading_day BOOLEAN NOT NULL,\n",
        "        is_month_end BOOLEAN NOT NULL,\n",
        "        is_quarter_end BOOLEAN NOT NULL,\n",
        "        is_year_end BOOLEAN NOT NULL,\n",
        "        week_of_year INTEGER NOT NULL,\n",
        "        day_of_year INTEGER NOT NULL\n",
        "    );\n",
        "\"\"\")\n",
        "print(\"   ‚úÖ Created dim_date\")\n",
        "\n",
        "# Create dim_time\n",
        "cur.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS dim_time (\n",
        "        time_key INTEGER PRIMARY KEY,\n",
        "        time VARCHAR(10) NOT NULL,\n",
        "        hour INTEGER NOT NULL,\n",
        "        minute INTEGER NOT NULL,\n",
        "        second INTEGER NOT NULL,\n",
        "        hour_24 INTEGER NOT NULL,\n",
        "        hour_12 INTEGER NOT NULL,\n",
        "        am_pm VARCHAR(2) NOT NULL,\n",
        "        trading_session VARCHAR(20) NOT NULL,\n",
        "        minute_of_day INTEGER NOT NULL,\n",
        "        is_trading_hours BOOLEAN NOT NULL\n",
        "    );\n",
        "\"\"\")\n",
        "print(\"   ‚úÖ Created dim_time\")\n",
        "\n",
        "# Create dim_asset_class\n",
        "cur.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS dim_asset_class (\n",
        "        asset_class_key INTEGER PRIMARY KEY,\n",
        "        asset_class_code VARCHAR(20) NOT NULL,\n",
        "        asset_class_name VARCHAR(50) NOT NULL,\n",
        "        description VARCHAR(200)\n",
        "    );\n",
        "\"\"\")\n",
        "print(\"   ‚úÖ Created dim_asset_class\")\n",
        "\n",
        "# Create dim_sector\n",
        "cur.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS dim_sector (\n",
        "        sector_key INTEGER PRIMARY KEY,\n",
        "        sector_code VARCHAR(20) NOT NULL,\n",
        "        sector_name VARCHAR(100) NOT NULL,\n",
        "        description VARCHAR(200)\n",
        "    );\n",
        "\"\"\")\n",
        "print(\"   ‚úÖ Created dim_sector\")\n",
        "\n",
        "# Create dim_security (with SCD Type 2)\n",
        "cur.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS dim_security (\n",
        "        security_key INTEGER PRIMARY KEY,\n",
        "        security_id VARCHAR(20) NOT NULL,\n",
        "        ticker_symbol VARCHAR(10) NOT NULL,\n",
        "        security_name VARCHAR(200) NOT NULL,\n",
        "        asset_class_key INTEGER,\n",
        "        sector VARCHAR(100),\n",
        "        industry VARCHAR(200),\n",
        "        currency_code CHAR(3) NOT NULL,\n",
        "        exchange_listed VARCHAR(10) NOT NULL,\n",
        "        market_cap BIGINT,\n",
        "        lot_size INTEGER DEFAULT 1,\n",
        "        tick_size DECIMAL(10,6) NOT NULL,\n",
        "        multiplier DECIMAL(10,2) DEFAULT 1.0,\n",
        "        expiry_date DATE,\n",
        "        strike_price DECIMAL(20,8),\n",
        "        option_type VARCHAR(4),\n",
        "        underlying_security_key INTEGER,\n",
        "        effective_date DATE NOT NULL,\n",
        "        expiry_date_scd DATE NOT NULL,\n",
        "        is_current BOOLEAN NOT NULL,\n",
        "        version INTEGER NOT NULL,\n",
        "        is_active BOOLEAN NOT NULL,\n",
        "        last_price DECIMAL(20,8),\n",
        "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "        FOREIGN KEY (asset_class_key) REFERENCES dim_asset_class(asset_class_key)\n",
        "    );\n",
        "\"\"\")\n",
        "print(\"   ‚úÖ Created dim_security (with SCD Type 2)\")\n",
        "\n",
        "# Create dim_trader (with SCD Type 2)\n",
        "cur.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS dim_trader (\n",
        "        trader_key INTEGER PRIMARY KEY,\n",
        "        trader_id VARCHAR(20) NOT NULL,\n",
        "        full_name VARCHAR(100) NOT NULL,\n",
        "        desk_name VARCHAR(50) NOT NULL,\n",
        "        authorization_level INTEGER NOT NULL,\n",
        "        trader_type VARCHAR(20) NOT NULL,\n",
        "        certifications VARCHAR(200),\n",
        "        compliance_status VARCHAR(20) NOT NULL,\n",
        "        effective_date DATE NOT NULL,\n",
        "        expiry_date DATE NOT NULL,\n",
        "        is_current BOOLEAN NOT NULL,\n",
        "        version INTEGER NOT NULL\n",
        "    );\n",
        "\"\"\")\n",
        "print(\"   ‚úÖ Created dim_trader (with SCD Type 2)\")\n",
        "\n",
        "# Create dim_account\n",
        "cur.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS dim_account (\n",
        "        account_key INTEGER PRIMARY KEY,\n",
        "        account_number VARCHAR(20) NOT NULL UNIQUE,\n",
        "        account_name VARCHAR(200) NOT NULL,\n",
        "        account_type VARCHAR(30) NOT NULL,\n",
        "        parent_account_key INTEGER,\n",
        "        account_level INTEGER NOT NULL,\n",
        "        risk_profile VARCHAR(20) NOT NULL,\n",
        "        margin_limit DECIMAL(20,2) NOT NULL,\n",
        "        cash_balance DECIMAL(20,2) NOT NULL,\n",
        "        total_equity DECIMAL(20,2) DEFAULT 0,\n",
        "        kyc_status VARCHAR(20) NOT NULL,\n",
        "        kyc_expiry_date DATE NOT NULL,\n",
        "        opening_date DATE NOT NULL,\n",
        "        is_active BOOLEAN NOT NULL,\n",
        "        FOREIGN KEY (parent_account_key) REFERENCES dim_account(account_key)\n",
        "    );\n",
        "\"\"\")\n",
        "print(\"   ‚úÖ Created dim_account\")\n",
        "\n",
        "# Create dim_exchange\n",
        "cur.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS dim_exchange (\n",
        "        exchange_key INTEGER PRIMARY KEY,\n",
        "        exchange_code VARCHAR(10) NOT NULL,\n",
        "        exchange_name VARCHAR(50) NOT NULL,\n",
        "        country VARCHAR(3) NOT NULL,\n",
        "        trading_hours VARCHAR(20) NOT NULL,\n",
        "        settlement_cycle VARCHAR(10) NOT NULL\n",
        "    );\n",
        "\"\"\")\n",
        "print(\"   ‚úÖ Created dim_exchange\")\n",
        "\n",
        "# Create dim_counterparty\n",
        "cur.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS dim_counterparty (\n",
        "        counterparty_key INTEGER PRIMARY KEY,\n",
        "        counterparty_id VARCHAR(20) NOT NULL,\n",
        "        counterparty_name VARCHAR(100) NOT NULL,\n",
        "        credit_rating VARCHAR(5) NOT NULL,\n",
        "        exposure_limit DECIMAL(20,2) NOT NULL\n",
        "    );\n",
        "\"\"\")\n",
        "print(\"   ‚úÖ Created dim_counterparty\")\n",
        "\n",
        "# Create dim_strategy\n",
        "cur.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS dim_strategy (\n",
        "        strategy_key INTEGER PRIMARY KEY,\n",
        "        strategy_id VARCHAR(20) NOT NULL,\n",
        "        strategy_name VARCHAR(100) NOT NULL,\n",
        "        strategy_type VARCHAR(20) NOT NULL,\n",
        "        risk_parameters VARCHAR(200)\n",
        "    );\n",
        "\"\"\")\n",
        "print(\"   ‚úÖ Created dim_strategy\")\n",
        "\n",
        "# Create dim_trade_attributes\n",
        "cur.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS dim_trade_attributes (\n",
        "        attributes_key INTEGER PRIMARY KEY,\n",
        "        is_algorithmic BOOLEAN NOT NULL,\n",
        "        is_day_trade BOOLEAN NOT NULL,\n",
        "        requires_review BOOLEAN NOT NULL,\n",
        "        settlement_status VARCHAR(20) NOT NULL\n",
        "    );\n",
        "\"\"\")\n",
        "print(\"   ‚úÖ Created dim_trade_attributes\")\n",
        "\n",
        "conn.commit()\n",
        "print(\"\\n‚úÖ All dimension tables created!\")\n",
        "\n",
        "# Create indexes on dimension tables\n",
        "print(\"\\n2. Creating indexes on dimension tables...\\n\")\n",
        "\n",
        "cur.execute(\"CREATE INDEX IF NOT EXISTS idx_security_ticker ON dim_security(ticker_symbol);\")\n",
        "cur.execute(\"CREATE INDEX IF NOT EXISTS idx_security_current ON dim_security(is_current) WHERE is_current = TRUE;\")\n",
        "cur.execute(\"CREATE INDEX IF NOT EXISTS idx_trader_current ON dim_trader(is_current) WHERE is_current = TRUE;\")\n",
        "cur.execute(\"CREATE INDEX IF NOT EXISTS idx_account_number ON dim_account(account_number);\")\n",
        "\n",
        "conn.commit()\n",
        "print(\"   ‚úÖ Indexes created\")\n",
        "\n",
        "cur.close()\n",
        "conn.close()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ DATABASE SCHEMA CREATED SUCCESSFULLY!\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXbwz3BnOZyd",
        "outputId": "09f5e33b-3023-433c-c881-4ce02dcc1332"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "CREATING DATABASE SCHEMA\n",
            "======================================================================\n",
            "\n",
            "1. Creating dimension tables...\n",
            "\n",
            "   ‚úÖ Created dim_date\n",
            "   ‚úÖ Created dim_time\n",
            "   ‚úÖ Created dim_asset_class\n",
            "   ‚úÖ Created dim_sector\n",
            "   ‚úÖ Created dim_security (with SCD Type 2)\n",
            "   ‚úÖ Created dim_trader (with SCD Type 2)\n",
            "   ‚úÖ Created dim_account\n",
            "   ‚úÖ Created dim_exchange\n",
            "   ‚úÖ Created dim_counterparty\n",
            "   ‚úÖ Created dim_strategy\n",
            "   ‚úÖ Created dim_trade_attributes\n",
            "\n",
            "‚úÖ All dimension tables created!\n",
            "\n",
            "2. Creating indexes on dimension tables...\n",
            "\n",
            "   ‚úÖ Indexes created\n",
            "\n",
            "======================================================================\n",
            "‚úÖ DATABASE SCHEMA CREATED SUCCESSFULLY!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I created the partitioned fact_trades table using PostgreSQL's native partitioning. The parent table is partitioned by RANGE on trade_timestamp, and I created monthly partitions for each month in our date range. This enables partition pruning - when querying by date, PostgreSQL only scans the relevant partitions, dramatically improving performance. I also created the fact_portfolio_snapshots table with all required foreign keys."
      ],
      "metadata": {
        "id": "Xlvy532tVE78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create partitioned fact tables\n",
        "# fact_trades will be partitioned by month for performance\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CREATING PARTITIONED FACT TABLES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "conn = psycopg2.connect(DATABASE_URL)\n",
        "cur = conn.cursor()\n",
        "\n",
        "print(\"\\n1. Creating partitioned fact_trades table...\\n\")\n",
        "\n",
        "# Create parent table for fact_trades (partitioned)\n",
        "cur.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS fact_trades (\n",
        "        trade_id BIGSERIAL,\n",
        "        trade_timestamp TIMESTAMP NOT NULL,\n",
        "        date_key INTEGER NOT NULL,\n",
        "        time_key INTEGER NOT NULL,\n",
        "        security_key INTEGER NOT NULL,\n",
        "        trader_key INTEGER NOT NULL,\n",
        "        account_key INTEGER NOT NULL,\n",
        "        exchange_key INTEGER NOT NULL,\n",
        "        counterparty_key INTEGER NOT NULL,\n",
        "        strategy_key INTEGER NOT NULL,\n",
        "        attributes_key INTEGER NOT NULL,\n",
        "        trade_type VARCHAR(10) NOT NULL CHECK (trade_type IN ('BUY', 'SELL', 'SHORT', 'COVER')),\n",
        "        quantity DECIMAL(20,8) NOT NULL,\n",
        "        price DECIMAL(20,8) NOT NULL,\n",
        "        trade_value DECIMAL(20,2) NOT NULL,\n",
        "        commission DECIMAL(12,2) DEFAULT 0,\n",
        "        net_proceeds DECIMAL(20,2) NOT NULL,\n",
        "        realized_pnl DECIMAL(20,2),\n",
        "        portfolio_exposure DECIMAL(20,2) NOT NULL,\n",
        "        margin_used DECIMAL(20,2) DEFAULT 0,\n",
        "        order_id VARCHAR(50) NOT NULL,\n",
        "        execution_venue VARCHAR(50),\n",
        "        settlement_date DATE NOT NULL,\n",
        "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "        PRIMARY KEY (trade_id, trade_timestamp),\n",
        "        FOREIGN KEY (date_key) REFERENCES dim_date(date_key),\n",
        "        FOREIGN KEY (time_key) REFERENCES dim_time(time_key),\n",
        "        FOREIGN KEY (security_key) REFERENCES dim_security(security_key),\n",
        "        FOREIGN KEY (trader_key) REFERENCES dim_trader(trader_key),\n",
        "        FOREIGN KEY (account_key) REFERENCES dim_account(account_key),\n",
        "        FOREIGN KEY (exchange_key) REFERENCES dim_exchange(exchange_key),\n",
        "        FOREIGN KEY (counterparty_key) REFERENCES dim_counterparty(counterparty_key),\n",
        "        FOREIGN KEY (strategy_key) REFERENCES dim_strategy(strategy_key),\n",
        "        FOREIGN KEY (attributes_key) REFERENCES dim_trade_attributes(attributes_key)\n",
        "    ) PARTITION BY RANGE (trade_timestamp);\n",
        "\"\"\")\n",
        "print(\"   ‚úÖ Created parent table fact_trades\")\n",
        "\n",
        "# Create partitions for each month in our date range\n",
        "print(\"\\n2. Creating monthly partitions...\\n\")\n",
        "\n",
        "# Get date range from fact_trades\n",
        "date_range = pd.date_range(\n",
        "    start=fact_trades['trade_timestamp'].min(),\n",
        "    end=fact_trades['trade_timestamp'].max(),\n",
        "    freq='MS'  # Month start\n",
        ")\n",
        "\n",
        "# Also add one month after for future data\n",
        "date_range = date_range.union([date_range[-1] + pd.DateOffset(months=1)])\n",
        "\n",
        "partitions_created = 0\n",
        "for i in range(len(date_range) - 1):\n",
        "    start_date = date_range[i]\n",
        "    end_date = date_range[i + 1]\n",
        "    partition_name = f\"fact_trades_{start_date.strftime('%Y_%m')}\"\n",
        "\n",
        "    try:\n",
        "        cur.execute(f\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS {partition_name} PARTITION OF fact_trades\n",
        "            FOR VALUES FROM ('{start_date}') TO ('{end_date}');\n",
        "        \"\"\")\n",
        "        print(f\"   ‚úÖ Created partition {partition_name}\")\n",
        "        partitions_created += 1\n",
        "    except Exception as e:\n",
        "        if \"already exists\" in str(e).lower():\n",
        "            print(f\"   ‚ö†Ô∏è Partition {partition_name} already exists (skipping)\")\n",
        "        else:\n",
        "            print(f\"   ‚ùå Error creating {partition_name}: {str(e)[:80]}\")\n",
        "\n",
        "conn.commit()\n",
        "print(f\"\\n‚úÖ Created {partitions_created} partitions!\")\n",
        "\n",
        "# Create fact_portfolio_snapshots\n",
        "print(\"\\n3. Creating fact_portfolio_snapshots table...\\n\")\n",
        "\n",
        "cur.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS fact_portfolio_snapshots (\n",
        "        snapshot_key BIGSERIAL PRIMARY KEY,\n",
        "        snapshot_date_key INTEGER NOT NULL,\n",
        "        account_key INTEGER NOT NULL,\n",
        "        security_key INTEGER NOT NULL,\n",
        "        position_quantity DECIMAL(20,8) NOT NULL,\n",
        "        average_cost DECIMAL(20,8) NOT NULL,\n",
        "        current_price DECIMAL(20,8) NOT NULL,\n",
        "        market_value DECIMAL(20,2) NOT NULL,\n",
        "        unrealized_pnl DECIMAL(20,2) NOT NULL,\n",
        "        realized_pnl_td DECIMAL(20,2) DEFAULT 0,\n",
        "        exposure_percentage DECIMAL(5,2) DEFAULT 0,\n",
        "        position_delta DECIMAL(10,4) DEFAULT 0,\n",
        "        position_gamma DECIMAL(10,4) DEFAULT 0,\n",
        "        var_contribution DECIMAL(20,2) DEFAULT 0,\n",
        "        margin_requirement DECIMAL(20,2) DEFAULT 0,\n",
        "        days_held INTEGER DEFAULT 1,\n",
        "        snapshot_timestamp TIMESTAMP NOT NULL,\n",
        "        FOREIGN KEY (snapshot_date_key) REFERENCES dim_date(date_key),\n",
        "        FOREIGN KEY (account_key) REFERENCES dim_account(account_key),\n",
        "        FOREIGN KEY (security_key) REFERENCES dim_security(security_key)\n",
        "    );\n",
        "\"\"\")\n",
        "print(\"   ‚úÖ Created fact_portfolio_snapshots\")\n",
        "\n",
        "conn.commit()\n",
        "cur.close()\n",
        "conn.close()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ FACT TABLES CREATED SUCCESSFULLY!\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6dA45lpU6mC",
        "outputId": "1f325f21-b3a7-4213-8bbc-6d7e6961afa9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "CREATING PARTITIONED FACT TABLES\n",
            "======================================================================\n",
            "\n",
            "1. Creating partitioned fact_trades table...\n",
            "\n",
            "   ‚úÖ Created parent table fact_trades\n",
            "\n",
            "2. Creating monthly partitions...\n",
            "\n",
            "   ‚úÖ Created partition fact_trades_2023_02\n",
            "   ‚úÖ Created partition fact_trades_2023_03\n",
            "   ‚úÖ Created partition fact_trades_2023_04\n",
            "   ‚úÖ Created partition fact_trades_2023_05\n",
            "   ‚úÖ Created partition fact_trades_2023_06\n",
            "   ‚úÖ Created partition fact_trades_2023_07\n",
            "   ‚úÖ Created partition fact_trades_2023_08\n",
            "   ‚úÖ Created partition fact_trades_2023_09\n",
            "   ‚úÖ Created partition fact_trades_2023_10\n",
            "   ‚úÖ Created partition fact_trades_2023_11\n",
            "   ‚úÖ Created partition fact_trades_2023_12\n",
            "   ‚úÖ Created partition fact_trades_2024_01\n",
            "   ‚úÖ Created partition fact_trades_2024_02\n",
            "   ‚úÖ Created partition fact_trades_2024_03\n",
            "   ‚úÖ Created partition fact_trades_2024_04\n",
            "   ‚úÖ Created partition fact_trades_2024_05\n",
            "   ‚úÖ Created partition fact_trades_2024_06\n",
            "   ‚úÖ Created partition fact_trades_2024_07\n",
            "   ‚úÖ Created partition fact_trades_2024_08\n",
            "   ‚úÖ Created partition fact_trades_2024_09\n",
            "   ‚úÖ Created partition fact_trades_2024_10\n",
            "   ‚úÖ Created partition fact_trades_2024_11\n",
            "   ‚úÖ Created partition fact_trades_2024_12\n",
            "\n",
            "‚úÖ Created 23 partitions!\n",
            "\n",
            "3. Creating fact_portfolio_snapshots table...\n",
            "\n",
            "   ‚úÖ Created fact_portfolio_snapshots\n",
            "\n",
            "======================================================================\n",
            "‚úÖ FACT TABLES CREATED SUCCESSFULLY!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I loaded all dimension tables to Supabase using pandas' to_sql() method with bulk insert (method='multi'). I loaded them in dependency order - simple lookup tables first, then tables with foreign keys. I used if_exists='replace' which allows re-running the cell if needed. If that fails, I try truncating and appending. The chunksize of 1000 ensures efficient memory usage while maintaining good insert performance."
      ],
      "metadata": {
        "id": "AjOjM-swVRaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load all dimension tables to Supabase\n",
        "# Using pandas to_sql for efficient bulk loading\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"LOADING DIMENSION TABLES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nLoading dimension tables to Supabase...\\n\")\n",
        "print(\"This will take a few minutes...\\n\")\n",
        "\n",
        "# First, truncate all dimension tables (in reverse dependency order)\n",
        "# This clears existing data without dropping tables\n",
        "print(\"Clearing existing data from dimension tables...\\n\")\n",
        "\n",
        "conn = psycopg2.connect(DATABASE_URL)\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Truncate in reverse dependency order (child tables first)\n",
        "tables_to_truncate = [\n",
        "    'dim_security',  # Has FK to dim_asset_class\n",
        "    'dim_trader',\n",
        "    'dim_account',\n",
        "    'fact_trades',  # If exists\n",
        "    'fact_portfolio_snapshots',  # If exists\n",
        "    'dim_asset_class',\n",
        "    'dim_sector',\n",
        "    'dim_exchange',\n",
        "    'dim_counterparty',\n",
        "    'dim_strategy',\n",
        "    'dim_trade_attributes',\n",
        "    'dim_time',\n",
        "    'dim_date',\n",
        "]\n",
        "\n",
        "for table in tables_to_truncate:\n",
        "    try:\n",
        "        cur.execute(f\"TRUNCATE TABLE {table} CASCADE;\")\n",
        "        print(f\"   ‚úÖ Cleared {table}\")\n",
        "    except Exception as e:\n",
        "        if \"does not exist\" in str(e).lower():\n",
        "            print(f\"   ‚ö†Ô∏è {table} doesn't exist yet (will create)\")\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è Could not truncate {table}: {str(e)[:60]}\")\n",
        "\n",
        "conn.commit()\n",
        "cur.close()\n",
        "conn.close()\n",
        "\n",
        "print(\"\\n‚úÖ Tables cleared. Now loading data...\\n\")\n",
        "\n",
        "# Load dimensions in order (respecting foreign key dependencies)\n",
        "dimensions_to_load = [\n",
        "    ('dim_date', dim_date),\n",
        "    ('dim_time', dim_time),\n",
        "    ('dim_asset_class', dim_asset_class),\n",
        "    ('dim_sector', dim_sector),\n",
        "    ('dim_exchange', dim_exchange),\n",
        "    ('dim_counterparty', dim_counterparty),\n",
        "    ('dim_strategy', dim_strategy),\n",
        "    ('dim_trade_attributes', dim_trade_attributes),\n",
        "    ('dim_security', dim_security),\n",
        "    ('dim_trader', dim_trader),\n",
        "    ('dim_account', dim_account),\n",
        "]\n",
        "\n",
        "for table_name, df in dimensions_to_load:\n",
        "    print(f\"Loading {table_name}... ({len(df):,} rows)\")\n",
        "\n",
        "    try:\n",
        "        # Use append since we've already truncated\n",
        "        df.to_sql(\n",
        "            table_name,\n",
        "            engine,\n",
        "            if_exists='append',  # Append since we truncated above\n",
        "            index=False,\n",
        "            method='multi',  # Bulk insert\n",
        "            chunksize=1000\n",
        "        )\n",
        "        print(f\"   ‚úÖ Loaded {len(df):,} rows to {table_name}\")\n",
        "    except Exception as e:\n",
        "        error_msg = str(e)\n",
        "        if \"already exists\" in error_msg.lower() or \"duplicate\" in error_msg.lower():\n",
        "            # Try truncate and reload\n",
        "            try:\n",
        "                conn = psycopg2.connect(DATABASE_URL)\n",
        "                cur = conn.cursor()\n",
        "                cur.execute(f\"TRUNCATE TABLE {table_name} CASCADE;\")\n",
        "                conn.commit()\n",
        "                cur.close()\n",
        "                conn.close()\n",
        "\n",
        "                df.to_sql(\n",
        "                    table_name,\n",
        "                    engine,\n",
        "                    if_exists='append',\n",
        "                    index=False,\n",
        "                    method='multi',\n",
        "                    chunksize=1000\n",
        "                )\n",
        "                print(f\"   ‚úÖ Loaded {len(df):,} rows to {table_name} (after truncate)\")\n",
        "            except Exception as e2:\n",
        "                print(f\"   ‚ùå Failed to load {table_name}: {str(e2)[:100]}\")\n",
        "        else:\n",
        "            print(f\"   ‚ùå Error loading {table_name}: {str(e)[:100]}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ ALL DIMENSION TABLES LOADED!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Verify counts\n",
        "print(\"\\nVerifying row counts...\\n\")\n",
        "conn = psycopg2.connect(DATABASE_URL)\n",
        "cur = conn.cursor()\n",
        "\n",
        "for table_name, _ in dimensions_to_load:\n",
        "    try:\n",
        "        cur.execute(f\"SELECT COUNT(*) FROM {table_name};\")\n",
        "        count = cur.fetchone()[0]\n",
        "        expected = len(dimensions_to_load[dimensions_to_load.index((table_name, _))][1])\n",
        "        if count == expected:\n",
        "            print(f\"   ‚úÖ {table_name}: {count:,} rows (expected {expected:,})\")\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è {table_name}: {count:,} rows (expected {expected:,})\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå {table_name}: Error checking count - {str(e)[:60]}\")\n",
        "\n",
        "cur.close()\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tTdfhl8VI1W",
        "outputId": "29d0d73c-ffb4-40b5-eca3-d1a77078c7f3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "LOADING DIMENSION TABLES\n",
            "======================================================================\n",
            "\n",
            "Loading dimension tables to Supabase...\n",
            "\n",
            "This will take a few minutes...\n",
            "\n",
            "Clearing existing data from dimension tables...\n",
            "\n",
            "   ‚úÖ Cleared dim_security\n",
            "   ‚úÖ Cleared dim_trader\n",
            "   ‚úÖ Cleared dim_account\n",
            "   ‚úÖ Cleared fact_trades\n",
            "   ‚úÖ Cleared fact_portfolio_snapshots\n",
            "   ‚úÖ Cleared dim_asset_class\n",
            "   ‚úÖ Cleared dim_sector\n",
            "   ‚úÖ Cleared dim_exchange\n",
            "   ‚úÖ Cleared dim_counterparty\n",
            "   ‚úÖ Cleared dim_strategy\n",
            "   ‚úÖ Cleared dim_trade_attributes\n",
            "   ‚úÖ Cleared dim_time\n",
            "   ‚úÖ Cleared dim_date\n",
            "\n",
            "‚úÖ Tables cleared. Now loading data...\n",
            "\n",
            "Loading dim_date... (729 rows)\n",
            "   ‚úÖ Loaded 729 rows to dim_date\n",
            "Loading dim_time... (390 rows)\n",
            "   ‚úÖ Loaded 390 rows to dim_time\n",
            "Loading dim_asset_class... (4 rows)\n",
            "   ‚úÖ Loaded 4 rows to dim_asset_class\n",
            "Loading dim_sector... (12 rows)\n",
            "   ‚úÖ Loaded 12 rows to dim_sector\n",
            "Loading dim_exchange... (6 rows)\n",
            "   ‚úÖ Loaded 6 rows to dim_exchange\n",
            "Loading dim_counterparty... (30 rows)\n",
            "   ‚úÖ Loaded 30 rows to dim_counterparty\n",
            "Loading dim_strategy... (12 rows)\n",
            "   ‚úÖ Loaded 12 rows to dim_strategy\n",
            "Loading dim_trade_attributes... (24 rows)\n",
            "   ‚úÖ Loaded 24 rows to dim_trade_attributes\n",
            "Loading dim_security... (413 rows)\n",
            "   ‚ùå Error loading dim_security: (psycopg2.errors.UndefinedColumn) column \"asset_class\" of relation \"dim_security\" does not exist\n",
            "LIN\n",
            "Loading dim_trader... (50 rows)\n",
            "   ‚úÖ Loaded 50 rows to dim_trader\n",
            "Loading dim_account... (200 rows)\n",
            "   ‚úÖ Loaded 200 rows to dim_account\n",
            "\n",
            "======================================================================\n",
            "‚úÖ ALL DIMENSION TABLES LOADED!\n",
            "======================================================================\n",
            "\n",
            "Verifying row counts...\n",
            "\n",
            "   ‚úÖ dim_date: 729 rows (expected 729)\n",
            "   ‚úÖ dim_time: 390 rows (expected 390)\n",
            "   ‚úÖ dim_asset_class: 4 rows (expected 4)\n",
            "   ‚úÖ dim_sector: 12 rows (expected 12)\n",
            "   ‚úÖ dim_exchange: 6 rows (expected 6)\n",
            "   ‚úÖ dim_counterparty: 30 rows (expected 30)\n",
            "   ‚úÖ dim_strategy: 12 rows (expected 12)\n",
            "   ‚úÖ dim_trade_attributes: 24 rows (expected 24)\n",
            "   ‚ö†Ô∏è dim_security: 0 rows (expected 413)\n",
            "   ‚úÖ dim_trader: 50 rows (expected 50)\n",
            "   ‚úÖ dim_account: 200 rows (expected 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix and load dim_security (only this table failed)\n",
        "# Map asset_class string to asset_class_key integer\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"FIXING AND LOADING dim_security\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nFixing dim_security data to match table schema...\\n\")\n",
        "\n",
        "# Create a copy to avoid modifying original\n",
        "dim_security_fixed = dim_security.copy()\n",
        "\n",
        "# Map asset_class string to asset_class_key\n",
        "# The table expects asset_class_key (FK), but we have asset_class (string)\n",
        "asset_class_map = {\n",
        "    'EQUITY': 1,\n",
        "    'OPTION': 2,\n",
        "    'FUTURE': 3,\n",
        "    'BOND': 4\n",
        "}\n",
        "\n",
        "# Map asset_class to asset_class_key\n",
        "if 'asset_class' in dim_security_fixed.columns:\n",
        "    dim_security_fixed['asset_class_key'] = dim_security_fixed['asset_class'].map(asset_class_map).fillna(1)  # Default to EQUITY\n",
        "    # Drop the asset_class column if it exists (we use asset_class_key instead)\n",
        "    if 'asset_class' in dim_security_fixed.columns:\n",
        "        dim_security_fixed = dim_security_fixed.drop('asset_class', axis=1)\n",
        "\n",
        "# Ensure all required columns exist and match table schema\n",
        "# Get the actual table columns from database\n",
        "conn = psycopg2.connect(DATABASE_URL)\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"\"\"\n",
        "    SELECT column_name\n",
        "    FROM information_schema.columns\n",
        "    WHERE table_name = 'dim_security'\n",
        "    ORDER BY ordinal_position;\n",
        "\"\"\")\n",
        "db_columns = [row[0] for row in cur.fetchall()]\n",
        "cur.close()\n",
        "conn.close()\n",
        "\n",
        "print(f\"Table expects columns: {db_columns[:10]}...\")\n",
        "\n",
        "# Select only columns that exist in both DataFrame and table\n",
        "columns_to_keep = [col for col in dim_security_fixed.columns if col in db_columns]\n",
        "dim_security_fixed = dim_security_fixed[columns_to_keep]\n",
        "\n",
        "# Add any missing required columns with defaults\n",
        "if 'asset_class_key' not in dim_security_fixed.columns:\n",
        "    dim_security_fixed['asset_class_key'] = 1  # Default to EQUITY\n",
        "if 'lot_size' not in dim_security_fixed.columns:\n",
        "    dim_security_fixed['lot_size'] = 1\n",
        "if 'tick_size' not in dim_security_fixed.columns:\n",
        "    dim_security_fixed['tick_size'] = 0.01\n",
        "if 'multiplier' not in dim_security_fixed.columns:\n",
        "    dim_security_fixed['multiplier'] = 1.0\n",
        "\n",
        "# Ensure column order matches table\n",
        "dim_security_fixed = dim_security_fixed[[col for col in db_columns if col in dim_security_fixed.columns]]\n",
        "\n",
        "print(f\"‚úÖ Fixed dim_security: {len(dim_security_fixed)} rows, {len(dim_security_fixed.columns)} columns\")\n",
        "print(f\"   Columns to load: {list(dim_security_fixed.columns)[:10]}...\")\n",
        "\n",
        "# Clear existing data first\n",
        "print(\"\\nClearing existing dim_security data...\")\n",
        "conn = psycopg2.connect(DATABASE_URL)\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"TRUNCATE TABLE dim_security CASCADE;\")\n",
        "conn.commit()\n",
        "cur.close()\n",
        "conn.close()\n",
        "print(\"   ‚úÖ Cleared\")\n",
        "\n",
        "# Now load dim_security with fixed data\n",
        "print(f\"\\nLoading dim_security (fixed)... ({len(dim_security_fixed):,} rows)\")\n",
        "\n",
        "try:\n",
        "    dim_security_fixed.to_sql(\n",
        "        'dim_security',\n",
        "        engine,\n",
        "        if_exists='append',\n",
        "        index=False,\n",
        "        method='multi',\n",
        "        chunksize=1000\n",
        "    )\n",
        "    print(f\"   ‚úÖ Loaded {len(dim_security_fixed):,} rows to dim_security\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Error loading dim_security: {str(e)[:150]}\")\n",
        "    raise\n",
        "\n",
        "# Verify dim_security was loaded\n",
        "conn = psycopg2.connect(DATABASE_URL)\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"SELECT COUNT(*) FROM dim_security;\")\n",
        "count = cur.fetchone()[0]\n",
        "print(f\"\\n‚úÖ dim_security now has {count:,} rows (expected {len(dim_security):,})\")\n",
        "\n",
        "if count == len(dim_security):\n",
        "    print(\"‚úÖ All rows loaded successfully!\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Row count mismatch - expected {len(dim_security)}, got {count}\")\n",
        "\n",
        "cur.close()\n",
        "conn.close()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ dim_security LOADED SUCCESSFULLY!\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rr5-_L-zVVPN",
        "outputId": "7c815655-9418-4e0b-c570-cceb1f34b786"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "FIXING AND LOADING dim_security\n",
            "======================================================================\n",
            "\n",
            "Fixing dim_security data to match table schema...\n",
            "\n",
            "Table expects columns: ['security_key', 'security_id', 'ticker_symbol', 'security_name', 'asset_class_key', 'sector', 'industry', 'currency_code', 'exchange_listed', 'market_cap']...\n",
            "‚úÖ Fixed dim_security: 413 rows, 24 columns\n",
            "   Columns to load: ['security_key', 'security_id', 'ticker_symbol', 'security_name', 'asset_class_key', 'sector', 'industry', 'currency_code', 'exchange_listed', 'market_cap']...\n",
            "\n",
            "Clearing existing dim_security data...\n",
            "   ‚úÖ Cleared\n",
            "\n",
            "Loading dim_security (fixed)... (413 rows)\n",
            "   ‚úÖ Loaded 413 rows to dim_security\n",
            "\n",
            "‚úÖ dim_security now has 413 rows (expected 413)\n",
            "‚úÖ All rows loaded successfully!\n",
            "\n",
            "======================================================================\n",
            "‚úÖ dim_security LOADED SUCCESSFULLY!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I loaded the fact_trades data into the partitioned table. PostgreSQL automatically routes each row to the correct monthly partition based on the trade_timestamp value. I loaded the data in batches of 50,000 rows to avoid memory issues and provide progress feedback. The partitioned structure means queries filtering by date will only scan relevant partitions, dramatically improving performance. I verified the row count and showed the distribution across partitions to confirm data was routed correctly."
      ],
      "metadata": {
        "id": "MV0mKLlekz_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Kxx4zKhirO0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Diagnose partition issue - check data range and existing partitions\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DIAGNOSING PARTITION ISSUE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Check the date range of fact_trades data\n",
        "print(\"\\n1. Checking fact_trades data date range...\")\n",
        "print(f\"   Min timestamp: {fact_trades['trade_timestamp'].min()}\")\n",
        "print(f\"   Max timestamp: {fact_trades['trade_timestamp'].max()}\")\n",
        "print(f\"   Date range: {(fact_trades['trade_timestamp'].max() - fact_trades['trade_timestamp'].min()).days} days\")\n",
        "\n",
        "# Check what partitions exist in the database\n",
        "print(\"\\n2. Checking existing partitions in database...\")\n",
        "conn = psycopg2.connect(DATABASE_URL)\n",
        "cur = conn.cursor()\n",
        "\n",
        "try:\n",
        "    cur.execute(\"\"\"\n",
        "        SELECT\n",
        "            schemaname,\n",
        "            tablename,\n",
        "            pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size\n",
        "        FROM pg_tables\n",
        "        WHERE tablename LIKE 'fact_trades_%'\n",
        "        ORDER BY tablename;\n",
        "    \"\"\")\n",
        "    partitions = cur.fetchall()\n",
        "\n",
        "    if partitions:\n",
        "        print(f\"   Found {len(partitions)} partitions:\")\n",
        "        for schema, table, size in partitions:\n",
        "            print(f\"      {table}: {size}\")\n",
        "    else:\n",
        "        print(\"   ‚ùå No partitions found!\")\n",
        "\n",
        "    # Check parent table\n",
        "    cur.execute(\"\"\"\n",
        "        SELECT EXISTS (\n",
        "            SELECT 1 FROM information_schema.tables\n",
        "            WHERE table_name = 'fact_trades'\n",
        "        );\n",
        "    \"\"\")\n",
        "    parent_exists = cur.fetchone()[0]\n",
        "    print(f\"\\n   Parent table 'fact_trades' exists: {parent_exists}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Error checking partitions: {e}\")\n",
        "\n",
        "cur.close()\n",
        "conn.close()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFCQCKdmq02k",
        "outputId": "a0e4416a-f37b-45d6-b216-5272dcabd451"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "DIAGNOSING PARTITION ISSUE\n",
            "======================================================================\n",
            "\n",
            "1. Checking fact_trades data date range...\n",
            "   Min timestamp: 2023-01-03 09:30:00\n",
            "   Max timestamp: 2024-12-31 15:59:52\n",
            "   Date range: 728 days\n",
            "\n",
            "2. Checking existing partitions in database...\n",
            "   Found 23 partitions:\n",
            "      fact_trades_2023_02: 8192 bytes\n",
            "      fact_trades_2023_03: 32 kB\n",
            "      fact_trades_2023_04: 80 kB\n",
            "      fact_trades_2023_05: 104 kB\n",
            "      fact_trades_2023_06: 104 kB\n",
            "      fact_trades_2023_07: 96 kB\n",
            "      fact_trades_2023_08: 96 kB\n",
            "      fact_trades_2023_09: 88 kB\n",
            "      fact_trades_2023_10: 96 kB\n",
            "      fact_trades_2023_11: 96 kB\n",
            "      fact_trades_2023_12: 88 kB\n",
            "      fact_trades_2024_01: 112 kB\n",
            "      fact_trades_2024_02: 104 kB\n",
            "      fact_trades_2024_03: 112 kB\n",
            "      fact_trades_2024_04: 120 kB\n",
            "      fact_trades_2024_05: 120 kB\n",
            "      fact_trades_2024_06: 120 kB\n",
            "      fact_trades_2024_07: 128 kB\n",
            "      fact_trades_2024_08: 128 kB\n",
            "      fact_trades_2024_09: 128 kB\n",
            "      fact_trades_2024_10: 136 kB\n",
            "      fact_trades_2024_11: 128 kB\n",
            "      fact_trades_2024_12: 136 kB\n",
            "\n",
            "   Parent table 'fact_trades' exists: True\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm checking the date range of the fact_trades data and what partitions currently exist in the database. This will help us understand why the data isn't loading - either partitions don't exist, or they don't cover the date range of our data."
      ],
      "metadata": {
        "id": "4KoxVqGerQwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create partitions for the entire date range of fact_trades data\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CREATING MISSING PARTITIONS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Get date range from fact_trades\n",
        "min_date = pd.to_datetime(fact_trades['trade_timestamp'].min()).date()\n",
        "max_date = pd.to_datetime(fact_trades['trade_timestamp'].max()).date()\n",
        "\n",
        "print(f\"\\nData date range: {min_date} to {max_date}\")\n",
        "\n",
        "# Generate monthly partitions\n",
        "# Start from the first day of the month containing min_date\n",
        "# End at the first day of the month after max_date\n",
        "start_month = min_date.replace(day=1)\n",
        "end_month = (max_date.replace(day=28) + timedelta(days=4)).replace(day=1)  # First of next month\n",
        "\n",
        "print(f\"Creating partitions from: {start_month} to {end_month}\")\n",
        "\n",
        "conn = psycopg2.connect(DATABASE_URL)\n",
        "cur = conn.cursor()\n",
        "\n",
        "# First, check if parent table exists and is partitioned\n",
        "try:\n",
        "    cur.execute(\"\"\"\n",
        "        SELECT\n",
        "            EXISTS (\n",
        "                SELECT 1 FROM information_schema.tables\n",
        "                WHERE table_name = 'fact_trades'\n",
        "            ) as table_exists,\n",
        "            EXISTS (\n",
        "                SELECT 1 FROM pg_class c\n",
        "                JOIN pg_namespace n ON n.oid = c.relnamespace\n",
        "                WHERE c.relname = 'fact_trades'\n",
        "                AND c.relkind = 'p'\n",
        "            ) as is_partitioned;\n",
        "    \"\"\")\n",
        "    result = cur.fetchone()\n",
        "    table_exists, is_partitioned = result\n",
        "\n",
        "    print(f\"\\nParent table exists: {table_exists}\")\n",
        "    print(f\"Parent table is partitioned: {is_partitioned}\")\n",
        "\n",
        "    if not table_exists:\n",
        "        print(\"\\n‚ùå Parent table 'fact_trades' does not exist!\")\n",
        "        print(\"   Please run Cell 16 (Create Partitioned Fact Tables) first.\")\n",
        "        cur.close()\n",
        "        conn.close()\n",
        "    elif not is_partitioned:\n",
        "        print(\"\\n‚ùå Parent table 'fact_trades' exists but is not partitioned!\")\n",
        "        print(\"   Please run Cell 16 (Create Partitioned Fact Tables) first.\")\n",
        "        cur.close()\n",
        "        conn.close()\n",
        "    else:\n",
        "        # Create monthly partitions\n",
        "        current_month = start_month\n",
        "        partitions_created = 0\n",
        "        partitions_existing = 0\n",
        "\n",
        "        while current_month < end_month:\n",
        "            # Calculate partition boundaries\n",
        "            partition_start = current_month\n",
        "            if current_month.month == 12:\n",
        "                partition_end = current_month.replace(year=current_month.year + 1, month=1)\n",
        "            else:\n",
        "                partition_end = current_month.replace(month=current_month.month + 1)\n",
        "\n",
        "            partition_name = f\"fact_trades_{partition_start.year}_{partition_start.month:02d}\"\n",
        "\n",
        "            # Check if partition already exists\n",
        "            cur.execute(\"\"\"\n",
        "                SELECT EXISTS (\n",
        "                    SELECT 1 FROM pg_class c\n",
        "                    JOIN pg_namespace n ON n.oid = c.relnamespace\n",
        "                    WHERE c.relname = %s\n",
        "                );\n",
        "            \"\"\", (partition_name,))\n",
        "            exists = cur.fetchone()[0]\n",
        "\n",
        "            if exists:\n",
        "                print(f\"   ‚è≠Ô∏è  {partition_name} already exists\")\n",
        "                partitions_existing += 1\n",
        "            else:\n",
        "                # Create partition\n",
        "                try:\n",
        "                    create_sql = f\"\"\"\n",
        "                        CREATE TABLE IF NOT EXISTS {partition_name}\n",
        "                        PARTITION OF fact_trades\n",
        "                        FOR VALUES FROM ('{partition_start}') TO ('{partition_end}');\n",
        "                    \"\"\"\n",
        "                    cur.execute(create_sql)\n",
        "                    print(f\"   ‚úÖ Created {partition_name} ({partition_start} to {partition_end})\")\n",
        "                    partitions_created += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"   ‚ùå Error creating {partition_name}: {str(e)[:80]}\")\n",
        "\n",
        "            # Move to next month\n",
        "            if current_month.month == 12:\n",
        "                current_month = current_month.replace(year=current_month.year + 1, month=1)\n",
        "            else:\n",
        "                current_month = current_month.replace(month=current_month.month + 1)\n",
        "\n",
        "        conn.commit()\n",
        "\n",
        "        print(f\"\\n‚úÖ Partition creation complete!\")\n",
        "        print(f\"   Created: {partitions_created} partitions\")\n",
        "        print(f\"   Already existed: {partitions_existing} partitions\")\n",
        "\n",
        "        # Verify partitions\n",
        "        cur.execute(\"\"\"\n",
        "            SELECT\n",
        "                tablename,\n",
        "                pg_size_pretty(pg_total_relation_size('public.'||tablename)) as size\n",
        "            FROM pg_tables\n",
        "            WHERE tablename LIKE 'fact_trades_%'\n",
        "            ORDER BY tablename;\n",
        "        \"\"\")\n",
        "        all_partitions = cur.fetchall()\n",
        "        print(f\"\\nüìä Total partitions: {len(all_partitions)}\")\n",
        "        for table, size in all_partitions:\n",
        "            print(f\"   {table}: {size}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error: {e}\")\n",
        "    conn.rollback()\n",
        "\n",
        "cur.close()\n",
        "conn.close()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ READY TO LOAD DATA!\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYqYBBBPrUK5",
        "outputId": "950fd866-4378-4daa-db30-6c5cb6ec7fc4"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "CREATING MISSING PARTITIONS\n",
            "======================================================================\n",
            "\n",
            "Data date range: 2023-01-03 to 2024-12-31\n",
            "Creating partitions from: 2023-01-01 to 2025-01-01\n",
            "\n",
            "Parent table exists: True\n",
            "Parent table is partitioned: True\n",
            "   ‚úÖ Created fact_trades_2023_01 (2023-01-01 to 2023-02-01)\n",
            "   ‚è≠Ô∏è  fact_trades_2023_02 already exists\n",
            "   ‚è≠Ô∏è  fact_trades_2023_03 already exists\n",
            "   ‚è≠Ô∏è  fact_trades_2023_04 already exists\n",
            "   ‚è≠Ô∏è  fact_trades_2023_05 already exists\n",
            "   ‚è≠Ô∏è  fact_trades_2023_06 already exists\n",
            "   ‚è≠Ô∏è  fact_trades_2023_07 already exists\n",
            "   ‚è≠Ô∏è  fact_trades_2023_08 already exists\n",
            "   ‚è≠Ô∏è  fact_trades_2023_09 already exists\n",
            "   ‚è≠Ô∏è  fact_trades_2023_10 already exists\n",
            "   ‚è≠Ô∏è  fact_trades_2023_11 already exists\n",
            "   ‚è≠Ô∏è  fact_trades_2023_12 already exists\n",
            "   ‚è≠Ô∏è  fact_trades_2024_01 already exists\n",
            "   ‚è≠Ô∏è  fact_trades_2024_02 already exists\n",
            "   ‚è≠Ô∏è  fact_trades_2024_03 already exists\n",
            "   ‚è≠Ô∏è  fact_trades_2024_04 already exists\n",
            "   ‚è≠Ô∏è  fact_trades_2024_05 already exists\n",
            "   ‚è≠Ô∏è  fact_trades_2024_06 already exists\n",
            "   ‚è≠Ô∏è  fact_trades_2024_07 already exists\n",
            "   ‚è≠Ô∏è  fact_trades_2024_08 already exists\n",
            "   ‚è≠Ô∏è  fact_trades_2024_09 already exists\n",
            "   ‚è≠Ô∏è  fact_trades_2024_10 already exists\n",
            "   ‚è≠Ô∏è  fact_trades_2024_11 already exists\n",
            "   ‚è≠Ô∏è  fact_trades_2024_12 already exists\n",
            "\n",
            "‚úÖ Partition creation complete!\n",
            "   Created: 1 partitions\n",
            "   Already existed: 23 partitions\n",
            "\n",
            "üìä Total partitions: 24\n",
            "   fact_trades_2023_01: 8192 bytes\n",
            "   fact_trades_2023_02: 8192 bytes\n",
            "   fact_trades_2023_03: 32 kB\n",
            "   fact_trades_2023_04: 80 kB\n",
            "   fact_trades_2023_05: 104 kB\n",
            "   fact_trades_2023_06: 104 kB\n",
            "   fact_trades_2023_07: 96 kB\n",
            "   fact_trades_2023_08: 96 kB\n",
            "   fact_trades_2023_09: 88 kB\n",
            "   fact_trades_2023_10: 96 kB\n",
            "   fact_trades_2023_11: 96 kB\n",
            "   fact_trades_2023_12: 88 kB\n",
            "   fact_trades_2024_01: 112 kB\n",
            "   fact_trades_2024_02: 104 kB\n",
            "   fact_trades_2024_03: 112 kB\n",
            "   fact_trades_2024_04: 120 kB\n",
            "   fact_trades_2024_05: 120 kB\n",
            "   fact_trades_2024_06: 120 kB\n",
            "   fact_trades_2024_07: 128 kB\n",
            "   fact_trades_2024_08: 128 kB\n",
            "   fact_trades_2024_09: 128 kB\n",
            "   fact_trades_2024_10: 136 kB\n",
            "   fact_trades_2024_11: 128 kB\n",
            "   fact_trades_2024_12: 136 kB\n",
            "\n",
            "======================================================================\n",
            "‚úÖ READY TO LOAD DATA!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm creating monthly partitions for the entire date range covered by the fact_trades data. The code automatically calculates the start and end months from the data, then creates a partition for each month. It checks if partitions already exist to avoid duplicates. After creating partitions, it verifies all partitions are in place."
      ],
      "metadata": {
        "id": "aNpnrJ3criL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load fact_trades data into partitioned table\n",
        "# Data will automatically route to correct partitions\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"LOADING FACT_TRADES (PARTITIONED)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nLoading trade data to partitioned table...\")\n",
        "print(\"Data will automatically route to correct monthly partitions...\\n\")\n",
        "print(f\"Total trades to load: {len(fact_trades):,}\\n\")\n",
        "print(\"‚ö†Ô∏è This will take 10-15 minutes for 961,100 rows...\\n\")\n",
        "\n",
        "# Prepare fact_trades data\n",
        "# Ensure trade_timestamp is datetime\n",
        "fact_trades['trade_timestamp'] = pd.to_datetime(fact_trades['trade_timestamp'])\n",
        "\n",
        "# Select columns in correct order (matching table schema)\n",
        "columns_to_load = [\n",
        "    'trade_timestamp', 'date_key', 'time_key', 'security_key', 'trader_key',\n",
        "    'account_key', 'exchange_key', 'counterparty_key', 'strategy_key', 'attributes_key',\n",
        "    'trade_type', 'quantity', 'price', 'trade_value', 'commission', 'net_proceeds',\n",
        "    'realized_pnl', 'portfolio_exposure', 'margin_used', 'order_id', 'execution_venue',\n",
        "    'settlement_date'\n",
        "]\n",
        "\n",
        "fact_trades_clean = fact_trades[columns_to_load].copy()\n",
        "\n",
        "# Convert settlement_date to date if it's not already\n",
        "if 'settlement_date' in fact_trades_clean.columns:\n",
        "    fact_trades_clean['settlement_date'] = pd.to_datetime(fact_trades_clean['settlement_date']).dt.date\n",
        "\n",
        "# Ensure all numeric columns are proper types\n",
        "fact_trades_clean['quantity'] = pd.to_numeric(fact_trades_clean['quantity'], errors='coerce')\n",
        "fact_trades_clean['price'] = pd.to_numeric(fact_trades_clean['price'], errors='coerce')\n",
        "fact_trades_clean['trade_value'] = pd.to_numeric(fact_trades_clean['trade_value'], errors='coerce')\n",
        "fact_trades_clean['commission'] = pd.to_numeric(fact_trades_clean['commission'], errors='coerce')\n",
        "fact_trades_clean['net_proceeds'] = pd.to_numeric(fact_trades_clean['net_proceeds'], errors='coerce')\n",
        "fact_trades_clean['realized_pnl'] = pd.to_numeric(fact_trades_clean['realized_pnl'], errors='coerce')\n",
        "fact_trades_clean['portfolio_exposure'] = pd.to_numeric(fact_trades_clean['portfolio_exposure'], errors='coerce')\n",
        "fact_trades_clean['margin_used'] = pd.to_numeric(fact_trades_clean['margin_used'], errors='coerce')\n",
        "\n",
        "# Remove any rows with null critical fields\n",
        "initial_count = len(fact_trades_clean)\n",
        "fact_trades_clean = fact_trades_clean.dropna(subset=['trade_timestamp', 'security_key', 'trader_key', 'account_key'])\n",
        "print(f\"Cleaned data: {len(fact_trades_clean):,} rows (removed {initial_count - len(fact_trades_clean):,} rows with nulls)\")\n",
        "\n",
        "print(\"\\nLoading in batches for better performance...\")\n",
        "\n",
        "# Load in batches to avoid memory issues and provide progress\n",
        "batch_size = 50000\n",
        "total_batches = (len(fact_trades_clean) + batch_size - 1) // batch_size\n",
        "\n",
        "successful_batches = 0\n",
        "failed_batches = 0\n",
        "\n",
        "for i in range(0, len(fact_trades_clean), batch_size):\n",
        "    batch = fact_trades_clean.iloc[i:i+batch_size]\n",
        "    batch_num = (i // batch_size) + 1\n",
        "\n",
        "    print(f\"  Loading batch {batch_num}/{total_batches} ({len(batch):,} rows)...\", end=' ')\n",
        "\n",
        "    try:\n",
        "        batch.to_sql(\n",
        "            'fact_trades',\n",
        "            engine,\n",
        "            if_exists='append',\n",
        "            index=False,\n",
        "            method='multi',\n",
        "            chunksize=5000\n",
        "        )\n",
        "        print(f\"‚úÖ\")\n",
        "        successful_batches += 1\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {str(e)[:80]}\")\n",
        "        failed_batches += 1\n",
        "        # Continue with next batch\n",
        "        continue\n",
        "\n",
        "print(f\"\\n‚úÖ Completed loading fact_trades\")\n",
        "print(f\"   Successful batches: {successful_batches}/{total_batches}\")\n",
        "print(f\"   Failed batches: {failed_batches}\")\n",
        "\n",
        "# Verify data loaded\n",
        "conn = psycopg2.connect(DATABASE_URL)\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"SELECT COUNT(*) FROM fact_trades;\")\n",
        "count = cur.fetchone()[0]\n",
        "print(f\"   Rows in database: {count:,}\")\n",
        "print(f\"   Expected: {len(fact_trades_clean):,}\")\n",
        "\n",
        "# Check partition distribution\n",
        "print(\"\\nüìä Partition distribution:\")\n",
        "cur.execute(\"\"\"\n",
        "    SELECT\n",
        "        tableoid::regclass as partition_name,\n",
        "        COUNT(*) as row_count\n",
        "    FROM fact_trades\n",
        "    GROUP BY tableoid::regclass\n",
        "    ORDER BY partition_name;\n",
        "\"\"\")\n",
        "partitions = cur.fetchall()\n",
        "for partition, row_count in partitions:\n",
        "    print(f\"   {partition}: {row_count:,} rows\")\n",
        "\n",
        "cur.close()\n",
        "conn.close()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ FACT_TRADES LOADED SUCCESSFULLY!\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2j2-hJCkNpf",
        "outputId": "8c56dd59-c410-4645-986c-bc2d09faa9e2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "LOADING FACT_TRADES (PARTITIONED)\n",
            "======================================================================\n",
            "\n",
            "Loading trade data to partitioned table...\n",
            "Data will automatically route to correct monthly partitions...\n",
            "\n",
            "Total trades to load: 961,100\n",
            "\n",
            "‚ö†Ô∏è This will take 10-15 minutes for 961,100 rows...\n",
            "\n",
            "Cleaned data: 961,100 rows (removed 0 rows with nulls)\n",
            "\n",
            "Loading in batches for better performance...\n",
            "  Loading batch 1/20 (50,000 rows)... ‚úÖ\n",
            "  Loading batch 2/20 (50,000 rows)... ‚úÖ\n",
            "  Loading batch 3/20 (50,000 rows)... ‚úÖ\n",
            "  Loading batch 4/20 (50,000 rows)... ‚úÖ\n",
            "  Loading batch 5/20 (50,000 rows)... ‚úÖ\n",
            "  Loading batch 6/20 (50,000 rows)... ‚úÖ\n",
            "  Loading batch 7/20 (50,000 rows)... ‚úÖ\n",
            "  Loading batch 8/20 (50,000 rows)... ‚úÖ\n",
            "  Loading batch 9/20 (50,000 rows)... ‚úÖ\n",
            "  Loading batch 10/20 (50,000 rows)... ‚úÖ\n",
            "  Loading batch 11/20 (50,000 rows)... ‚úÖ\n",
            "  Loading batch 12/20 (50,000 rows)... ‚úÖ\n",
            "  Loading batch 13/20 (50,000 rows)... ‚úÖ\n",
            "  Loading batch 14/20 (50,000 rows)... ‚úÖ\n",
            "  Loading batch 15/20 (50,000 rows)... ‚úÖ\n",
            "  Loading batch 16/20 (50,000 rows)... ‚úÖ\n",
            "  Loading batch 17/20 (50,000 rows)... ‚úÖ\n",
            "  Loading batch 18/20 (50,000 rows)... ‚úÖ\n",
            "  Loading batch 19/20 (50,000 rows)... ‚úÖ\n",
            "  Loading batch 20/20 (11,100 rows)... ‚úÖ\n",
            "\n",
            "‚úÖ Completed loading fact_trades\n",
            "   Successful batches: 20/20\n",
            "   Failed batches: 0\n",
            "   Rows in database: 961,100\n",
            "   Expected: 961,100\n",
            "\n",
            "üìä Partition distribution:\n",
            "   fact_trades_2023_02: 36,297 rows\n",
            "   fact_trades_2023_03: 47,143 rows\n",
            "   fact_trades_2023_04: 34,678 rows\n",
            "   fact_trades_2023_05: 43,396 rows\n",
            "   fact_trades_2023_06: 41,887 rows\n",
            "   fact_trades_2023_07: 37,685 rows\n",
            "   fact_trades_2023_08: 40,743 rows\n",
            "   fact_trades_2023_09: 36,822 rows\n",
            "   fact_trades_2023_10: 42,374 rows\n",
            "   fact_trades_2023_11: 40,734 rows\n",
            "   fact_trades_2023_12: 39,788 rows\n",
            "   fact_trades_2024_01: 41,556 rows\n",
            "   fact_trades_2024_02: 40,299 rows\n",
            "   fact_trades_2024_03: 39,827 rows\n",
            "   fact_trades_2024_04: 40,789 rows\n",
            "   fact_trades_2024_05: 40,990 rows\n",
            "   fact_trades_2024_06: 36,709 rows\n",
            "   fact_trades_2024_07: 40,551 rows\n",
            "   fact_trades_2024_08: 40,758 rows\n",
            "   fact_trades_2024_09: 38,090 rows\n",
            "   fact_trades_2024_10: 40,945 rows\n",
            "   fact_trades_2024_11: 39,990 rows\n",
            "   fact_trades_2024_12: 40,396 rows\n",
            "   fact_trades_2023_01: 38,653 rows\n",
            "\n",
            "======================================================================\n",
            "‚úÖ FACT_TRADES LOADED SUCCESSFULLY!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load fact_portfolio_snapshots\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"LOADING FACT_PORTFOLIO_SNAPSHOTS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nLoading portfolio snapshots... ({len(fact_portfolio_snapshots):,} rows)\\n\")\n",
        "\n",
        "# Prepare data\n",
        "columns_to_load = [\n",
        "    'snapshot_date_key', 'account_key', 'security_key', 'position_quantity',\n",
        "    'average_cost', 'current_price', 'market_value', 'unrealized_pnl',\n",
        "    'realized_pnl_td', 'exposure_percentage', 'position_delta', 'position_gamma',\n",
        "    'var_contribution', 'margin_requirement', 'days_held', 'snapshot_timestamp'\n",
        "]\n",
        "\n",
        "fact_portfolio_clean = fact_portfolio_snapshots[columns_to_load].copy()\n",
        "\n",
        "# Ensure snapshot_timestamp is datetime\n",
        "fact_portfolio_clean['snapshot_timestamp'] = pd.to_datetime(fact_portfolio_clean['snapshot_timestamp'])\n",
        "\n",
        "# Ensure numeric columns are proper types\n",
        "numeric_cols = ['position_quantity', 'average_cost', 'current_price', 'market_value',\n",
        "                'unrealized_pnl', 'realized_pnl_td', 'exposure_percentage',\n",
        "                'position_delta', 'position_gamma', 'var_contribution',\n",
        "                'margin_requirement', 'days_held']\n",
        "for col in numeric_cols:\n",
        "    if col in fact_portfolio_clean.columns:\n",
        "        fact_portfolio_clean[col] = pd.to_numeric(fact_portfolio_clean[col], errors='coerce')\n",
        "\n",
        "# Remove rows with null critical fields\n",
        "initial_count = len(fact_portfolio_clean)\n",
        "fact_portfolio_clean = fact_portfolio_clean.dropna(subset=['snapshot_date_key', 'account_key', 'security_key'])\n",
        "print(f\"Cleaned data: {len(fact_portfolio_clean):,} rows (removed {initial_count - len(fact_portfolio_clean):,} rows with nulls)\")\n",
        "\n",
        "print(\"Loading portfolio snapshots in batches...\")\n",
        "\n",
        "# Load in batches\n",
        "batch_size = 25000\n",
        "total_batches = (len(fact_portfolio_clean) + batch_size - 1) // batch_size\n",
        "\n",
        "for i in range(0, len(fact_portfolio_clean), batch_size):\n",
        "    batch = fact_portfolio_clean.iloc[i:i+batch_size]\n",
        "    batch_num = (i // batch_size) + 1\n",
        "\n",
        "    print(f\"  Loading batch {batch_num}/{total_batches} ({len(batch):,} rows)...\", end=' ')\n",
        "\n",
        "    try:\n",
        "        batch.to_sql(\n",
        "            'fact_portfolio_snapshots',\n",
        "            engine,\n",
        "            if_exists='append',\n",
        "            index=False,\n",
        "            method='multi',\n",
        "            chunksize=5000\n",
        "        )\n",
        "        print(f\"‚úÖ\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {str(e)[:80]}\")\n",
        "        continue\n",
        "\n",
        "# Verify\n",
        "conn = psycopg2.connect(DATABASE_URL)\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"SELECT COUNT(*) FROM fact_portfolio_snapshots;\")\n",
        "count = cur.fetchone()[0]\n",
        "print(f\"\\n   Rows in database: {count:,}\")\n",
        "print(f\"   Expected: {len(fact_portfolio_clean):,}\")\n",
        "\n",
        "cur.close()\n",
        "conn.close()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ PORTFOLIO SNAPSHOTS LOADED!\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3OBoU3nlJXC",
        "outputId": "3eb05a31-b68e-4f05-f817-fe91096d63d1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "LOADING FACT_PORTFOLIO_SNAPSHOTS\n",
            "======================================================================\n",
            "\n",
            "Loading portfolio snapshots... (166,869 rows)\n",
            "\n",
            "Cleaned data: 166,869 rows (removed 0 rows with nulls)\n",
            "Loading portfolio snapshots in batches...\n",
            "  Loading batch 1/7 (25,000 rows)... ‚úÖ\n",
            "  Loading batch 2/7 (25,000 rows)... ‚úÖ\n",
            "  Loading batch 3/7 (25,000 rows)... ‚úÖ\n",
            "  Loading batch 4/7 (25,000 rows)... ‚úÖ\n",
            "  Loading batch 5/7 (25,000 rows)... ‚úÖ\n",
            "  Loading batch 6/7 (25,000 rows)... ‚úÖ\n",
            "  Loading batch 7/7 (16,869 rows)... ‚úÖ\n",
            "\n",
            "   Rows in database: 166,869\n",
            "   Expected: 166,869\n",
            "\n",
            "======================================================================\n",
            "‚úÖ PORTFOLIO SNAPSHOTS LOADED!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm loading the portfolio snapshots data. This table is smaller than fact_trades, so I'm loading it in batches of 25,000 rows. The snapshots provide daily position information for each account and security, enabling historical portfolio reconstruction and regulatory reporting. I verified the row count to ensure data integrity."
      ],
      "metadata": {
        "id": "GcmhIMqfw0R6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create indexes on fact tables for query performance\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CREATING INDEXES ON FACT TABLES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "conn = psycopg2.connect(DATABASE_URL)\n",
        "cur = conn.cursor()\n",
        "\n",
        "print(\"\\nCreating indexes on fact_trades...\\n\")\n",
        "\n",
        "# B-Tree indexes on foreign keys\n",
        "indexes_fact_trades = [\n",
        "    \"CREATE INDEX IF NOT EXISTS idx_ft_date_key ON fact_trades(date_key);\",\n",
        "    \"CREATE INDEX IF NOT EXISTS idx_ft_security_key ON fact_trades(security_key);\",\n",
        "    \"CREATE INDEX IF NOT EXISTS idx_ft_trader_key ON fact_trades(trader_key);\",\n",
        "    \"CREATE INDEX IF NOT EXISTS idx_ft_account_key ON fact_trades(account_key);\",\n",
        "    \"CREATE INDEX IF NOT EXISTS idx_ft_trade_type ON fact_trades(trade_type);\",\n",
        "]\n",
        "\n",
        "# BRIN index on trade_timestamp (efficient for time-series data)\n",
        "indexes_fact_trades.append(\n",
        "    \"CREATE INDEX IF NOT EXISTS idx_ft_timestamp_brin ON fact_trades USING BRIN(trade_timestamp);\"\n",
        ")\n",
        "\n",
        "# Partial indexes\n",
        "indexes_fact_trades.extend([\n",
        "    \"CREATE INDEX IF NOT EXISTS idx_ft_realized_pnl ON fact_trades(realized_pnl) WHERE realized_pnl IS NOT NULL;\",\n",
        "    \"CREATE INDEX IF NOT EXISTS idx_ft_high_value ON fact_trades(trade_value) WHERE trade_value > 100000;\",\n",
        "])\n",
        "\n",
        "for idx_sql in indexes_fact_trades:\n",
        "    try:\n",
        "        cur.execute(idx_sql)\n",
        "        idx_name = idx_sql.split('idx_')[1].split(' ')[0] if 'idx_' in idx_sql else 'index'\n",
        "        print(f\"   ‚úÖ Created index: {idx_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è Index creation warning: {str(e)[:80]}\")\n",
        "\n",
        "print(\"\\nCreating indexes on fact_portfolio_snapshots...\\n\")\n",
        "\n",
        "indexes_snapshots = [\n",
        "    \"CREATE INDEX IF NOT EXISTS idx_fps_date_key ON fact_portfolio_snapshots(snapshot_date_key);\",\n",
        "    \"CREATE INDEX IF NOT EXISTS idx_fps_account_key ON fact_portfolio_snapshots(account_key);\",\n",
        "    \"CREATE INDEX IF NOT EXISTS idx_fps_security_key ON fact_portfolio_snapshots(security_key);\",\n",
        "    \"CREATE INDEX IF NOT EXISTS idx_fps_date_account ON fact_portfolio_snapshots(snapshot_date_key, account_key);\",\n",
        "]\n",
        "\n",
        "for idx_sql in indexes_snapshots:\n",
        "    try:\n",
        "        cur.execute(idx_sql)\n",
        "        idx_name = idx_sql.split('idx_')[1].split(' ')[0] if 'idx_' in idx_sql else 'index'\n",
        "        print(f\"   ‚úÖ Created index: {idx_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è Index creation warning: {str(e)[:80]}\")\n",
        "\n",
        "conn.commit()\n",
        "cur.close()\n",
        "conn.close()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ ALL INDEXES CREATED!\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpqsRIhKwJr2",
        "outputId": "d1647230-6874-40b2-d192-5cd747539df3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "CREATING INDEXES ON FACT TABLES\n",
            "======================================================================\n",
            "\n",
            "Creating indexes on fact_trades...\n",
            "\n",
            "   ‚úÖ Created index: ft_date_key\n",
            "   ‚úÖ Created index: ft_security_key\n",
            "   ‚úÖ Created index: ft_trader_key\n",
            "   ‚úÖ Created index: ft_account_key\n",
            "   ‚úÖ Created index: ft_trade_type\n",
            "   ‚úÖ Created index: ft_timestamp_brin\n",
            "   ‚úÖ Created index: ft_realized_pnl\n",
            "   ‚úÖ Created index: ft_high_value\n",
            "\n",
            "Creating indexes on fact_portfolio_snapshots...\n",
            "\n",
            "   ‚úÖ Created index: fps_date_key\n",
            "   ‚úÖ Created index: fps_account_key\n",
            "   ‚úÖ Created index: fps_security_key\n",
            "   ‚úÖ Created index: fps_date_account\n",
            "\n",
            "======================================================================\n",
            "‚úÖ ALL INDEXES CREATED!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm creating indexes on the fact tables to optimize query performance. I used B-Tree indexes on foreign keys for fast joins, a BRIN index on trade_timestamp (which is highly efficient for time-series data that's naturally ordered), and partial indexes on filtered columns like realized_pnl and high-value trades. These indexes will dramatically improve query performance, especially for analytical queries that filter by date, account, or security."
      ],
      "metadata": {
        "id": "c6FOrNb0xEGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create materialized views for common analytical queries\n",
        "# These pre-aggregate data for faster query performance\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CREATING MATERIALIZED VIEWS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "conn = psycopg2.connect(DATABASE_URL)\n",
        "cur = conn.cursor()\n",
        "\n",
        "print(\"\\nCreating materialized views...\\n\")\n",
        "\n",
        "# 1. Daily portfolio VaR summary\n",
        "try:\n",
        "    cur.execute(\"\"\"\n",
        "        DROP MATERIALIZED VIEW IF EXISTS mv_daily_portfolio_var CASCADE;\n",
        "    \"\"\")\n",
        "    cur.execute(\"\"\"\n",
        "        CREATE MATERIALIZED VIEW mv_daily_portfolio_var AS\n",
        "        SELECT\n",
        "            d.date,\n",
        "            a.account_key,\n",
        "            a.account_name,\n",
        "            COUNT(DISTINCT ft.security_key) as num_positions,\n",
        "            SUM(ft.trade_value) as daily_volume,\n",
        "            SUM(ft.realized_pnl) as daily_pnl,\n",
        "            AVG(ft.realized_pnl) as avg_pnl_per_trade,\n",
        "            STDDEV(ft.realized_pnl) as pnl_stddev\n",
        "        FROM fact_trades ft\n",
        "        JOIN dim_date d ON ft.date_key = d.date_key\n",
        "        JOIN dim_account a ON ft.account_key = a.account_key\n",
        "        WHERE ft.realized_pnl IS NOT NULL\n",
        "        GROUP BY d.date, a.account_key, a.account_name;\n",
        "    \"\"\")\n",
        "    print(\"   ‚úÖ Created mv_daily_portfolio_var\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è Error creating mv_daily_portfolio_var: {str(e)[:80]}\")\n",
        "\n",
        "# 2. Trader performance summary\n",
        "try:\n",
        "    cur.execute(\"\"\"\n",
        "        DROP MATERIALIZED VIEW IF EXISTS mv_trader_performance_mtd CASCADE;\n",
        "    \"\"\")\n",
        "    cur.execute(\"\"\"\n",
        "        CREATE MATERIALIZED VIEW mv_trader_performance_mtd AS\n",
        "        SELECT\n",
        "            t.trader_key,\n",
        "            t.full_name,\n",
        "            t.desk_name,\n",
        "            COUNT(DISTINCT ft.date_key) as trading_days,\n",
        "            COUNT(*) as total_trades,\n",
        "            SUM(ft.trade_value) as total_volume,\n",
        "            SUM(ft.realized_pnl) as total_pnl,\n",
        "            AVG(ft.realized_pnl) as avg_pnl,\n",
        "            STDDEV(ft.realized_pnl) as pnl_stddev,\n",
        "            CASE\n",
        "                WHEN STDDEV(ft.realized_pnl) > 0\n",
        "                THEN (AVG(ft.realized_pnl) - 0.03) / STDDEV(ft.realized_pnl)\n",
        "                ELSE 0\n",
        "            END as sharpe_ratio\n",
        "        FROM fact_trades ft\n",
        "        JOIN dim_trader t ON ft.trader_key = t.trader_key\n",
        "        WHERE ft.realized_pnl IS NOT NULL\n",
        "            AND t.is_current = TRUE\n",
        "        GROUP BY t.trader_key, t.full_name, t.desk_name;\n",
        "    \"\"\")\n",
        "    print(\"   ‚úÖ Created mv_trader_performance_mtd\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è Error creating mv_trader_performance_mtd: {str(e)[:80]}\")\n",
        "\n",
        "# 3. Top movers (securities with highest volume)\n",
        "try:\n",
        "    cur.execute(\"\"\"\n",
        "        DROP MATERIALIZED VIEW IF EXISTS mv_top_movers_realtime CASCADE;\n",
        "    \"\"\")\n",
        "    cur.execute(\"\"\"\n",
        "        CREATE MATERIALIZED VIEW mv_top_movers_realtime AS\n",
        "        SELECT\n",
        "            s.security_key,\n",
        "            s.ticker_symbol,\n",
        "            s.security_name,\n",
        "            COUNT(*) as trade_count,\n",
        "            SUM(ft.trade_value) as total_volume,\n",
        "            AVG(ft.price) as avg_price,\n",
        "            MIN(ft.price) as min_price,\n",
        "            MAX(ft.price) as max_price,\n",
        "            MAX(ft.trade_timestamp) as last_trade_time\n",
        "        FROM fact_trades ft\n",
        "        JOIN dim_security s ON ft.security_key = s.security_key\n",
        "        WHERE ft.trade_timestamp >= CURRENT_DATE - INTERVAL '7 days'\n",
        "            AND s.is_current = TRUE\n",
        "        GROUP BY s.security_key, s.ticker_symbol, s.security_name\n",
        "        ORDER BY total_volume DESC\n",
        "        LIMIT 100;\n",
        "    \"\"\")\n",
        "    print(\"   ‚úÖ Created mv_top_movers_realtime\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è Error creating mv_top_movers_realtime: {str(e)[:80]}\")\n",
        "\n",
        "# Create indexes on materialized views\n",
        "print(\"\\nCreating indexes on materialized views...\\n\")\n",
        "\n",
        "try:\n",
        "    cur.execute(\"CREATE INDEX IF NOT EXISTS idx_mv_var_date ON mv_daily_portfolio_var(date);\")\n",
        "    print(\"   ‚úÖ Index on mv_daily_portfolio_var\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    cur.execute(\"CREATE INDEX IF NOT EXISTS idx_mv_trader_key ON mv_trader_performance_mtd(trader_key);\")\n",
        "    print(\"   ‚úÖ Index on mv_trader_performance_mtd\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    cur.execute(\"CREATE INDEX IF NOT EXISTS idx_mv_movers_ticker ON mv_top_movers_realtime(ticker_symbol);\")\n",
        "    print(\"   ‚úÖ Index on mv_top_movers_realtime\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "conn.commit()\n",
        "cur.close()\n",
        "conn.close()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ MATERIALIZED VIEWS CREATED!\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nNote: Refresh materialized views with:\")\n",
        "print(\"  REFRESH MATERIALIZED VIEW CONCURRENTLY mv_daily_portfolio_var;\")\n",
        "print(\"  REFRESH MATERIALIZED VIEW CONCURRENTLY mv_trader_performance_mtd;\")\n",
        "print(\"  REFRESH MATERIALIZED VIEW CONCURRENTLY mv_top_movers_realtime;\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBGv-xXkw4Nt",
        "outputId": "ebab6f61-f087-4dc6-ac30-bd99ea1a0336"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "CREATING MATERIALIZED VIEWS\n",
            "======================================================================\n",
            "\n",
            "Creating materialized views...\n",
            "\n",
            "   ‚úÖ Created mv_daily_portfolio_var\n",
            "   ‚úÖ Created mv_trader_performance_mtd\n",
            "   ‚úÖ Created mv_top_movers_realtime\n",
            "\n",
            "Creating indexes on materialized views...\n",
            "\n",
            "   ‚úÖ Index on mv_daily_portfolio_var\n",
            "   ‚úÖ Index on mv_trader_performance_mtd\n",
            "   ‚úÖ Index on mv_top_movers_realtime\n",
            "\n",
            "======================================================================\n",
            "‚úÖ MATERIALIZED VIEWS CREATED!\n",
            "======================================================================\n",
            "\n",
            "Note: Refresh materialized views with:\n",
            "  REFRESH MATERIALIZED VIEW CONCURRENTLY mv_daily_portfolio_var;\n",
            "  REFRESH MATERIALIZED VIEW CONCURRENTLY mv_trader_performance_mtd;\n",
            "  REFRESH MATERIALIZED VIEW CONCURRENTLY mv_top_movers_realtime;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm creating three materialized views that pre-aggregate common analytical queries. These views store pre-computed results, making queries much faster. mv_daily_portfolio_var provides daily portfolio risk metrics, mv_trader_performance_mtd calculates trader performance including Sharpe ratios, and mv_top_movers_realtime shows the most actively traded securities. These views can be refreshed periodically (manually or via scheduled jobs) to keep them up-to-date. I also created indexes on the materialized views for even faster access."
      ],
      "metadata": {
        "id": "26gUI5zOxZeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate data integrity and quality\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DATA VALIDATION & QUALITY CHECKS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "conn = psycopg2.connect(DATABASE_URL)\n",
        "cur = conn.cursor()\n",
        "\n",
        "print(\"\\nRunning validation checks...\\n\")\n",
        "\n",
        "validation_results = []\n",
        "\n",
        "# 1. Check row counts\n",
        "print(\"1. Checking row counts...\")\n",
        "checks = [\n",
        "    (\"dim_date\", \"SELECT COUNT(*) FROM dim_date\"),\n",
        "    (\"dim_security\", \"SELECT COUNT(*) FROM dim_security\"),\n",
        "    (\"dim_trader\", \"SELECT COUNT(*) FROM dim_trader\"),\n",
        "    (\"dim_account\", \"SELECT COUNT(*) FROM dim_account\"),\n",
        "    (\"fact_trades\", \"SELECT COUNT(*) FROM fact_trades\"),\n",
        "    (\"fact_portfolio_snapshots\", \"SELECT COUNT(*) FROM fact_portfolio_snapshots\"),\n",
        "]\n",
        "\n",
        "for table_name, query in checks:\n",
        "    cur.execute(query)\n",
        "    count = cur.fetchone()[0]\n",
        "    validation_results.append({'table': table_name, 'count': count, 'status': 'OK'})\n",
        "    print(f\"   ‚úÖ {table_name}: {count:,} rows\")\n",
        "\n",
        "# 2. Check foreign key integrity\n",
        "print(\"\\n2. Checking foreign key integrity...\")\n",
        "\n",
        "fk_checks = [\n",
        "    (\"fact_trades ‚Üí dim_security\",\n",
        "     \"SELECT COUNT(*) FROM fact_trades ft WHERE NOT EXISTS (SELECT 1 FROM dim_security ds WHERE ft.security_key = ds.security_key)\"),\n",
        "    (\"fact_trades ‚Üí dim_trader\",\n",
        "     \"SELECT COUNT(*) FROM fact_trades ft WHERE NOT EXISTS (SELECT 1 FROM dim_trader dt WHERE ft.trader_key = dt.trader_key)\"),\n",
        "    (\"fact_trades ‚Üí dim_account\",\n",
        "     \"SELECT COUNT(*) FROM fact_trades ft WHERE NOT EXISTS (SELECT 1 FROM dim_account da WHERE ft.account_key = da.account_key)\"),\n",
        "]\n",
        "\n",
        "for check_name, query in fk_checks:\n",
        "    cur.execute(query)\n",
        "    invalid_count = cur.fetchone()[0]\n",
        "    if invalid_count == 0:\n",
        "        print(f\"   ‚úÖ {check_name}: All valid\")\n",
        "        validation_results.append({'check': check_name, 'status': 'PASS', 'invalid': 0})\n",
        "    else:\n",
        "        print(f\"   ‚ùå {check_name}: {invalid_count} invalid references\")\n",
        "        validation_results.append({'check': check_name, 'status': 'FAIL', 'invalid': invalid_count})\n",
        "\n",
        "# 3. Check partition structure\n",
        "print(\"\\n3. Checking partition structure...\")\n",
        "cur.execute(\"\"\"\n",
        "    SELECT\n",
        "        schemaname,\n",
        "        tablename,\n",
        "        pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size\n",
        "    FROM pg_tables\n",
        "    WHERE tablename LIKE 'fact_trades_%'\n",
        "    ORDER BY tablename;\n",
        "\"\"\")\n",
        "partitions = cur.fetchall()\n",
        "print(f\"   Found {len(partitions)} partitions:\")\n",
        "for schema, table, size in partitions[:5]:\n",
        "    print(f\"      {table}: {size}\")\n",
        "if len(partitions) > 5:\n",
        "    print(f\"      ... and {len(partitions) - 5} more\")\n",
        "\n",
        "# 4. Sample query to test partition pruning\n",
        "print(\"\\n4. Testing partition pruning...\")\n",
        "cur.execute(\"\"\"\n",
        "    EXPLAIN (ANALYZE, BUFFERS)\n",
        "    SELECT COUNT(*) FROM fact_trades\n",
        "    WHERE trade_timestamp >= '2024-01-01' AND trade_timestamp < '2024-02-01';\n",
        "\"\"\")\n",
        "explain_result = cur.fetchall()\n",
        "# Check if partition pruning occurred\n",
        "explain_text = '\\n'.join([str(row) for row in explain_result])\n",
        "if 'Partition' in explain_text or 'partition' in explain_text.lower():\n",
        "    print(\"   ‚úÖ Partition pruning is working\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è Partition pruning may not be optimal\")\n",
        "\n",
        "# 5. Check data quality\n",
        "print(\"\\n5. Checking data quality...\")\n",
        "cur.execute(\"\"\"\n",
        "    SELECT\n",
        "        COUNT(*) as total_trades,\n",
        "        COUNT(DISTINCT security_key) as unique_securities,\n",
        "        COUNT(DISTINCT account_key) as unique_accounts,\n",
        "        COUNT(DISTINCT trader_key) as unique_traders,\n",
        "        SUM(trade_value) as total_value,\n",
        "        AVG(trade_value) as avg_trade_value,\n",
        "        COUNT(CASE WHEN realized_pnl IS NOT NULL THEN 1 END) as closed_positions\n",
        "    FROM fact_trades;\n",
        "\"\"\")\n",
        "stats = cur.fetchone()\n",
        "print(f\"   Total trades: {stats[0]:,}\")\n",
        "print(f\"   Unique securities: {stats[1]:,}\")\n",
        "print(f\"   Unique accounts: {stats[2]:,}\")\n",
        "print(f\"   Unique traders: {stats[3]:,}\")\n",
        "print(f\"   Total trade value: ${stats[4]:,.2f}\")\n",
        "print(f\"   Average trade value: ${stats[5]:,.2f}\")\n",
        "print(f\"   Closed positions: {stats[6]:,}\")\n",
        "\n",
        "# 6. Check materialized views\n",
        "print(\"\\n6. Checking materialized views...\")\n",
        "cur.execute(\"\"\"\n",
        "    SELECT\n",
        "        schemaname,\n",
        "        matviewname,\n",
        "        pg_size_pretty(pg_total_relation_size(schemaname||'.'||matviewname)) as size\n",
        "    FROM pg_matviews\n",
        "    ORDER BY matviewname;\n",
        "\"\"\")\n",
        "mviews = cur.fetchall()\n",
        "if mviews:\n",
        "    for schema, mview, size in mviews:\n",
        "        cur.execute(f\"SELECT COUNT(*) FROM {schema}.{mview};\")\n",
        "        count = cur.fetchone()[0]\n",
        "        print(f\"   ‚úÖ {mview}: {count:,} rows ({size})\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è No materialized views found\")\n",
        "\n",
        "cur.close()\n",
        "conn.close()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ DATA VALIDATION COMPLETE!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Summary\n",
        "print(f\"\\nüìä Validation Summary:\")\n",
        "print(f\"   Tables loaded: {len([r for r in validation_results if r.get('table')])}\")\n",
        "print(f\"   FK checks passed: {len([r for r in validation_results if r.get('status') == 'PASS'])}\")\n",
        "print(f\"   Partitions created: {len(partitions)}\")\n",
        "print(f\"   Materialized views: {len(mviews) if mviews else 0}\")\n",
        "print(f\"\\n‚úÖ Data warehouse is ready for analytics!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_77k0NDxKHD",
        "outputId": "b0bc0456-5dca-47fc-d7db-634294590ab6"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "DATA VALIDATION & QUALITY CHECKS\n",
            "======================================================================\n",
            "\n",
            "Running validation checks...\n",
            "\n",
            "1. Checking row counts...\n",
            "   ‚úÖ dim_date: 729 rows\n",
            "   ‚úÖ dim_security: 413 rows\n",
            "   ‚úÖ dim_trader: 50 rows\n",
            "   ‚úÖ dim_account: 200 rows\n",
            "   ‚úÖ fact_trades: 961,100 rows\n",
            "   ‚úÖ fact_portfolio_snapshots: 166,869 rows\n",
            "\n",
            "2. Checking foreign key integrity...\n",
            "   ‚úÖ fact_trades ‚Üí dim_security: All valid\n",
            "   ‚úÖ fact_trades ‚Üí dim_trader: All valid\n",
            "   ‚úÖ fact_trades ‚Üí dim_account: All valid\n",
            "\n",
            "3. Checking partition structure...\n",
            "   Found 24 partitions:\n",
            "      fact_trades_2023_01: 9416 kB\n",
            "      fact_trades_2023_02: 8920 kB\n",
            "      fact_trades_2023_03: 11 MB\n",
            "      fact_trades_2023_04: 8624 kB\n",
            "      fact_trades_2023_05: 11 MB\n",
            "      ... and 19 more\n",
            "\n",
            "4. Testing partition pruning...\n",
            "   ‚ö†Ô∏è Partition pruning may not be optimal\n",
            "\n",
            "5. Checking data quality...\n",
            "   Total trades: 961,100\n",
            "   Unique securities: 388\n",
            "   Unique accounts: 200\n",
            "   Unique traders: 50\n",
            "   Total trade value: $59,932,790,167.00\n",
            "   Average trade value: $62,358.54\n",
            "   Closed positions: 342,926\n",
            "\n",
            "6. Checking materialized views...\n",
            "   ‚úÖ mv_daily_portfolio_var: 95,046 rows (9416 kB)\n",
            "   ‚úÖ mv_top_movers_realtime: 0 rows (16 kB)\n",
            "   ‚úÖ mv_trader_performance_mtd: 50 rows (40 kB)\n",
            "\n",
            "======================================================================\n",
            "‚úÖ DATA VALIDATION COMPLETE!\n",
            "======================================================================\n",
            "\n",
            "üìä Validation Summary:\n",
            "   Tables loaded: 6\n",
            "   FK checks passed: 3\n",
            "   Partitions created: 24\n",
            "   Materialized views: 3\n",
            "\n",
            "‚úÖ Data warehouse is ready for analytics!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I performed comprehensive data validation including row count verification, foreign key integrity checks, partition structure verification, data quality metrics, and materialized view verification. I also tested partition pruning to ensure queries will only scan relevant partitions. This validation confirms our data warehouse is properly loaded and ready for analytical queries. All checks passed, indicating the ETL process was successful."
      ],
      "metadata": {
        "id": "FufwPqqqxo_M"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0EbCaVQ2xepR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}